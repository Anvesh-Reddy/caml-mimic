{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import evaluation\n",
    "import csv\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 40000\n",
    "embedding_size = 50\n",
    "Y = 10\n",
    "dropout = 0.3\n",
    "kernel_size = 3\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#yield some tensors. file should hold data sorted by sequence length, for batching\n",
    "def data_generator(filename, batch_size, Y):\n",
    "    with open(filename, 'r') as infile:\n",
    "        r = csv.reader(infile)\n",
    "        #header\n",
    "        next(r)\n",
    "        cur_insts = []\n",
    "        cur_labels = []\n",
    "        cur_length = 0\n",
    "        for row in r:\n",
    "            #find the next batch_size instances with the same length\n",
    "            text = row[1]\n",
    "            length = int(row[3])\n",
    "            if length > cur_length:\n",
    "                if len(cur_insts) > 0:\n",
    "                    #create the tensors\n",
    "                    yield torch.LongTensor(cur_insts), torch.FloatTensor(cur_labels)\n",
    "                    #clear\n",
    "                    cur_insts = []\n",
    "                    cur_labels = []\n",
    "                cur_insts.append([int(w) for w in text.split()])\n",
    "                labels = [int(l) for l in row[2].split(';')]\n",
    "                cur_labels.append([1 if i in labels else 0 for i in range(Y)])\n",
    "                #reset length\n",
    "                cur_length = length\n",
    "            else:\n",
    "                if len(cur_insts) == batch_size:\n",
    "                    #create the tensors\n",
    "                    yield torch.LongTensor(cur_insts), torch.FloatTensor(cur_labels)\n",
    "                    #clear\n",
    "                    cur_insts = []\n",
    "                    cur_labels = []\n",
    "                cur_insts.append([int(w) for w in text.split()])\n",
    "                labels = [int(l) for l in row[2].split(';')]\n",
    "                cur_labels.append([1 if i in labels else 0 for i in range(Y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.embed_drop = nn.Dropout(p=dropout)\n",
    "        self.conv = nn.Conv1d(embedding_size, Y, kernel_size=kernel_size)\n",
    "        self.conv_drop = nn.Dropout(p=dropout)\n",
    "        self.fc = nn.Linear(Y, Y)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed_drop(self.embed(x))\n",
    "        x = torch.transpose(x, 1, 2).contiguous()\n",
    "        \n",
    "        x = self.conv_drop(self.conv(x))\n",
    "        \n",
    "        x = F.tanh(F.max_pool1d(x, kernel_size=x.size()[2]))\n",
    "        x = torch.squeeze(x, dim=2)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, dataset):\n",
    "    filename = '../mimicdata/notes_10_train_' + dataset + '_sorted.csv'\n",
    "#     train_loader = data_generator(filename, batch_size, Y)\n",
    "    #just sets the model into 'train' mode\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(data_generator(filename, batch_size, Y)):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        #clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        #forward computation\n",
    "        output = model(data)\n",
    "        loss = F.binary_cross_entropy(output, target)\n",
    "        #backward pass\n",
    "        loss.backward()\n",
    "        #kick it in the right direction\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [batch #{}, batch_size {}, seq length {}]\\tLoss: {:.6f}'.format(\n",
    "                epoch+1, batch_idx, data.size()[0], data.size()[1], loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(epoch, dataset):\n",
    "    filename = '../mimicdata/notes_10_dev_' + dataset + '_sorted.csv'\n",
    "#     test_loader = data_generator(filename, batch_size, Y)\n",
    "    #set model to 'eval' mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    y = []\n",
    "    yhat = []\n",
    "    yhat_raw = []\n",
    "    for data, target in data_generator(filename, 1, Y):\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        #predict\n",
    "        output = model(data)\n",
    "        test_loss += F.binary_cross_entropy(output, target)\n",
    "        yhat_raw.append(output.data.numpy())\n",
    "        output[output >= 0.5] = 1\n",
    "        output[output < 0.5] = 0\n",
    "        y.append(target.data.numpy())\n",
    "        yhat.append(output.data.numpy())\n",
    "        \n",
    "    y = np.squeeze(np.array(y))\n",
    "    yhat = np.squeeze(np.array(yhat))\n",
    "    yhat_raw = np.squeeze(np.array(yhat))\n",
    "    print(y.shape)\n",
    "    print(yhat.shape)\n",
    "    acc, prec, rec, f1 = evaluation.all_metrics(yhat, y)\n",
    "    print(\"acc, prec, rec, f1\")\n",
    "    print(acc, prec, rec, f1)\n",
    "    return y, yhat, yhat_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [batch #0, batch_size 1, seq length 6]\tLoss: 0.209235\n",
      "Train Epoch: 0 [batch #100, batch_size 31, seq length 116]\tLoss: 0.270123\n",
      "Train Epoch: 0 [batch #200, batch_size 7, seq length 160]\tLoss: 0.286257\n",
      "Train Epoch: 0 [batch #300, batch_size 32, seq length 196]\tLoss: 0.261735\n",
      "Train Epoch: 0 [batch #400, batch_size 21, seq length 235]\tLoss: 0.262348\n",
      "Train Epoch: 0 [batch #500, batch_size 32, seq length 278]\tLoss: 0.293996\n",
      "Train Epoch: 0 [batch #600, batch_size 32, seq length 324]\tLoss: 0.292046\n",
      "Train Epoch: 0 [batch #700, batch_size 32, seq length 373]\tLoss: 0.286776\n",
      "Train Epoch: 0 [batch #800, batch_size 14, seq length 466]\tLoss: 0.253835\n",
      "Train Epoch: 0 [batch #900, batch_size 2, seq length 567]\tLoss: 0.233735\n",
      "Train Epoch: 0 [batch #1000, batch_size 4, seq length 671]\tLoss: 0.376782\n",
      "Train Epoch: 0 [batch #1100, batch_size 2, seq length 781]\tLoss: 0.347014\n",
      "Train Epoch: 0 [batch #1200, batch_size 1, seq length 899]\tLoss: 0.229172\n",
      "Train Epoch: 0 [batch #1300, batch_size 1, seq length 1032]\tLoss: 0.196748\n",
      "Train Epoch: 0 [batch #1400, batch_size 1, seq length 1197]\tLoss: 0.213750\n",
      "Train Epoch: 0 [batch #1500, batch_size 1, seq length 1503]\tLoss: 0.176512\n",
      "(8970, 10)\n",
      "(8970, 10)\n",
      "acc, prec, rec, f1\n",
      "0.0 0.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/james/Windows8_OS/Users/James/Documents/SCHOOL/MS/research/mimic/src/evaluation.py:21: RuntimeWarning: invalid value encountered in true_divide\n",
      "  num = intersect_size(yhat, y, 1) / yhat.sum(axis=1)\n"
     ]
    }
   ],
   "source": [
    "y = None\n",
    "yhat = None\n",
    "yhat_raw = None\n",
    "for epoch in range(1):\n",
    "    train(epoch, 'single')\n",
    "    y,yhat, yhat_raw = test(epoch, 'single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_raw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
