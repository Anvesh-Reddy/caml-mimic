{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import operator\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from constants import DATA_DIR, DISCH_DIR, MODEL_DIR\n",
    "import datasets\n",
    "import evaluation\n",
    "import log_reg\n",
    "from dataproc import pipeline_disch\n",
    "import learn.training as training\n",
    "import learn.tools as tools\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance by code type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mdir = '%s/conv_attn_reg_spearmint_best_mimic_3_full_final' % MODEL_DIR\n",
    "Y = 'full'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_labels = tools.get_num_labels(Y, version='mimic3', h_train=False)\n",
    "\n",
    "dicts = datasets.load_lookups('%s/train_full.csv' % DISCH_DIR, '%s/vocab.csv' % DISCH_DIR, Y=Y, version='mimic3')\n",
    "ind2c, c2ind = dicts[2], dicts[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#redo the code lookups IF model was rare-label\n",
    "code_freqs, n = datasets.load_code_freqs('%s/train_full.csv' % DISCH_DIR)\n",
    "freqs = sorted(code_freqs.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "ind2c = defaultdict(str)\n",
    "for i, (code, freq) in enumerate(freqs):\n",
    "    ind2c[i] = code\n",
    "c2ind = defaultdict(int, {c:i for i,c in ind2c.iteritems()})\n",
    "dicts = (dicts[0], dicts[1], ind2c, c2ind, dicts[4], dicts[5], dicts[6], dicts[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d2ind = {}\n",
    "p2ind = {}\n",
    "ind2d = {}\n",
    "ind2p = {}\n",
    "all_codes = set()\n",
    "\n",
    "#get ground truth for diagnoses and procedures\n",
    "diag_golds = {}#defaultdict(lambda: set([]))\n",
    "proc_golds = {}#defaultdict(lambda: set([]))\n",
    "with open('%s/test_%s.csv' % (DISCH_DIR, str(Y)), 'r') as f:\n",
    "    r = csv.reader(f)\n",
    "    #header\n",
    "    next(r)\n",
    "    for row in r:\n",
    "        codes = set([c for c in row[3].split(';')])\n",
    "        diag_golds[row[1]] = set()\n",
    "        proc_golds[row[1]] = set()\n",
    "        for code in codes:\n",
    "            try:\n",
    "                pos = code.index('.')\n",
    "                if pos == 3:\n",
    "                    if code not in d2ind:\n",
    "                        d2ind[code] = len(d2ind)\n",
    "                    diag_golds[row[1]].add(code)\n",
    "                elif pos == 2:\n",
    "                    if code not in p2ind:\n",
    "                        p2ind[code] = len(p2ind)\n",
    "                    proc_golds[row[1]].add(code)\n",
    "            except:\n",
    "                if len(code) == 3 or (code[0] == 'E' and len(code) == 4):\n",
    "                    if code not in d2ind:\n",
    "                        d2ind[code] = len(d2ind)\n",
    "                    diag_golds[row[1]].add(code)\n",
    "                else:\n",
    "                    print(\"this code is wack\", code)\n",
    "#         golds[row[1]] = codes\n",
    "\n",
    "\n",
    "#get predictions for diagnoses and procedures\n",
    "diag_preds = {}#defaultdict(lambda: set([]))\n",
    "proc_preds = {}#defaultdict(lambda: set([]))\n",
    "with open('%s/preds_test.csv' % mdir, 'r') as f:\n",
    "    r = csv.reader(f, delimiter='|')\n",
    "    for row in r:\n",
    "        if len(row) > 1:\n",
    "            diag_preds[row[0]] = set()\n",
    "            proc_preds[row[0]] = set()\n",
    "            for c in row[1:]:\n",
    "                if c != '':\n",
    "                    code = ind2c[int(c)]\n",
    "                    try:\n",
    "                        pos = code.index('.')\n",
    "                        if pos == 3 or (code[0] == 'E' and pos == 4):\n",
    "                            if code not in d2ind:\n",
    "                                d2ind[code] = len(d2ind)\n",
    "                            diag_preds[row[0]].add(code)\n",
    "                        elif pos == 2:\n",
    "                            if code not in p2ind:\n",
    "                                p2ind[code] = len(p2ind)\n",
    "                            proc_preds[row[0]].add(code)\n",
    "                        else:\n",
    "                            print(\"this code is wack\", code)\n",
    "                    except:\n",
    "                        if len(code) == 3 or (code[0] == 'E' and len(code) == 4):\n",
    "                            if code not in d2ind:\n",
    "                                d2ind[code] = len(d2ind)\n",
    "                            diag_preds[row[0]].add(code)\n",
    "                        else:\n",
    "                            print(\"this code is wack\", code)\n",
    "                    #preds[row[0]] = set([int(ind) for ind in row[1:] if ind != ''])\n",
    "\n",
    "\n",
    "hadm_ids = sorted(set(diag_golds.keys()).intersection(set(diag_preds.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(954, 3121, 3372)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p2ind), len(d2ind), len(hadm_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind2d = {i:d for d,i in d2ind.iteritems()}\n",
    "ind2p = {i:p for p,i in p2ind.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:02, 239.03it/s]"
     ]
    }
   ],
   "source": [
    "#DIAGNOSIS METRICS\n",
    "num_labels = len(d2ind)\n",
    "yhat_diag = np.zeros((len(hadm_ids), num_labels))\n",
    "y_diag = np.zeros((len(hadm_ids), num_labels))\n",
    "for i,hadm_id in tqdm(enumerate(hadm_ids)):\n",
    "    yhat_diag_inds = [1 if ind2d[j] in diag_preds[hadm_id] else 0 for j in range(num_labels)]\n",
    "    gold_diag_inds = [1 if ind2d[j] in diag_golds[hadm_id] else 0 for j in range(num_labels)]\n",
    "    yhat_diag[i] = yhat_diag_inds\n",
    "    y_diag[i] = gold_diag_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(evaluation)\n",
    "evaluation.all_metrics(yhat_diag, y_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROCEDURE METRICS\n",
    "num_labels = len(p2ind)\n",
    "yhat_proc = np.zeros((len(hadm_ids), num_labels))\n",
    "y_proc = np.zeros((len(hadm_ids), num_labels))\n",
    "for i,hadm_id in tqdm(enumerate(hadm_ids)):\n",
    "    yhat_proc_inds = [1 if ind2p[j] in proc_preds[hadm_id] else 0 for j in range(num_labels)]\n",
    "    gold_proc_inds = [1 if ind2p[j] in proc_golds[hadm_id] else 0 for j in range(num_labels)]\n",
    "    yhat_proc[i] = yhat_proc_inds\n",
    "    y_proc[i] = gold_proc_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(evaluation)\n",
    "evaluation.all_metrics(yhat_proc, y_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bar chart of model performance vs label frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pick the two models to compare and set up constants.\n",
    "model_dirs = ['%s/conv_attn_Nov_20_02:02' % MODEL_DIR, '%s/conv_attn_Nov_27_16:52' % MODEL_DIR]\n",
    "Y = 'full'\n",
    "num_labels = tools.get_num_labels(Y, version='mimic3', h_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dicts = datasets.load_lookups('%s/train_%s.csv' % (DISCH_DIR, str(Y)), '%s/vocab.csv' % DISCH_DIR,\n",
    "                              Y=Y, version='mimic3')\n",
    "ind2c, c2ind = dicts[2], dicts[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freqs, n = datasets.load_code_freqs('%s/train_%s.csv' % (DISCH_DIR, str(Y)), version='mimic3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq_list = sorted(freqs.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "freq2ind = defaultdict(int)\n",
    "for i, (code, freq) in enumerate(freq_list):\n",
    "    freq2ind[i] = c2ind[code]\n",
    "ind2freq = defaultdict(str, {c:i for i,c in freq2ind.iteritems()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'401.9'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind2c[freq2ind[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nethome/jmullenbach3/cnn-medical-text/saved_models/conv_attn_Nov_20_02:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3372it [00:17, 197.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nethome/jmullenbach3/cnn-medical-text/saved_models/conv_attn_Nov_27_16:52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3372it [00:17, 197.04it/s]\n"
     ]
    }
   ],
   "source": [
    "#get the metrics for each model\n",
    "accs, precs, recs, f1s = [], [], [], []\n",
    "for idx, mdir in enumerate(model_dirs):\n",
    "    print(mdir)\n",
    "    preds = defaultdict(lambda: [])\n",
    "    with open('%s/preds_test.csv' % mdir, 'r') as f:\n",
    "        r = csv.reader(f, delimiter='|')\n",
    "        for row in r:\n",
    "            if len(row) > 1:\n",
    "                if idx == 0:\n",
    "                    preds[row[0]] = set([ind2freq[int(ind)] for ind in row[1:] if ind != ''])\n",
    "                else:\n",
    "                    preds[row[0]] = set([int(ind) for ind in row[1:] if ind != ''])\n",
    "            else:\n",
    "                preds[row[0]] = set([])\n",
    "\n",
    "    golds = defaultdict(lambda: [])\n",
    "    with open('%s/test_%s.csv' % (DISCH_DIR, str(Y)), 'r') as f:\n",
    "        r = csv.reader(f)\n",
    "        #header\n",
    "        next(r)\n",
    "        for row in r:\n",
    "            codes = set([ind2freq[c2ind[c]] for c in row[3].split(';')])\n",
    "            golds[row[1]] = codes\n",
    "\n",
    "    hadm_ids = sorted(set(golds.keys()).intersection(set(preds.keys())))\n",
    "\n",
    "    #rebuild prediction / ground truth matrices\n",
    "    yhat = np.zeros((len(hadm_ids), num_labels))\n",
    "    y = np.zeros((len(hadm_ids), num_labels))\n",
    "    for i,hadm_id in tqdm(enumerate(hadm_ids)):\n",
    "        yhat_inds = [1 if j in preds[hadm_id] else 0 for j in range(num_labels)]\n",
    "        gold_inds = [1 if j in golds[hadm_id] else 0 for j in range(num_labels)]\n",
    "        yhat[i] = yhat_inds\n",
    "        y[i] = gold_inds\n",
    "\n",
    "    acc, prec, rec, f1 = evaluation.all_per_label(yhat, y)\n",
    "    accs.append(acc)\n",
    "    precs.append(prec)\n",
    "    recs.append(rec)\n",
    "    f1s.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.061483123615620916"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f1s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,8))\n",
    "# colors = 'brgk'\n",
    "# for i,metric in enumerate(zip([accs, precs, recs, f1s], ['accuracy', 'precision', 'recall', 'f1'])):\n",
    "#     metric, name = metric\n",
    "#     plt.subplot(2,2,i+1)\n",
    "    \n",
    "#     plt.plot(np.log(freqs_idx), metric[0], 'bo')\n",
    "#     plt.plot(np.log(freqs_idx), metric[1], 'ro')\n",
    "    \n",
    "#     plt.ylabel(name)\n",
    "#     plt.title('%s vs log frequency' % name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super noisy. Let's do a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8922"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAAHiCAYAAADWL2tuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAgttSZIkqQQW2pIkSVIJ/j/9iVjywlKDZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f67778e0bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "colors = 'brgk'\n",
    "#q = how many buckets to use\n",
    "q = num_labels / 500\n",
    "# q = 20\n",
    "step = num_labels / q\n",
    "for i,metric in enumerate(zip([accs, precs, recs, f1s], ['accuracy', 'precision', 'recall', 'f1'])):\n",
    "    metric, name = metric\n",
    "    plt.subplot(2,2,i+1)\n",
    "\n",
    "    met_quantile_avgs = [np.mean(metric[0][j*step:(j+1)*step]) for j in range(q)]\n",
    "    met_quantile_avgs_2 = [np.mean(metric[1][j*step:(j+1)*step]) for j in range(q)]\n",
    "#     met_quantile_avgs = [np.sqrt(np.mean(metric[0][quantile_idxs[j]])) for j in range(q)]\n",
    "#     met_quantile_avgs_2 = [np.sqrt(np.mean(metric[1][quantile_idxs[j]])) for j in range(q)]\n",
    "    \n",
    "    #side by side bar\n",
    "    width = 0.3*step\n",
    "    p0 = plt.bar(np.arange(q)*step, met_quantile_avgs, width, alpha=0.5)\n",
    "    p1 = plt.bar(np.arange(q)*step+width, met_quantile_avgs_2, width, alpha=0.5)\n",
    "\n",
    "    #line plots\n",
    "#     p0 = plt.plot(np.arange(q), met_quantile_avgs, 'r*')\n",
    "#     plt.plot(np.arange(q), met_quantile_avgs, 'r')\n",
    "#     p1 = plt.plot(np.arange(q), met_quantile_avgs_2, 'b*')\n",
    "#     plt.plot(np.arange(q), met_quantile_avgs_2, 'b')\n",
    "    \n",
    "    plt.ylabel(name)\n",
    "    plt.title('%s vs frequency rank' % name)\n",
    "    plt.legend((p0[0], p1[0]), ('conv+attn', 'conv+attn w/ rare-label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60075717473022638"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f1s[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57568831189860781"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f1s[1][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,  892, 1784, 2676, 3568, 4460, 5352, 6244, 7136, 8028])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(q)*step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Looks like the description embedding model can handle codes outside the top ~2500 for MIMIC-III\n",
    "#What's the frequency of these groupings?\n",
    "freqs_idx = [freqs[ind2c[ind]] if ind2c[ind] in freqs.keys() else 0.0 for ind in range(num_labels)]\n",
    "freqs_idx = np.array(freqs_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.38032469749269809,\n",
       " 0.27394454348898079,\n",
       " 0.24356105147365625,\n",
       " 0.23885748966354362,\n",
       " 0.23098661002162121,\n",
       " 0.18837006410499563,\n",
       " 0.17374729734855668,\n",
       " 0.16892993968819936,\n",
       " 0.16659712475818381,\n",
       " 0.16346773887645563,\n",
       " 0.16128665174676629,\n",
       " 0.13748435307059137,\n",
       " 0.13554982361643211,\n",
       " 0.12913932405264955,\n",
       " 0.12217881121268444,\n",
       " 0.11675454235102227,\n",
       " 0.11239236809164359,\n",
       " 0.1093578120851193,\n",
       " 0.10044380381595418,\n",
       " 0.099381709213670677,\n",
       " 0.09080908849523954,\n",
       " 0.08976595986799682,\n",
       " 0.086769335811554077,\n",
       " 0.085327921708455032,\n",
       " 0.083260630429010357,\n",
       " 0.082653719227705494,\n",
       " 0.081477828775177327,\n",
       " 0.079126047870121008,\n",
       " 0.071918977354625796,\n",
       " 0.07013617570079278,\n",
       " 0.068125782346470432,\n",
       " 0.067898190645981113,\n",
       " 0.066380912642718962,\n",
       " 0.063858437962295636,\n",
       " 0.063536016386602431,\n",
       " 0.063479118461480105,\n",
       " 0.06205667033342184,\n",
       " 0.061601486932443196,\n",
       " 0.060748018055608241,\n",
       " 0.060463528429996589,\n",
       " 0.060273868679588817,\n",
       " 0.05693585707241209,\n",
       " 0.056916891097371317,\n",
       " 0.056613435496718885,\n",
       " 0.055816864545006256,\n",
       " 0.054584076167355763,\n",
       " 0.054034062891173233,\n",
       " 0.051985737586769339,\n",
       " 0.051075370784812051,\n",
       " 0.050430527633425634,\n",
       " 0.050278799833099422,\n",
       " 0.048666691954633384,\n",
       " 0.047964950878124642,\n",
       " 0.047737359177635323,\n",
       " 0.047433903576982892,\n",
       " 0.047358039676819785,\n",
       " 0.045442476197701323,\n",
       " 0.044152789894928497,\n",
       " 0.043868300269316846,\n",
       " 0.043697606493949853,\n",
       " 0.042351022266054701,\n",
       " 0.042161362515646929,\n",
       " 0.042123430565565376,\n",
       " 0.041440655464097406,\n",
       " 0.041402723514015853,\n",
       " 0.041307893638811974,\n",
       " 0.040530288662140118,\n",
       " 0.040378560861813906,\n",
       " 0.04018890111140614,\n",
       " 0.04016993513636536,\n",
       " 0.040075105261161474,\n",
       " 0.040075105261161474,\n",
       " 0.039714751735386716,\n",
       " 0.039335432234571178,\n",
       " 0.03855782725789933,\n",
       " 0.037837120206349807,\n",
       " 0.037780222281227481,\n",
       " 0.036225012327883777,\n",
       " 0.035978454652353678,\n",
       " 0.035637067101619693,\n",
       " 0.035428441376171148,\n",
       " 0.034309448848765314,\n",
       " 0.03396806129803133,\n",
       " 0.033721503622501231,\n",
       " 0.033512877897052686,\n",
       " 0.03330425217160414,\n",
       " 0.032033531843872094,\n",
       " 0.031597314417934223,\n",
       " 0.031426620642567231,\n",
       " 0.031274892842241019,\n",
       " 0.030326594090202177,\n",
       " 0.029738648863938094,\n",
       " 0.029435193263285666,\n",
       " 0.029435193263285666,\n",
       " 0.028885179987103136,\n",
       " 0.028600690361491484,\n",
       " 0.028411030611083715,\n",
       " 0.028126540985472064,\n",
       " 0.028012745135227401,\n",
       " 0.027917915260023518,\n",
       " 0.027804119409778855,\n",
       " 0.027766187459697302,\n",
       " 0.027197208208473999,\n",
       " 0.027102378333270113,\n",
       " 0.027083412358229336,\n",
       " 0.026855820657740014,\n",
       " 0.026552365057087586,\n",
       " 0.026381671281720594,\n",
       " 0.02592648788074195,\n",
       " 0.02586958995561962,\n",
       " 0.02579372605545651,\n",
       " 0.025698896180252628,\n",
       " 0.025585100330007965,\n",
       " 0.025547168379926412,\n",
       " 0.024978189128703105,\n",
       " 0.024636801577969124,\n",
       " 0.024560937677806018,\n",
       " 0.024541971702765238,\n",
       " 0.024541971702765238,\n",
       " 0.024295414027235139,\n",
       " 0.02418161817699048,\n",
       " 0.024029890376664264,\n",
       " 0.023897128551378825,\n",
       " 0.023726434776011836,\n",
       " 0.023707468800971059,\n",
       " 0.02351780905056329,\n",
       " 0.023290217350073968,\n",
       " 0.023157455524788529,\n",
       " 0.023100557599666199,\n",
       " 0.02304365967454387,\n",
       " 0.023005727724462313,\n",
       " 0.022948829799339984,\n",
       " 0.022740204073891438,\n",
       " 0.022588476273565226,\n",
       " 0.022550544323483669,\n",
       " 0.022512612373402116,\n",
       " 0.022455714448279786,\n",
       " 0.02237985054811668,\n",
       " 0.022322952622994347,\n",
       " 0.022285020672912794,\n",
       " 0.022057428972423472,\n",
       " 0.021943633122178813,\n",
       " 0.021810871296893374,\n",
       " 0.021753973371771044,\n",
       " 0.021697075446648711,\n",
       " 0.021374653870955506,\n",
       " 0.021128096195425407,\n",
       " 0.021014300345180745,\n",
       " 0.02074877669460987,\n",
       " 0.020502219019079771,\n",
       " 0.020445321093957441,\n",
       " 0.020445321093957441,\n",
       " 0.020407389143875888,\n",
       " 0.020103933543223457,\n",
       " 0.020066001593141904,\n",
       " 0.020028069643060351,\n",
       " 0.019857375867693358,\n",
       " 0.019781511967530252,\n",
       " 0.019667716117285589,\n",
       " 0.01961081819216326,\n",
       " 0.019591852217122483,\n",
       " 0.019421158441755491,\n",
       " 0.019193566741266169,\n",
       " 0.019022872965899176,\n",
       " 0.018795281265409854,\n",
       " 0.018681485415165195,\n",
       " 0.018662519440124418,\n",
       " 0.018415961764594316,\n",
       " 0.018359063839471987,\n",
       " 0.01834009786443121,\n",
       " 0.018302165914349657,\n",
       " 0.018169404089064218,\n",
       " 0.017998710313697226,\n",
       " 0.017998710313697226,\n",
       " 0.017941812388574896,\n",
       " 0.017884914463452566,\n",
       " 0.017884914463452566,\n",
       " 0.017695254713044797,\n",
       " 0.017695254713044797,\n",
       " 0.017505594962637028,\n",
       " 0.017069377536699161,\n",
       " 0.016765921936046733,\n",
       " 0.016709024010924403,\n",
       " 0.016633160110761293,\n",
       " 0.016557296210598187,\n",
       " 0.016481432310435081,\n",
       " 0.016177976709782649,\n",
       " 0.016102112809619543,\n",
       " 0.016083146834578767,\n",
       " 0.016026248909456434,\n",
       " 0.016007282934415657,\n",
       " 0.015817623184007888,\n",
       " 0.015741759283844782,\n",
       " 0.015741759283844782,\n",
       " 0.015419337708151576,\n",
       " 0.015362439783029246,\n",
       " 0.015134848082539924,\n",
       " 0.015077950157417593,\n",
       " 0.014945188332132155,\n",
       " 0.014907256382050602,\n",
       " 0.014888290407009824,\n",
       " 0.014793460531805941,\n",
       " 0.014755528581724386,\n",
       " 0.01473656260668361,\n",
       " 0.014660698706520502,\n",
       " 0.014660698706520502,\n",
       " 0.014490004931153511,\n",
       " 0.014452072981071956,\n",
       " 0.014452072981071956,\n",
       " 0.014300345180745742,\n",
       " 0.014300345180745742,\n",
       " 0.014186549330501081,\n",
       " 0.014091719455297197,\n",
       " 0.013996889580093312,\n",
       " 0.013958957630011759,\n",
       " 0.013902059704889428,\n",
       " 0.013883093729848651,\n",
       " 0.013826195804726321,\n",
       " 0.013807229829685545,\n",
       " 0.013731365929522437,\n",
       " 0.01371239995448166,\n",
       " 0.013484808253992338,\n",
       " 0.01346584227895156,\n",
       " 0.01329514850358457,\n",
       " 0.013276182528543793,\n",
       " 0.013257216553503015,\n",
       " 0.013143420703258354,\n",
       " 0.013124454728217577,\n",
       " 0.013105488753176801,\n",
       " 0.013029624853013693,\n",
       " 0.013010658877972916,\n",
       " 0.01299169290293214,\n",
       " 0.012877897052687479,\n",
       " 0.012858931077646702,\n",
       " 0.012858931077646702,\n",
       " 0.012802033152524371,\n",
       " 0.012783067177483594,\n",
       " 0.012783067177483594,\n",
       " 0.012745135227402041,\n",
       " 0.012574441452035049,\n",
       " 0.012498577551871943,\n",
       " 0.012441679626749611,\n",
       " 0.012422713651708835,\n",
       " 0.012422713651708835,\n",
       " 0.012422713651708835,\n",
       " 0.012403747676668058,\n",
       " 0.012403747676668058,\n",
       " 0.012365815726586503,\n",
       " 0.012346849751545727,\n",
       " 0.01232788377650495,\n",
       " 0.012308917801464174,\n",
       " 0.012195121951219513,\n",
       " 0.012138224026097181,\n",
       " 0.012100292076015628,\n",
       " 0.012062360125934075,\n",
       " 0.011948564275689412,\n",
       " 0.011872700375526306,\n",
       " 0.011777870500322422,\n",
       " 0.011739938550240869,\n",
       " 0.011702006600159314,\n",
       " 0.0115502787998331,\n",
       " 0.011512346849751545,\n",
       " 0.011493380874710768,\n",
       " 0.011398550999506884,\n",
       " 0.011360619049425331,\n",
       " 0.011303721124303001,\n",
       " 0.011265789174221446,\n",
       " 0.01124682319918067,\n",
       " 0.011151993323976785,\n",
       " 0.010962333573569016,\n",
       " 0.010962333573569016,\n",
       " 0.01094336759852824,\n",
       " 0.010924401623487463,\n",
       " 0.010867503698365134,\n",
       " 0.010772673823161249,\n",
       " 0.010677843947957365,\n",
       " 0.010658877972916588,\n",
       " 0.010620946022835033,\n",
       " 0.01058301407275348,\n",
       " 0.010564048097712704,\n",
       " 0.010526116147631149,\n",
       " 0.010507150172590372,\n",
       " 0.010507150172590372,\n",
       " 0.010488184197549596,\n",
       " 0.010469218222508819,\n",
       " 0.010450252247468043,\n",
       " 0.010374388347304935,\n",
       " 0.010336456397223382,\n",
       " 0.010298524447141827,\n",
       " 0.010298524447141827,\n",
       " 0.010222660546978721,\n",
       " 0.010089898721693281,\n",
       " 0.010033000796570952,\n",
       " 0.0099192049463262908,\n",
       " 0.0099002389712855143,\n",
       " 0.0098812729962447377,\n",
       " 0.0098243750711224063,\n",
       " 0.0098243750711224063,\n",
       " 0.0098054090960816298,\n",
       " 0.0097674771460000767,\n",
       " 0.0097485111709592984,\n",
       " 0.0097485111709592984,\n",
       " 0.0096916132458369688,\n",
       " 0.0096726472707961923,\n",
       " 0.0095967833706330843,\n",
       " 0.0095778173955923078,\n",
       " 0.0095778173955923078,\n",
       " 0.0095209194704699764,\n",
       " 0.0095019534954291999,\n",
       " 0.0095019534954291999,\n",
       " 0.0094829875203884233,\n",
       " 0.0094640215453476468,\n",
       " 0.0094640215453476468,\n",
       " 0.009426089595266092,\n",
       " 0.009426089595266092,\n",
       " 0.0094071236202253154,\n",
       " 0.0093691916701437623,\n",
       " 0.0093691916701437623,\n",
       " 0.0092933277699806544,\n",
       " 0.0092743617949398779,\n",
       " 0.0092743617949398779,\n",
       " 0.0092553958198991013,\n",
       " 0.0092364298448583248,\n",
       " 0.0091795319197359934,\n",
       " 0.0091415999696544403,\n",
       " 0.009084702044532109,\n",
       " 0.0090467700944505559,\n",
       " 0.0090088381443690028,\n",
       " 0.0090088381443690028,\n",
       " 0.0089898721693282245,\n",
       " 0.0089519402192466714,\n",
       " 0.0089140082691651183,\n",
       " 0.0089140082691651183,\n",
       " 0.0088191783939612339,\n",
       " 0.0088002124189204573,\n",
       " 0.0088002124189204573,\n",
       " 0.008781246443879679,\n",
       " 0.0087243485187573494,\n",
       " 0.0086484846185942415,\n",
       " 0.0086484846185942415,\n",
       " 0.0086105526685126884,\n",
       " 0.0086105526685126884,\n",
       " 0.0085726207184311353,\n",
       " 0.0085726207184311353,\n",
       " 0.008553654743390357,\n",
       " 0.008553654743390357,\n",
       " 0.008553654743390357,\n",
       " 0.0085346887683495805,\n",
       " 0.008439858893145696,\n",
       " 0.0084208929181049195,\n",
       " 0.0084208929181049195,\n",
       " 0.0084019269430641429,\n",
       " 0.0083829609680233664,\n",
       " 0.0083450290179418116,\n",
       " 0.008326063042901035,\n",
       " 0.0082501991427379288,\n",
       " 0.0081933012176155975,\n",
       " 0.0081553692675340444,\n",
       " 0.0081553692675340444,\n",
       " 0.0081553692675340444,\n",
       " 0.0081553692675340444,\n",
       " 0.0081553692675340444,\n",
       " 0.0081364032924932678,\n",
       " 0.008098471342411713,\n",
       " 0.0080795053673709365,\n",
       " 0.0080795053673709365,\n",
       " 0.0080795053673709365,\n",
       " 0.0080605393923301599,\n",
       " 0.0080605393923301599,\n",
       " 0.0080605393923301599,\n",
       " 0.0080415734172893834,\n",
       " 0.0080415734172893834,\n",
       " 0.0080226074422486068,\n",
       " 0.0080226074422486068,\n",
       " 0.0080036414672078286,\n",
       " 0.0079657095171262755,\n",
       " 0.0079657095171262755,\n",
       " 0.0079467435420854989,\n",
       " 0.0079467435420854989,\n",
       " 0.0079467435420854989,\n",
       " 0.0079277775670447224,\n",
       " 0.0078898456169631676,\n",
       " 0.0078898456169631676,\n",
       " 0.007870879641922391,\n",
       " 0.0078139817168000614,\n",
       " 0.0078139817168000614,\n",
       " 0.0077760497667185074,\n",
       " 0.00775708379167773,\n",
       " 0.0077381178166369535,\n",
       " 0.007719151841596176,\n",
       " 0.0077001858665553995,\n",
       " 0.007681219891514623,\n",
       " 0.007681219891514623,\n",
       " 0.007605355991351515,\n",
       " 0.0075863900163107385,\n",
       " 0.0075863900163107385,\n",
       " 0.007567424041269962,\n",
       " 0.0075484580662291845,\n",
       " 0.0075484580662291845,\n",
       " 0.007529492091188408,\n",
       " 0.007491560141106854,\n",
       " 0.0074725941660660775,\n",
       " 0.0074536281910253009,\n",
       " 0.0074346622159845235,\n",
       " 0.0074346622159845235,\n",
       " 0.007415696240943747,\n",
       " 0.007415696240943747,\n",
       " 0.0073587983158214165,\n",
       " 0.0073208663657398625,\n",
       " 0.007301900390699086,\n",
       " 0.0072450024655767555,\n",
       " 0.0072450024655767555,\n",
       " 0.0072260364905359781,\n",
       " 0.0072070705154952015,\n",
       " 0.0072070705154952015,\n",
       " 0.0072070705154952015,\n",
       " 0.0071691385654136476,\n",
       " 0.007150172590372871,\n",
       " 0.0071122406402913171,\n",
       " 0.007074308690209764,\n",
       " 0.0070553427151689866,\n",
       " 0.00703637674012821,\n",
       " 0.0070174107650874335,\n",
       " 0.0069984447900466561,\n",
       " 0.0069794788150058795,\n",
       " 0.0069794788150058795,\n",
       " 0.006960512839965103,\n",
       " 0.0069415468649243256,\n",
       " 0.006922580889883549,\n",
       " 0.0068846489398019951,\n",
       " 0.0068846489398019951,\n",
       " 0.0068656829647612185,\n",
       " 0.0068656829647612185,\n",
       " 0.0068277510146796646,\n",
       " 0.006808785039638888,\n",
       " 0.0067329211394757801,\n",
       " 0.0067329211394757801,\n",
       " 0.006694989189394227,\n",
       " 0.006694989189394227,\n",
       " 0.006694989189394227,\n",
       " 0.006694989189394227,\n",
       " 0.0066570572393126731,\n",
       " 0.0066380912642718965,\n",
       " 0.0066380912642718965,\n",
       " 0.0066380912642718965,\n",
       " 0.0066191252892311191,\n",
       " 0.0065622273641087886,\n",
       " 0.0065432613890680121,\n",
       " 0.0065432613890680121,\n",
       " 0.0065242954140272355,\n",
       " 0.0065053294389864581,\n",
       " 0.0064863634639456815,\n",
       " 0.006467397488904905,\n",
       " 0.0063725676137010205,\n",
       " 0.0063725676137010205,\n",
       " 0.0063346356636194666,\n",
       " 0.00631566968857869,\n",
       " 0.00631566968857869,\n",
       " 0.00631566968857869,\n",
       " 0.0062967037135379126,\n",
       " 0.0062967037135379126,\n",
       " 0.0062777377384971361,\n",
       " 0.0062587717634563595,\n",
       " 0.0062398057884155821,\n",
       " 0.0062398057884155821,\n",
       " 0.0062398057884155821,\n",
       " 0.0062208398133748056,\n",
       " 0.0062208398133748056,\n",
       " 0.0062208398133748056,\n",
       " 0.0061829078632932516,\n",
       " 0.0061829078632932516,\n",
       " 0.0061639418882524751,\n",
       " 0.0061639418882524751,\n",
       " 0.0061449759132116985,\n",
       " 0.0061449759132116985,\n",
       " 0.0061260099381709211,\n",
       " 0.0061260099381709211,\n",
       " 0.0061070439631301446,\n",
       " 0.0061070439631301446,\n",
       " 0.006088077988089368,\n",
       " 0.0060501460380078141,\n",
       " 0.0060501460380078141,\n",
       " 0.0060122140879262601,\n",
       " 0.0059932481128854836,\n",
       " 0.0059742821378447062,\n",
       " 0.0059742821378447062,\n",
       " 0.0059553161628039296,\n",
       " 0.0059363501877631531,\n",
       " 0.0059173842127223757,\n",
       " 0.0058984182376815991,\n",
       " 0.0058604862876000452,\n",
       " 0.0058604862876000452,\n",
       " 0.0058415203125592686,\n",
       " 0.0058415203125592686,\n",
       " 0.0057846223874369381,\n",
       " 0.0057846223874369381,\n",
       " 0.0057656564123961616,\n",
       " 0.0057656564123961616,\n",
       " 0.0057466904373553842,\n",
       " 0.0057277244623146076,\n",
       " 0.0056708265371922771,\n",
       " 0.0056708265371922771,\n",
       " 0.0056708265371922771,\n",
       " 0.0056708265371922771,\n",
       " 0.0056708265371922771,\n",
       " 0.0056518605621515006,\n",
       " 0.0056518605621515006,\n",
       " 0.0056518605621515006,\n",
       " 0.0056328945871107232,\n",
       " 0.0056328945871107232,\n",
       " 0.0056328945871107232,\n",
       " 0.0056328945871107232,\n",
       " 0.0055949626370291701,\n",
       " 0.0055949626370291701,\n",
       " 0.0055949626370291701,\n",
       " 0.0055949626370291701,\n",
       " 0.0055949626370291701,\n",
       " 0.0055759966619883927,\n",
       " 0.0055570306869476161,\n",
       " 0.0055570306869476161,\n",
       " 0.0055380647119068396,\n",
       " 0.0055380647119068396,\n",
       " 0.0055190987368660622,\n",
       " 0.0055001327618252856,\n",
       " 0.0055001327618252856,\n",
       " 0.0054811667867845082,\n",
       " 0.0054622008117437316,\n",
       " 0.0054622008117437316,\n",
       " 0.0054432348367029551,\n",
       " 0.0054053028866214011,\n",
       " 0.0053673709365398472,\n",
       " 0.0053673709365398472,\n",
       " 0.0053673709365398472,\n",
       " 0.0053484049614990706,\n",
       " 0.0053294389864582941,\n",
       " 0.0053104730114175167,\n",
       " 0.0053104730114175167,\n",
       " 0.0053104730114175167,\n",
       " 0.0052915070363767401,\n",
       " 0.0052915070363767401,\n",
       " 0.0052915070363767401,\n",
       " 0.0052725410613359636,\n",
       " 0.0052725410613359636,\n",
       " 0.0052725410613359636,\n",
       " 0.0052535750862951862,\n",
       " 0.0052346091112544096,\n",
       " 0.0052346091112544096,\n",
       " 0.0052156431362136331,\n",
       " 0.0051966771611728557,\n",
       " 0.0051966771611728557,\n",
       " 0.0051966771611728557,\n",
       " 0.0051777111861320791,\n",
       " 0.0051777111861320791,\n",
       " 0.0051587452110913026,\n",
       " 0.0051397792360505252,\n",
       " 0.0051397792360505252,\n",
       " 0.0051397792360505252,\n",
       " 0.0051018472859689721,\n",
       " 0.0051018472859689721,\n",
       " 0.0051018472859689721,\n",
       " 0.0050828813109281947,\n",
       " 0.0050639153358874181,\n",
       " 0.0050449493608466407,\n",
       " 0.0049880514357243102,\n",
       " 0.0049880514357243102,\n",
       " 0.0049690854606835337,\n",
       " 0.0049501194856427571,\n",
       " 0.0049501194856427571,\n",
       " 0.0049501194856427571,\n",
       " 0.0049501194856427571,\n",
       " 0.0049501194856427571,\n",
       " 0.0049121875355612032,\n",
       " 0.0049121875355612032,\n",
       " 0.0049121875355612032,\n",
       " 0.0049121875355612032,\n",
       " 0.0048932215605204266,\n",
       " 0.0048742555854796492,\n",
       " 0.0048552896104388727,\n",
       " 0.0048552896104388727,\n",
       " 0.0048363236353980961,\n",
       " 0.0048173576603573187,\n",
       " 0.0047604597352349882,\n",
       " 0.0047414937601942117,\n",
       " 0.0047414937601942117,\n",
       " 0.0047225277851534351,\n",
       " 0.0047225277851534351,\n",
       " 0.0047035618101126577,\n",
       " 0.0046845958350718812,\n",
       " 0.0046845958350718812,\n",
       " 0.0046466638849903272,\n",
       " 0.0046466638849903272,\n",
       " 0.0046466638849903272,\n",
       " 0.0046276979099495507,\n",
       " 0.0046276979099495507,\n",
       " 0.0046087319349087733,\n",
       " 0.0046087319349087733,\n",
       " 0.0045707999848272202,\n",
       " 0.0045707999848272202,\n",
       " 0.0045707999848272202,\n",
       " 0.0045707999848272202,\n",
       " 0.0045518340097864428,\n",
       " 0.0045518340097864428,\n",
       " 0.0045328680347456662,\n",
       " 0.0045328680347456662,\n",
       " 0.0045328680347456662,\n",
       " 0.0044949360846641122,\n",
       " 0.0044949360846641122,\n",
       " 0.0044759701096233357,\n",
       " 0.0044570041345825592,\n",
       " 0.0044570041345825592,\n",
       " 0.0044190721845010052,\n",
       " 0.0044190721845010052,\n",
       " 0.0044190721845010052,\n",
       " 0.0044001062094602287,\n",
       " 0.0043811402344194512,\n",
       " 0.0043811402344194512,\n",
       " 0.0043811402344194512,\n",
       " 0.0043811402344194512,\n",
       " 0.0043242423092971207,\n",
       " 0.0043052763342563442,\n",
       " 0.0042863103592155677,\n",
       " 0.0042863103592155677,\n",
       " 0.0042863103592155677,\n",
       " 0.0042863103592155677,\n",
       " 0.0042673443841747902,\n",
       " 0.0042483784091340137,\n",
       " 0.0042483784091340137,\n",
       " 0.0042294124340932372,\n",
       " 0.0042294124340932372,\n",
       " 0.0042104464590524597,\n",
       " 0.0042104464590524597,\n",
       " 0.0041914804840116832,\n",
       " 0.0041914804840116832,\n",
       " 0.0041914804840116832,\n",
       " 0.0041914804840116832,\n",
       " 0.0041914804840116832,\n",
       " 0.0041725145089709058,\n",
       " 0.0041725145089709058,\n",
       " 0.0041535485339301292,\n",
       " 0.0041535485339301292,\n",
       " 0.0041535485339301292,\n",
       " 0.0041345825588893527,\n",
       " 0.0041156165838485753,\n",
       " 0.0040966506088077987,\n",
       " 0.0040776846337670222,\n",
       " 0.0040587186587262448,\n",
       " 0.0040587186587262448,\n",
       " 0.0040397526836854682,\n",
       " 0.0040397526836854682,\n",
       " 0.0040397526836854682,\n",
       " 0.0040207867086446917,\n",
       " 0.0040207867086446917,\n",
       " 0.0040207867086446917,\n",
       " 0.0040018207336039143,\n",
       " 0.0040018207336039143,\n",
       " 0.0040018207336039143,\n",
       " 0.0039828547585631377,\n",
       " 0.0039638887835223612,\n",
       " 0.0039638887835223612,\n",
       " 0.0039638887835223612,\n",
       " 0.0039449228084815838,\n",
       " 0.0039449228084815838,\n",
       " 0.0039259568334408072,\n",
       " 0.0039259568334408072,\n",
       " 0.0039259568334408072,\n",
       " 0.0039259568334408072,\n",
       " 0.0039259568334408072,\n",
       " 0.0038880248833592537,\n",
       " 0.0038880248833592537,\n",
       " 0.0038880248833592537,\n",
       " 0.0038690589083184767,\n",
       " 0.0038500929332776998,\n",
       " 0.0038500929332776998,\n",
       " 0.0038500929332776998,\n",
       " 0.0038500929332776998,\n",
       " 0.0038311269582369228,\n",
       " 0.0038311269582369228,\n",
       " 0.0038121609831961462,\n",
       " 0.0038121609831961462,\n",
       " 0.0038121609831961462,\n",
       " 0.0038121609831961462,\n",
       " 0.0037931950081553692,\n",
       " 0.0037742290331145923,\n",
       " 0.0037742290331145923,\n",
       " 0.0037742290331145923,\n",
       " 0.0037742290331145923,\n",
       " 0.0037552630580738157,\n",
       " 0.0037552630580738157,\n",
       " 0.0037552630580738157,\n",
       " 0.0037552630580738157,\n",
       " 0.0037552630580738157,\n",
       " 0.0037362970830330387,\n",
       " 0.0037362970830330387,\n",
       " 0.0037362970830330387,\n",
       " 0.0037362970830330387,\n",
       " 0.0037362970830330387,\n",
       " 0.0037173311079922618,\n",
       " 0.0037173311079922618,\n",
       " 0.0036983651329514852,\n",
       " 0.0036793991579107082,\n",
       " 0.0036793991579107082,\n",
       " 0.0036793991579107082,\n",
       " 0.0036604331828699313,\n",
       " 0.0036604331828699313,\n",
       " 0.0036604331828699313,\n",
       " 0.0036604331828699313,\n",
       " 0.0036604331828699313,\n",
       " 0.0036414672078291543,\n",
       " 0.0036225012327883777,\n",
       " 0.0036225012327883777,\n",
       " 0.0036225012327883777,\n",
       " 0.0036225012327883777,\n",
       " 0.0036035352577476008,\n",
       " 0.0035845692827068238,\n",
       " 0.0035845692827068238,\n",
       " 0.0035656033076660472,\n",
       " 0.0035656033076660472,\n",
       " 0.0035656033076660472,\n",
       " 0.0035466373326252703,\n",
       " 0.0035466373326252703,\n",
       " 0.0035276713575844933,\n",
       " 0.0035087053825437167,\n",
       " 0.0035087053825437167,\n",
       " 0.0035087053825437167,\n",
       " 0.0035087053825437167,\n",
       " 0.0034897394075029398,\n",
       " 0.0034897394075029398,\n",
       " 0.0034707734324621628,\n",
       " 0.0034707734324621628,\n",
       " 0.0034518074574213862,\n",
       " 0.0034518074574213862,\n",
       " 0.0034518074574213862,\n",
       " 0.0034518074574213862,\n",
       " 0.0034328414823806093,\n",
       " 0.0034328414823806093,\n",
       " 0.0034328414823806093,\n",
       " 0.0034328414823806093,\n",
       " 0.0034328414823806093,\n",
       " 0.0034138755073398323,\n",
       " 0.0034138755073398323,\n",
       " 0.0034138755073398323,\n",
       " 0.0033949095322990553,\n",
       " 0.0033949095322990553,\n",
       " 0.0033949095322990553,\n",
       " 0.0033949095322990553,\n",
       " 0.0033759435572582788,\n",
       " 0.0033759435572582788,\n",
       " 0.0033759435572582788,\n",
       " 0.0033759435572582788,\n",
       " 0.0033759435572582788,\n",
       " 0.0033759435572582788,\n",
       " 0.0033569775822175018,\n",
       " 0.0033569775822175018,\n",
       " 0.0033569775822175018,\n",
       " 0.0033569775822175018,\n",
       " 0.0033569775822175018,\n",
       " 0.0033569775822175018,\n",
       " 0.0033569775822175018,\n",
       " 0.0033380116071767248,\n",
       " 0.0033190456321359483,\n",
       " 0.0033190456321359483,\n",
       " 0.0033190456321359483,\n",
       " 0.0033190456321359483,\n",
       " 0.0033190456321359483,\n",
       " 0.0033000796570951713,\n",
       " 0.0033000796570951713,\n",
       " 0.0033000796570951713,\n",
       " 0.0032811136820543943,\n",
       " 0.0032811136820543943,\n",
       " 0.0032811136820543943,\n",
       " 0.0032811136820543943,\n",
       " 0.0032811136820543943,\n",
       " 0.0032811136820543943,\n",
       " 0.0032621477070136178,\n",
       " 0.0032621477070136178,\n",
       " 0.0032621477070136178,\n",
       " 0.0032621477070136178,\n",
       " 0.0032431817319728408,\n",
       " 0.0032431817319728408,\n",
       " 0.0032242157569320638,\n",
       " 0.0032242157569320638,\n",
       " 0.0032242157569320638,\n",
       " 0.0032242157569320638,\n",
       " 0.0032242157569320638,\n",
       " 0.0032052497818912868,\n",
       " 0.0032052497818912868,\n",
       " 0.0032052497818912868,\n",
       " 0.0031862838068505103,\n",
       " 0.0031673178318097333,\n",
       " 0.0031673178318097333,\n",
       " 0.0031483518567689563,\n",
       " 0.0031483518567689563,\n",
       " 0.0031483518567689563,\n",
       " 0.0031483518567689563,\n",
       " 0.0031483518567689563,\n",
       " 0.0031293858817281798,\n",
       " 0.0031293858817281798,\n",
       " 0.0031293858817281798,\n",
       " 0.0031104199066874028,\n",
       " 0.0030914539316466258,\n",
       " 0.0030914539316466258,\n",
       " 0.0030724879566058493,\n",
       " 0.0030724879566058493,\n",
       " 0.0030724879566058493,\n",
       " 0.0030724879566058493,\n",
       " 0.0030724879566058493,\n",
       " 0.0030535219815650723,\n",
       " 0.0030535219815650723,\n",
       " 0.0030345560065242953,\n",
       " 0.0030345560065242953,\n",
       " 0.0030345560065242953,\n",
       " 0.0030155900314835188,\n",
       " 0.0029966240564427418,\n",
       " 0.0029966240564427418,\n",
       " 0.0029966240564427418,\n",
       " 0.0029966240564427418,\n",
       " 0.0029966240564427418,\n",
       " 0.0029966240564427418,\n",
       " 0.0029966240564427418,\n",
       " 0.0029776580814019648,\n",
       " 0.0029776580814019648,\n",
       " 0.0029586921063611878,\n",
       " 0.0029586921063611878,\n",
       " 0.0029586921063611878,\n",
       " 0.0029586921063611878,\n",
       " 0.0029586921063611878,\n",
       " 0.0029586921063611878,\n",
       " 0.0029586921063611878,\n",
       " 0.0029397261313204113,\n",
       " 0.0029397261313204113,\n",
       " 0.0029397261313204113,\n",
       " 0.0029207601562796343,\n",
       " 0.0029207601562796343,\n",
       " 0.0029207601562796343,\n",
       " 0.0029207601562796343,\n",
       " 0.0029017941812388573,\n",
       " 0.0029017941812388573,\n",
       " 0.0029017941812388573,\n",
       " 0.0029017941812388573,\n",
       " 0.0029017941812388573,\n",
       " 0.0028638622311573038,\n",
       " 0.0028448962561165268,\n",
       " 0.0028448962561165268,\n",
       " 0.0028448962561165268,\n",
       " 0.0028259302810757503,\n",
       " 0.0028259302810757503,\n",
       " 0.0028069643060349733,\n",
       " 0.0028069643060349733,\n",
       " 0.0028069643060349733,\n",
       " 0.0028069643060349733,\n",
       " 0.0028069643060349733,\n",
       " 0.0028069643060349733,\n",
       " 0.0028069643060349733,\n",
       " 0.0028069643060349733,\n",
       " 0.0027879983309941963,\n",
       " 0.0027879983309941963,\n",
       " 0.0027879983309941963,\n",
       " 0.0027879983309941963,\n",
       " 0.0027879983309941963,\n",
       " 0.0027690323559534198,\n",
       " 0.0027500663809126428,\n",
       " 0.0027500663809126428,\n",
       " 0.0027500663809126428,\n",
       " 0.0027500663809126428,\n",
       " 0.0027500663809126428,\n",
       " 0.0027311004058718658,\n",
       " 0.0027311004058718658,\n",
       " 0.0027311004058718658,\n",
       " 0.0027311004058718658,\n",
       " 0.0027311004058718658,\n",
       " 0.0027121344308310888,\n",
       " 0.0027121344308310888,\n",
       " 0.0026931684557903123,\n",
       " 0.0026931684557903123,\n",
       " 0.0026931684557903123,\n",
       " 0.0026931684557903123,\n",
       " 0.0026931684557903123,\n",
       " 0.0026931684557903123,\n",
       " 0.0026931684557903123,\n",
       " 0.0026742024807495353,\n",
       " 0.0026742024807495353,\n",
       " 0.0026552365057087583,\n",
       " 0.0026552365057087583,\n",
       " 0.0026552365057087583,\n",
       " 0.0026552365057087583,\n",
       " 0.0026362705306679818,\n",
       " 0.0026362705306679818,\n",
       " 0.0026362705306679818,\n",
       " 0.0026362705306679818,\n",
       " 0.0026362705306679818,\n",
       " 0.0026362705306679818,\n",
       " 0.0026362705306679818,\n",
       " 0.0026362705306679818,\n",
       " 0.0026362705306679818,\n",
       " 0.0026362705306679818,\n",
       " 0.0026173045556272048,\n",
       " 0.0026173045556272048,\n",
       " 0.0026173045556272048,\n",
       " 0.0026173045556272048,\n",
       " 0.0026173045556272048,\n",
       " 0.0026173045556272048,\n",
       " 0.0025983385805864278,\n",
       " 0.0025983385805864278,\n",
       " 0.0025983385805864278,\n",
       " 0.0025983385805864278,\n",
       " 0.0025983385805864278,\n",
       " 0.0025983385805864278,\n",
       " 0.0025793726055456513,\n",
       " 0.0025793726055456513,\n",
       " 0.0025793726055456513,\n",
       " 0.0025793726055456513,\n",
       " 0.0025793726055456513,\n",
       " 0.0025793726055456513,\n",
       " 0.0025604066305048743,\n",
       " 0.0025604066305048743,\n",
       " 0.0025604066305048743,\n",
       " 0.0025604066305048743,\n",
       " 0.0025414406554640973,\n",
       " 0.0025414406554640973,\n",
       " 0.0025414406554640973,\n",
       " 0.0025414406554640973,\n",
       " 0.0025414406554640973,\n",
       " 0.0025414406554640973,\n",
       " 0.0025224746804233204,\n",
       " 0.0025224746804233204,\n",
       " 0.0025224746804233204,\n",
       " 0.0025224746804233204,\n",
       " 0.0025224746804233204,\n",
       " 0.0025035087053825438,\n",
       " 0.0025035087053825438,\n",
       " 0.0025035087053825438,\n",
       " 0.0025035087053825438,\n",
       " 0.0025035087053825438,\n",
       " 0.0024845427303417668,\n",
       " 0.0024845427303417668,\n",
       " 0.0024845427303417668,\n",
       " 0.0024845427303417668,\n",
       " 0.0024845427303417668,\n",
       " 0.0024845427303417668,\n",
       " 0.0024655767553009899,\n",
       " 0.0024655767553009899,\n",
       " 0.0024655767553009899,\n",
       " 0.0024655767553009899,\n",
       " 0.0024655767553009899,\n",
       " 0.0024655767553009899,\n",
       " 0.0024655767553009899,\n",
       " 0.0024655767553009899,\n",
       " 0.0024466107802602133,\n",
       " 0.0024466107802602133,\n",
       " 0.0024466107802602133,\n",
       " 0.0024466107802602133,\n",
       " 0.0024466107802602133,\n",
       " 0.0024466107802602133,\n",
       " 0.0024466107802602133,\n",
       " 0.0024276448052194363,\n",
       " 0.0024276448052194363,\n",
       " 0.0024276448052194363,\n",
       " 0.0024276448052194363,\n",
       " 0.0024276448052194363,\n",
       " 0.0024086788301786594,\n",
       " 0.0024086788301786594,\n",
       " 0.0023897128551378828,\n",
       " 0.0023897128551378828,\n",
       " 0.0023897128551378828,\n",
       " 0.0023897128551378828,\n",
       " 0.0023897128551378828,\n",
       " 0.0023707468800971058,\n",
       " 0.0023707468800971058,\n",
       " 0.0023517809050563289,\n",
       " 0.0023517809050563289,\n",
       " 0.0023517809050563289,\n",
       " 0.0023517809050563289,\n",
       " 0.0023517809050563289,\n",
       " 0.0023517809050563289,\n",
       " 0.0023517809050563289,\n",
       " 0.0023517809050563289,\n",
       " 0.0023517809050563289,\n",
       " 0.0023517809050563289,\n",
       " 0.0023328149300155523,\n",
       " 0.0023328149300155523,\n",
       " 0.0023328149300155523,\n",
       " 0.0023328149300155523,\n",
       " 0.0023328149300155523,\n",
       " 0.0023328149300155523,\n",
       " 0.0023138489549747753,\n",
       " 0.0023138489549747753,\n",
       " 0.0023138489549747753,\n",
       " 0.0023138489549747753,\n",
       " 0.0023138489549747753,\n",
       " 0.0023138489549747753,\n",
       " 0.0022948829799339984,\n",
       " 0.0022948829799339984,\n",
       " 0.0022948829799339984,\n",
       " 0.0022948829799339984,\n",
       " 0.0022948829799339984,\n",
       " 0.0022948829799339984,\n",
       " 0.0022948829799339984,\n",
       " 0.0022948829799339984,\n",
       " 0.0022759170048932214,\n",
       " 0.0022759170048932214,\n",
       " ...]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(freqs_idx, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('count of 4500th most common code: ', 6.0)\n",
      "('mean count, frequency of top 4500: ', 2.2336047037539575, 4.236249106235932e-05)\n",
      "('mean count, frequency of bottom rest: ', 186.40333333333334, 0.003535320967517607)\n"
     ]
    }
   ],
   "source": [
    "top = 4500\n",
    "freq_sorted = sorted(freqs_idx, reverse=True)\n",
    "print(\"count of %dth most common code: \" % top, freq_sorted[top-1]*n)\n",
    "print(\"mean count, frequency of top %d: \" % top, n*np.mean(freq_sorted[top:]), np.mean(freq_sorted[top:]))\n",
    "print(\"mean count, frequency of bottom rest: \", n*np.mean(freq_sorted[:top]), np.mean(freq_sorted[:top]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011258265875800907"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#figure out k for flattened/'normalized' sampling\n",
    "exp = 1./2\n",
    "freqs_idx_exp = [math.pow(f, exp) for f in freq_sorted[2500:]]\n",
    "k = sum(freq_sorted[2500:]) / sum(freqs_idx_exp)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.096271289306983"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(freqs.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-evaluate and get @k values for several k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dir = '%s/conv_attn_reg_spearmint_best_mimic_3_full_final' % MODEL_DIR\n",
    "Y = 'full'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_labels = tools.get_num_labels(Y, version='mimic3', h_train=False)\n",
    "\n",
    "dicts = datasets.load_lookups('%s/train_full.csv' % DISCH_DIR, '%s/vocab.csv' % DISCH_DIR, Y=Y, version='mimic3')\n",
    "ind2c, c2ind = dicts[2], dicts[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #rig up an args object to pass into the training script\n",
    "# #re-evaluate if we have to\n",
    "# reload(training)\n",
    "# args = argparse.Namespace()\n",
    "# args.vocab = '%s/vocab_no_stopwords.csv' % DISCH_DIR\n",
    "# args.embed_file = '%s/processed_full.embed' % DISCH_DIR\n",
    "# args.Y = 5000\n",
    "\n",
    "# args.vocab_min = 3\n",
    "# args.model = 'saved'\n",
    "# args.n_epochs = 1\n",
    "# args.objective = 'bce'\n",
    "# args.saved_dir = model_dir\n",
    "# args.data_path = '%s/train_5000_disch_smaller.csv' % DISCH_DIR\n",
    "# args.gpu = True\n",
    "# args.testing = True\n",
    "# args.samples = False\n",
    "# args.mimic = 3\n",
    "# args.desc_embed = False\n",
    "# args.h_train = False\n",
    "# args.weight_decay=0\n",
    "# args.lr = 0.001\n",
    "\n",
    "# training.main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3372it [00:52, 64.47it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('%s/pred_100_scores_test.json' % model_dir, 'r') as f:\n",
    "    scors = json.load(f)\n",
    "\n",
    "golds = defaultdict(lambda: [])\n",
    "with open('%s/test_%s.csv' % (DISCH_DIR, str(Y)), 'r') as f:\n",
    "    r = csv.reader(f)\n",
    "    #header\n",
    "    next(r)\n",
    "    for row in r:\n",
    "        codes = set([str(c2ind[c]) for c in row[3].split(';')])\n",
    "        golds[row[1]] = codes\n",
    "\n",
    "hadm_ids = sorted(set(golds.keys()).intersection(set(scors.keys())))\n",
    "\n",
    "#rebuild prediction / ground truth matrices\n",
    "yhat_raw = np.zeros((len(hadm_ids), num_labels))\n",
    "y = np.zeros((len(hadm_ids), num_labels))\n",
    "for i,hadm_id in tqdm(enumerate(hadm_ids)):\n",
    "    yhat_raw_inds = [scors[hadm_id][str(j)] if str(j) in scors[hadm_id] else 0 for j in range(num_labels)]\n",
    "    gold_inds = [1 if str(j) in golds[hadm_id] else 0 for j in range(num_labels)]\n",
    "    yhat_raw[i] = yhat_raw_inds\n",
    "    y[i] = gold_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78448823903369358"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(evaluation)\n",
    "evaluation.precision_at_k(yhat_raw, y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('%s/train_50_smaller.csv' % DISCH_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['num_codes'] = df.apply(lambda row: len(row['LABELS'].split(';')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    8066.000000\n",
       "mean        5.692909\n",
       "std         3.320199\n",
       "min         1.000000\n",
       "25%         3.000000\n",
       "50%         5.000000\n",
       "75%         8.000000\n",
       "max        24.000000\n",
       "Name: num_codes, dtype: float64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['num_codes'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:18<00:00,  1.00it/s]\n",
      "100%|██████████| 19/19 [00:20<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "#put together data for plotting\n",
    "reload(evaluation)\n",
    "ks = range(1,20)\n",
    "prec_at_ks = [evaluation.precision_at_k(yhat_raw, y, k) for k in tqdm(ks)]\n",
    "rec_at_ks = [evaluation.recall_at_k(yhat_raw, y, k) for k in tqdm(ks)]\n",
    "\n",
    "f1_at_ks = [2*(prec*rec)/(prec+rec) for prec,rec in zip(prec_at_ks, rec_at_ks)]\n",
    "\n",
    "which_ks = [1, 8, 15]\n",
    "labels_prec = [('k=%d: %.4f' % (k, prec_at_ks[k-1]), k, prec_at_ks[k-1]) for k in which_ks]\n",
    "labels_rec = [('k=%d: %.4f' % (k, rec_at_ks[k-1]), k, rec_at_ks[k-1]) for k in which_ks]\n",
    "labels_f1 = [('k=%d: %.4f' % (k, f1_at_ks[k-1]), k, f1_at_ks[k-1]) for k in which_ks + [15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_at_ks[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f29b7f0ee90>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtQAAAJQCAYAAACuDPM+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSVIZ/gOmeSMt+2+ZHgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f29cd375450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot, with legend and annotations\n",
    "plt.figure(figsize=(12,10))\n",
    "p_prec, = plt.plot(ks, prec_at_ks, 'bo', label='precision@k')\n",
    "p_rec, = plt.plot(ks, rec_at_ks, 'ro', label='recall@k')\n",
    "p_f1, = plt.plot(ks, f1_at_ks, 'ko', label='f1@k')\n",
    "#annotate\n",
    "for label, x, y in labels_prec:\n",
    "    plt.plot(x, y, 'g*')\n",
    "    plt.annotate(label, xy=(x,y), xytext=(x+1,y), \n",
    "        bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "        arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
    "for label, x, y in labels_rec:\n",
    "    plt.plot(x, y, 'g*')\n",
    "    plt.annotate(label, xy=(x,y), xytext=(x+1,y),\n",
    "        bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "        arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
    "for label, x, y in labels_f1:\n",
    "    plt.plot(x, y, 'g*')\n",
    "    plt.annotate(label, xy=(x,y), xytext=(x+1,y),\n",
    "        bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "        arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
    "plt.grid(True)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('value')\n",
    "plt.legend(handles=[p_prec, p_rec, p_f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Instance-based metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mdir = '%s/conv_attn_Dec_02_22:44' % MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = 50\n",
    "num_labels = tools.get_num_labels(Y, version='mimic3', h_train=False)\n",
    "\n",
    "dicts = datasets.load_lookups('%s/train_%s.csv' % (DISCH_DIR, str(Y)), '%s/vocab.csv' % DISCH_DIR,\n",
    "                              Y=Y, version='mimic3')\n",
    "ind2c, c2ind = dicts[2], dicts[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freqs, n = datasets.load_code_freqs('%s/train_%s.csv' % (DISCH_DIR, str(Y)), version='mimic3')\n",
    "freq_list = sorted(freqs.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "freq2ind = defaultdict(int)\n",
    "for i, (code, freq) in enumerate(freq_list):\n",
    "    freq2ind[i] = c2ind[code]\n",
    "ind2freq = defaultdict(str, {c:i for i,c in freq2ind.iteritems()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind2freq[c2ind['401.9']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3241it [00:00, 26555.15it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('%s/preds_test.csv' % mdir, 'r') as f:\n",
    "    r = csv.reader(f, delimiter='|')\n",
    "    for row in r:\n",
    "        if len(row) > 1:\n",
    "            preds[row[0]] = set([ind2freq[int(ind)] for ind in row[1:] if ind != ''])\n",
    "        else:\n",
    "            preds[row[0]] = set([])\n",
    "\n",
    "golds = defaultdict(lambda: [])\n",
    "with open('%s/test_%s.csv' % (DISCH_DIR, str(Y)), 'r') as f:\n",
    "    r = csv.reader(f)\n",
    "    #header\n",
    "    next(r)\n",
    "    for row in r:\n",
    "        codes = set([ind2freq[c2ind[c]] for c in row[3].split(';')])\n",
    "        golds[row[1]] = codes\n",
    "\n",
    "hadm_ids = sorted(set(golds.keys()).intersection(set(preds.keys())))\n",
    "\n",
    "#rebuild prediction / ground truth matrices\n",
    "yhat = np.zeros((len(hadm_ids), num_labels))\n",
    "y = np.zeros((len(hadm_ids), num_labels))\n",
    "for i,hadm_id in tqdm(enumerate(hadm_ids)):\n",
    "    yhat_inds = [1 if j in preds[hadm_id] else 0 for j in range(num_labels)]\n",
    "    gold_inds = [1 if j in golds[hadm_id] else 0 for j in range(num_labels)]\n",
    "    yhat[i] = yhat_inds\n",
    "    y[i] = gold_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65259212169869718"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(evaluation)\n",
    "evaluation.inst_f1(yhat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08730638691761802"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamming_loss(y, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting the list of relevant subjects\n",
      "processing notes file\n",
      "writing to /nethome/jmullenbach3/mimicdata/disch/disch_100.csv\n",
      "0\n",
      "0 disch, 0 not disch\n",
      "10000\n",
      "9816 disch, 0 not disch\n",
      "20000\n",
      "19439 disch, 0 not disch\n",
      "30000\n",
      "29005 disch, 0 not disch\n",
      "40000\n",
      "38629 disch, 0 not disch\n",
      "50000\n",
      "48316 disch, 0 not disch\n",
      "60000\n",
      "57743 disch, 393 not disch\n",
      "70000\n",
      "57788 disch, 10273 not disch\n",
      "80000\n",
      "57788 disch, 20153 not disch\n",
      "90000\n",
      "57788 disch, 30057 not disch\n",
      "100000\n",
      "57788 disch, 40005 not disch\n",
      "110000\n",
      "57788 disch, 49965 not disch\n",
      "120000\n",
      "57788 disch, 59921 not disch\n",
      "130000\n",
      "57788 disch, 69895 not disch\n",
      "140000\n",
      "57788 disch, 79842 not disch\n",
      "150000\n",
      "57788 disch, 89739 not disch\n",
      "160000\n",
      "57788 disch, 99643 not disch\n",
      "170000\n",
      "57788 disch, 109564 not disch\n",
      "180000\n",
      "57788 disch, 119475 not disch\n",
      "190000\n",
      "57788 disch, 129343 not disch\n",
      "200000\n",
      "57788 disch, 139192 not disch\n",
      "210000\n",
      "57788 disch, 149071 not disch\n",
      "220000\n",
      "57788 disch, 158936 not disch\n",
      "230000\n",
      "57788 disch, 168793 not disch\n",
      "240000\n",
      "57788 disch, 178610 not disch\n",
      "250000\n",
      "57788 disch, 188493 not disch\n",
      "260000\n",
      "57788 disch, 198410 not disch\n",
      "270000\n",
      "57788 disch, 208353 not disch\n",
      "280000\n",
      "57788 disch, 218294 not disch\n",
      "290000\n",
      "57788 disch, 228251 not disch\n",
      "300000\n",
      "57788 disch, 238185 not disch\n",
      "310000\n",
      "57788 disch, 248167 not disch\n",
      "320000\n",
      "57788 disch, 258118 not disch\n",
      "330000\n",
      "57788 disch, 268054 not disch\n",
      "340000\n",
      "57788 disch, 277948 not disch\n",
      "350000\n",
      "57788 disch, 287915 not disch\n",
      "360000\n",
      "57788 disch, 297915 not disch\n",
      "370000\n",
      "57788 disch, 307894 not disch\n",
      "380000\n",
      "57788 disch, 317878 not disch\n",
      "390000\n",
      "57788 disch, 327878 not disch\n",
      "400000\n",
      "57788 disch, 337878 not disch\n",
      "410000\n",
      "57788 disch, 347835 not disch\n",
      "420000\n",
      "57788 disch, 357564 not disch\n",
      "430000\n",
      "57788 disch, 367360 not disch\n",
      "440000\n",
      "57788 disch, 377189 not disch\n",
      "450000\n",
      "57788 disch, 387041 not disch\n",
      "460000\n",
      "57788 disch, 396903 not disch\n",
      "470000\n",
      "57788 disch, 406786 not disch\n",
      "480000\n",
      "57788 disch, 416647 not disch\n",
      "490000\n",
      "57788 disch, 426552 not disch\n",
      "500000\n",
      "57788 disch, 436489 not disch\n",
      "510000\n",
      "57788 disch, 446404 not disch\n",
      "520000\n",
      "57788 disch, 456321 not disch\n",
      "530000\n",
      "57788 disch, 466095 not disch\n",
      "540000\n",
      "57788 disch, 475904 not disch\n",
      "550000\n",
      "57788 disch, 485761 not disch\n",
      "560000\n",
      "57788 disch, 495696 not disch\n",
      "570000\n",
      "57788 disch, 505619 not disch\n",
      "580000\n",
      "57788 disch, 515518 not disch\n",
      "590000\n",
      "57788 disch, 525433 not disch\n",
      "600000\n",
      "57788 disch, 535367 not disch\n",
      "610000\n",
      "57788 disch, 545210 not disch\n",
      "620000\n",
      "57788 disch, 555052 not disch\n",
      "630000\n",
      "57788 disch, 564887 not disch\n",
      "640000\n",
      "57788 disch, 574813 not disch\n",
      "650000\n",
      "57788 disch, 584622 not disch\n",
      "660000\n",
      "57788 disch, 594519 not disch\n",
      "670000\n",
      "57788 disch, 604344 not disch\n",
      "680000\n",
      "57788 disch, 614188 not disch\n",
      "690000\n",
      "57788 disch, 624116 not disch\n",
      "700000\n",
      "57788 disch, 633773 not disch\n",
      "710000\n",
      "57788 disch, 643710 not disch\n",
      "720000\n",
      "57788 disch, 653623 not disch\n",
      "730000\n",
      "57788 disch, 663549 not disch\n",
      "740000\n",
      "57788 disch, 673061 not disch\n",
      "750000\n",
      "57788 disch, 682592 not disch\n",
      "760000\n",
      "57788 disch, 692022 not disch\n",
      "770000\n",
      "57788 disch, 701465 not disch\n",
      "780000\n",
      "57788 disch, 710821 not disch\n",
      "790000\n",
      "57788 disch, 720311 not disch\n",
      "800000\n",
      "57788 disch, 729832 not disch\n",
      "810000\n",
      "57788 disch, 739420 not disch\n",
      "820000\n",
      "57788 disch, 749027 not disch\n",
      "830000\n",
      "57788 disch, 758709 not disch\n",
      "840000\n",
      "57788 disch, 768285 not disch\n",
      "850000\n",
      "57788 disch, 777999 not disch\n",
      "860000\n",
      "57788 disch, 787640 not disch\n",
      "870000\n",
      "57788 disch, 797313 not disch\n",
      "880000\n",
      "57788 disch, 806920 not disch\n",
      "890000\n",
      "57788 disch, 816702 not disch\n",
      "900000\n",
      "57788 disch, 826439 not disch\n",
      "910000\n",
      "57788 disch, 836173 not disch\n",
      "920000\n",
      "57788 disch, 845847 not disch\n",
      "930000\n",
      "57788 disch, 855522 not disch\n",
      "940000\n",
      "57788 disch, 865233 not disch\n",
      "950000\n",
      "57788 disch, 874802 not disch\n",
      "960000\n",
      "57788 disch, 884462 not disch\n",
      "970000\n",
      "57788 disch, 894152 not disch\n",
      "980000\n",
      "57788 disch, 903909 not disch\n",
      "990000\n",
      "57788 disch, 913634 not disch\n",
      "1000000\n",
      "57788 disch, 923415 not disch\n",
      "1010000\n",
      "57788 disch, 933165 not disch\n",
      "1020000\n",
      "57788 disch, 942891 not disch\n",
      "1030000\n",
      "57788 disch, 952645 not disch\n",
      "1040000\n",
      "57788 disch, 962400 not disch\n",
      "1050000\n",
      "57788 disch, 972122 not disch\n",
      "1060000\n",
      "57788 disch, 981927 not disch\n",
      "1070000\n",
      "57788 disch, 991701 not disch\n",
      "1080000\n",
      "57788 disch, 1001374 not disch\n",
      "1090000\n",
      "57788 disch, 1011205 not disch\n",
      "1100000\n",
      "57788 disch, 1020959 not disch\n",
      "1110000\n",
      "57788 disch, 1030763 not disch\n",
      "1120000\n",
      "57788 disch, 1040534 not disch\n",
      "1130000\n",
      "57788 disch, 1050274 not disch\n",
      "1140000\n",
      "57788 disch, 1059947 not disch\n",
      "1150000\n",
      "57788 disch, 1069686 not disch\n",
      "1160000\n",
      "57788 disch, 1079516 not disch\n",
      "1170000\n",
      "57788 disch, 1089285 not disch\n",
      "1180000\n",
      "57788 disch, 1099047 not disch\n",
      "1190000\n",
      "57788 disch, 1108826 not disch\n",
      "1200000\n",
      "57788 disch, 1118584 not disch\n",
      "1210000\n",
      "57788 disch, 1128375 not disch\n",
      "1220000\n",
      "57788 disch, 1138139 not disch\n",
      "1230000\n",
      "57788 disch, 1147946 not disch\n",
      "1240000\n",
      "57788 disch, 1157773 not disch\n",
      "1250000\n",
      "57788 disch, 1167588 not disch\n",
      "1260000\n",
      "57788 disch, 1177401 not disch\n",
      "1270000\n",
      "57788 disch, 1187202 not disch\n",
      "1280000\n",
      "57788 disch, 1196914 not disch\n",
      "1290000\n",
      "57788 disch, 1206705 not disch\n",
      "1300000\n",
      "57788 disch, 1216449 not disch\n",
      "1310000\n",
      "57788 disch, 1226217 not disch\n",
      "1320000\n",
      "57788 disch, 1235986 not disch\n",
      "1330000\n",
      "57788 disch, 1245853 not disch\n",
      "1340000\n",
      "57788 disch, 1255587 not disch\n",
      "1350000\n",
      "57788 disch, 1265440 not disch\n",
      "1360000\n",
      "57788 disch, 1275288 not disch\n",
      "1370000\n",
      "57788 disch, 1285085 not disch\n",
      "1380000\n",
      "57788 disch, 1294953 not disch\n",
      "1390000\n",
      "57788 disch, 1304791 not disch\n",
      "1400000\n",
      "57788 disch, 1314599 not disch\n",
      "1410000\n",
      "57788 disch, 1324379 not disch\n",
      "1420000\n",
      "57788 disch, 1334154 not disch\n",
      "1430000\n",
      "57788 disch, 1344004 not disch\n",
      "1440000\n",
      "57788 disch, 1353808 not disch\n",
      "1450000\n",
      "57788 disch, 1363591 not disch\n",
      "1460000\n",
      "57788 disch, 1373382 not disch\n",
      "1470000\n",
      "57788 disch, 1383278 not disch\n",
      "1480000\n",
      "57788 disch, 1393173 not disch\n",
      "1490000\n",
      "57788 disch, 1402984 not disch\n",
      "1500000\n",
      "57788 disch, 1412831 not disch\n",
      "1510000\n",
      "57788 disch, 1422657 not disch\n",
      "1520000\n",
      "57788 disch, 1432455 not disch\n",
      "1530000\n",
      "57788 disch, 1442206 not disch\n",
      "1540000\n",
      "57788 disch, 1451950 not disch\n",
      "1550000\n",
      "57788 disch, 1461616 not disch\n",
      "1560000\n",
      "57788 disch, 1471302 not disch\n",
      "1570000\n",
      "57788 disch, 1481165 not disch\n",
      "1580000\n",
      "57788 disch, 1490977 not disch\n",
      "1590000\n",
      "57788 disch, 1500687 not disch\n",
      "1600000\n",
      "57788 disch, 1510508 not disch\n",
      "1610000\n",
      "57788 disch, 1520320 not disch\n",
      "1620000\n",
      "57788 disch, 1530137 not disch\n",
      "1630000\n",
      "57788 disch, 1539831 not disch\n",
      "1640000\n",
      "57788 disch, 1549695 not disch\n",
      "1650000\n",
      "57788 disch, 1559467 not disch\n",
      "1660000\n",
      "57788 disch, 1569256 not disch\n",
      "1670000\n",
      "57788 disch, 1579250 not disch\n",
      "1680000\n",
      "57788 disch, 1589179 not disch\n",
      "1690000\n",
      "57788 disch, 1599152 not disch\n",
      "1700000\n",
      "57788 disch, 1609082 not disch\n",
      "1710000\n",
      "57788 disch, 1619058 not disch\n",
      "1720000\n",
      "57788 disch, 1629038 not disch\n",
      "1730000\n",
      "57788 disch, 1639019 not disch\n",
      "1740000\n",
      "57788 disch, 1648993 not disch\n",
      "1750000\n",
      "57788 disch, 1658979 not disch\n",
      "1760000\n",
      "57788 disch, 1668979 not disch\n",
      "1770000\n",
      "57788 disch, 1678848 not disch\n",
      "1780000\n",
      "57788 disch, 1688837 not disch\n",
      "1790000\n",
      "57788 disch, 1698655 not disch\n",
      "1800000\n",
      "57788 disch, 1708607 not disch\n",
      "1810000\n",
      "57788 disch, 1718600 not disch\n",
      "1820000\n",
      "57788 disch, 1728583 not disch\n",
      "1830000\n",
      "57788 disch, 1738540 not disch\n",
      "1840000\n",
      "57788 disch, 1748460 not disch\n",
      "1850000\n",
      "57788 disch, 1758316 not disch\n",
      "1860000\n",
      "57788 disch, 1768288 not disch\n",
      "1870000\n",
      "57788 disch, 1778135 not disch\n",
      "1880000\n",
      "57788 disch, 1788120 not disch\n",
      "1890000\n",
      "57788 disch, 1798090 not disch\n",
      "1900000\n",
      "57788 disch, 1807900 not disch\n",
      "1910000\n",
      "57788 disch, 1817871 not disch\n",
      "1920000\n",
      "57788 disch, 1827837 not disch\n",
      "1930000\n",
      "57788 disch, 1837805 not disch\n",
      "1940000\n",
      "57788 disch, 1847559 not disch\n",
      "1950000\n",
      "57788 disch, 1857409 not disch\n",
      "1960000\n",
      "57788 disch, 1867345 not disch\n",
      "1970000\n",
      "57788 disch, 1877323 not disch\n",
      "1980000\n",
      "57788 disch, 1887272 not disch\n",
      "1990000\n",
      "57788 disch, 1897237 not disch\n",
      "2000000\n",
      "57788 disch, 1907209 not disch\n",
      "2010000\n",
      "57788 disch, 1917209 not disch\n",
      "2020000\n",
      "57788 disch, 1927205 not disch\n",
      "2030000\n",
      "57788 disch, 1937155 not disch\n",
      "2040000\n",
      "57788 disch, 1947090 not disch\n",
      "2050000\n",
      "57788 disch, 1957083 not disch\n",
      "2060000\n",
      "57788 disch, 1967079 not disch\n",
      "2070000\n",
      "57788 disch, 1977055 not disch\n",
      "2080000\n",
      "57788 disch, 1986999 not disch\n",
      "reading data from /nethome/jmullenbach3/mimicdata/disch/disch_100.csv\n",
      "dropping if no HADM_ID or TEXT\n",
      "sorting\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'sort'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-d2a4614eed8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m tr_file, dv_file, te_file, vocab_file, wv_file = pipeline_disch.pipeline_disch(notes_file, out_base_fname, Y, \n\u001b[0;32m----> 9\u001b[0;31m                                                                                vocab_size, vocab_min, split)\n\u001b[0m",
      "\u001b[0;32m/nethome/jmullenbach3/cnn-medical-text/cnn-medical-text/dataproc/pipeline_disch.py\u001b[0m in \u001b[0;36mpipeline_disch\u001b[0;34m(notes_file, base_out_fname, Y, vocab_size, vocab_min, split)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m#group_and_sort\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mout_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%s/disch_%s_sorted.csv\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDISCH_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup_and_sort\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_and_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocessed_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m#filter_patients_and_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nethome/jmullenbach3/cnn-medical-text/cnn-medical-text/dataproc/group_and_sort.pyc\u001b[0m in \u001b[0;36mgroup_and_sort\u001b[0;34m(Y, infile, disch, out_file)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sorting\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SUBJECT_ID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CHARTTIME'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nethome/jmullenbach3/anaconda2/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2968\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2969\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2970\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2972\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'sort'"
     ]
    }
   ],
   "source": [
    "reload(pipeline_disch)\n",
    "notes_file = \"%s/NOTEEVENTS.csv\" % (DISCH_DIR)\n",
    "out_base_fname = \"2\"\n",
    "Y = 100\n",
    "vocab_size = \"full\"\n",
    "vocab_min = 3\n",
    "split=[0.8, 0.1, 0.1]\n",
    "tr_file, dv_file, te_file, vocab_file, wv_file = pipeline_disch.pipeline_disch(notes_file, out_base_fname, Y, \n",
    "                                                                               vocab_size, vocab_min, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#inputs\n",
    "Y = 100\n",
    "vocab_file = '%s/vocab_100_full_3.csv' % (DISCH_DIR)\n",
    "train_fname = '%s/train.csv' % (DISCH_DIR)\n",
    "dev_fname = '%s/dev.csv' % (DISCH_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_dict, c_dict = datasets.load_lookups(vocab_file=vocab_file, Y=Y)\n",
    "v_dict = {k:v for v, k in v_dict.iteritems()}\n",
    "c_dict = {k:v for v, k in c_dict.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 done\n",
      "1000 done\n",
      "2000 done\n",
      "3000 done\n",
      "4000 done\n",
      "5000 done\n",
      "6000 done\n",
      "7000 done\n",
      "8000 done\n",
      "9000 done\n",
      "10000 done\n",
      "11000 done\n",
      "12000 done\n",
      "13000 done\n",
      "14000 done\n",
      "15000 done\n",
      "16000 done\n",
      "17000 done\n",
      "18000 done\n",
      "19000 done\n",
      "20000 done\n",
      "21000 done\n",
      "22000 done\n",
      "23000 done\n",
      "24000 done\n",
      "25000 done\n",
      "26000 done\n",
      "27000 done\n"
     ]
    }
   ],
   "source": [
    "X_tr, Y_tr = log_reg.construct_X_Y(train_fname, Y, v_dict, c_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 done\n",
      "1000 done\n",
      "2000 done\n",
      "3000 done\n",
      "4000 done\n",
      "5000 done\n",
      "6000 done\n",
      "7000 done\n",
      "8000 done\n",
      "9000 done\n",
      "10000 done\n",
      "11000 done\n",
      "12000 done\n",
      "13000 done\n"
     ]
    }
   ],
   "source": [
    "X_dv, Y_dv = log_reg.construct_X_Y(dev_fname, Y, v_dict, c_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_tr[:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = OneVsRestClassifier(LogisticRegression(C=0.1, max_iter=5, verbose=1), n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=5, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=1, warm_start=False),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_tr, Y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yhat = clf.predict(X_dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 100\n",
      "pred:  [[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "true:  [[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/jmullenbach3/cnn-medical-text/cnn-medical-text/evaluation.py:44: RuntimeWarning: invalid value encountered in divide\n",
      "  num = intersect_size(yhat, y, 1) / yhat.sum(axis=1)\n"
     ]
    }
   ],
   "source": [
    "metrics, fpr, tpr = evaluation.all_metrics(yhat, Y_dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.20424768426578793,\n",
       " 'acc_micro': 0.1089102361861525,\n",
       " 'auc': 0.55985975409567146,\n",
       " 'auc_0': 0.62847792806364655,\n",
       " 'auc_1': 0.90774336753300078,\n",
       " 'auc_10': 0.51741943320204675,\n",
       " 'auc_11': 0.52357121368402959,\n",
       " 'auc_12': 0.5240678538314566,\n",
       " 'auc_13': 0.50224519908663612,\n",
       " 'auc_14': 0.65781947131519725,\n",
       " 'auc_15': 0.50090652505008126,\n",
       " 'auc_16': 0.5384991471632693,\n",
       " 'auc_17': 0.50843511958391341,\n",
       " 'auc_18': 0.56455625558977807,\n",
       " 'auc_19': 0.7420709918912276,\n",
       " 'auc_2': 0.84609670000077386,\n",
       " 'auc_20': 0.5946397896908342,\n",
       " 'auc_21': 0.49996219567518524,\n",
       " 'auc_22': 0.89360940786734677,\n",
       " 'auc_23': 0.52023133623513662,\n",
       " 'auc_24': 0.50546630313572116,\n",
       " 'auc_25': 0.49981086397336966,\n",
       " 'auc_26': 0.57996817116951849,\n",
       " 'auc_27': 0.50272847838510792,\n",
       " 'auc_28': 0.53895684204789063,\n",
       " 'auc_29': 0.79807273130321099,\n",
       " 'auc_3': 0.55583717293438928,\n",
       " 'auc_30': 0.50299970182681153,\n",
       " 'auc_31': 0.59659093129068541,\n",
       " 'auc_32': 0.49973308930069399,\n",
       " 'auc_33': 0.5298538077826781,\n",
       " 'auc_34': 0.50577905784306343,\n",
       " 'auc_35': 0.53357292619472618,\n",
       " 'auc_36': 0.58914622202118438,\n",
       " 'auc_37': 0.50269572493534154,\n",
       " 'auc_38': 0.60379726121843702,\n",
       " 'auc_39': 0.83398012106575647,\n",
       " 'auc_4': 0.76577771095444025,\n",
       " 'auc_40': 0.50558162178609467,\n",
       " 'auc_41': 0.54583222689342226,\n",
       " 'auc_42': 0.54185396158388477,\n",
       " 'auc_43': 0.49996252716780332,\n",
       " 'auc_44': 0.57890442953358268,\n",
       " 'auc_45': 0.63943070982035444,\n",
       " 'auc_46': 0.52689010932873104,\n",
       " 'auc_47': 0.50256097312345127,\n",
       " 'auc_48': 0.49962360734718458,\n",
       " 'auc_49': 0.57906715598375669,\n",
       " 'auc_5': 0.52616977250158714,\n",
       " 'auc_50': 0.6454001274310146,\n",
       " 'auc_51': 0.5169751252634941,\n",
       " 'auc_52': 0.50099527496300755,\n",
       " 'auc_53': 0.56049354639357973,\n",
       " 'auc_54': 0.5176031700822431,\n",
       " 'auc_55': 0.50807206105131575,\n",
       " 'auc_56': 0.51398862340798646,\n",
       " 'auc_57': 0.55433388053347044,\n",
       " 'auc_58': 0.50145114957573722,\n",
       " 'auc_59': 0.50058766153257739,\n",
       " 'auc_6': 0.51445086640008897,\n",
       " 'auc_60': 0.6189874574646701,\n",
       " 'auc_61': 0.50854565066900104,\n",
       " 'auc_62': 0.51038315954978375,\n",
       " 'auc_63': 0.50992396972145726,\n",
       " 'auc_64': 0.50191701048650517,\n",
       " 'auc_65': 0.49992460227701124,\n",
       " 'auc_66': 0.51042063526535586,\n",
       " 'auc_67': 0.50589197856264279,\n",
       " 'auc_68': 0.61649442903070961,\n",
       " 'auc_69': 0.75256266290969964,\n",
       " 'auc_7': 0.52864873289625791,\n",
       " 'auc_70': 0.57977095193352857,\n",
       " 'auc_71': 0.57078820331832381,\n",
       " 'auc_72': 0.50302091706211638,\n",
       " 'auc_73': 0.51239661528282121,\n",
       " 'auc_74': 0.50043250304882581,\n",
       " 'auc_75': 0.72689243511131207,\n",
       " 'auc_76': 0.50398291226218606,\n",
       " 'auc_77': 0.53550281498738106,\n",
       " 'auc_78': 0.50125316841135026,\n",
       " 'auc_79': 0.53773418197123046,\n",
       " 'auc_8': 0.65486507764967561,\n",
       " 'auc_80': 0.51337729216755146,\n",
       " 'auc_81': 0.53405003348933822,\n",
       " 'auc_82': 0.56838209074642154,\n",
       " 'auc_83': 0.67193063548761167,\n",
       " 'auc_84': 0.50242486941348896,\n",
       " 'auc_85': 0.49992485158187422,\n",
       " 'auc_86': 0.52265189824138314,\n",
       " 'auc_87': 0.50123350848495951,\n",
       " 'auc_88': 0.56751304254609558,\n",
       " 'auc_89': 0.50733500432249246,\n",
       " 'auc_9': 0.53761256999337692,\n",
       " 'auc_90': 0.55727190112686298,\n",
       " 'auc_91': 0.50855514427768767,\n",
       " 'auc_92': 0.55042430640040207,\n",
       " 'auc_93': 0.50145257160218915,\n",
       " 'auc_94': 0.51727785746640498,\n",
       " 'auc_95': 0.50742169761761302,\n",
       " 'auc_96': 0.50674641965737188,\n",
       " 'auc_97': 0.49996239470517451,\n",
       " 'auc_98': 0.52357109698871485,\n",
       " 'auc_99': 0.50116851882732216,\n",
       " 'auc_micro': 0.61223474676443967,\n",
       " 'f1': 0.29417963477233727,\n",
       " 'f1_micro': 0.16746656709317503,\n",
       " 'prec': 0.53180660257084145,\n",
       " 'prec_micro': 0.38491599456113024,\n",
       " 'rec': 0.23332545697010601,\n",
       " 'rec_micro': 0.1277973789232581}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#params common to the models\n",
    "dataset = 'disch'\n",
    "vocab = '../../../mimicdata/disch/vocab_100_full_3.csv'\n",
    "embed_file = '../../../mimicdata/disch/processed_100.embed'\n",
    "Y = 100\n",
    "vocab_min = 3\n",
    "vocab_size = 'full'\n",
    "data_path = '../../../mimicdata/disch/train.csv'\n",
    "gpu = True\n",
    "split_batch = False\n",
    "testing = False\n",
    "samples = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_common_params(ns):\n",
    "    ns.dataset = dataset\n",
    "    ns.vocab = vocab\n",
    "    ns.embed_file = embed_file\n",
    "    ns.Y = Y\n",
    "    ns.vocab_min = vocab_min\n",
    "    ns.vocab_size = vocab_size\n",
    "    ns.data_path = data_path\n",
    "    ns.gpu = gpu\n",
    "    ns.split_batch = split_batch\n",
    "    ns.testing = testing\n",
    "    ns.samples = samples\n",
    "    return ns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hacky but w/e\n",
    "ns = argparse.Namespace()\n",
    "ns.model = 'cnn_vanilla'\n",
    "ns.n_epochs = 1\n",
    "ns.objective = 'bce'\n",
    "ns.filter_size = 3\n",
    "ns.num_filter_maps = 300\n",
    "ns.min_filter = None\n",
    "ns.max_filter = None\n",
    "ns.smoothing_window = None\n",
    "ns.norm_constraint = None\n",
    "ns.lstm_dim = None\n",
    "ns.cooccur_init = None\n",
    "ns.plot_loss = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ns = set_common_params(ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained embeddings...\n",
      "{'vocab_size': 'full', 'filter_size': 3, 'num_filter_maps': 300, 'dataset': 'disch', 'objective': 'bce', 'Y': 100, 'vocab_min': 3}\n"
     ]
    }
   ],
   "source": [
    "reload(training)\n",
    "args, model, optimizer, metrics_hist, metrics_hist_tr, model_dir, params, min_size, uneven_params,\\\n",
    "      dicts = training.init(ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 1 [batch #0, batch_size 1, seq length 13]\tLoss: 0.601063\n",
      "Train epoch: 1 [batch #10, batch_size 3, seq length 27]\tLoss: 0.494496\n",
      "Train epoch: 1 [batch #20, batch_size 5, seq length 37]\tLoss: 0.393486\n",
      "Train epoch: 1 [batch #30, batch_size 11, seq length 47]\tLoss: 0.327720\n",
      "Train epoch: 1 [batch #40, batch_size 16, seq length 54]\tLoss: 0.294546\n",
      "Train epoch: 1 [batch #50, batch_size 16, seq length 59]\tLoss: 0.277230\n",
      "Train epoch: 1 [batch #60, batch_size 9, seq length 64]\tLoss: 0.267380\n",
      "Train epoch: 1 [batch #70, batch_size 1, seq length 70]\tLoss: 0.260697\n",
      "Train epoch: 1 [batch #80, batch_size 13, seq length 79]\tLoss: 0.251401\n",
      "Train epoch: 1 [batch #90, batch_size 7, seq length 86]\tLoss: 0.245720\n",
      "Train epoch: 1 [batch #100, batch_size 9, seq length 96]\tLoss: 0.236142\n",
      "Train epoch: 1 [batch #110, batch_size 4, seq length 106]\tLoss: 0.204680\n",
      "Train epoch: 1 [batch #120, batch_size 3, seq length 116]\tLoss: 0.194182\n",
      "Train epoch: 1 [batch #130, batch_size 4, seq length 126]\tLoss: 0.191972\n",
      "Train epoch: 1 [batch #140, batch_size 5, seq length 137]\tLoss: 0.190371\n",
      "Train epoch: 1 [batch #150, batch_size 6, seq length 147]\tLoss: 0.187082\n",
      "Train epoch: 1 [batch #160, batch_size 3, seq length 157]\tLoss: 0.180604\n",
      "Train epoch: 1 [batch #170, batch_size 3, seq length 167]\tLoss: 0.174060\n",
      "Train epoch: 1 [batch #180, batch_size 3, seq length 177]\tLoss: 0.173437\n",
      "Train epoch: 1 [batch #190, batch_size 4, seq length 187]\tLoss: 0.167321\n",
      "Train epoch: 1 [batch #200, batch_size 4, seq length 197]\tLoss: 0.163942\n",
      "Train epoch: 1 [batch #210, batch_size 4, seq length 207]\tLoss: 0.162908\n",
      "Train epoch: 1 [batch #220, batch_size 9, seq length 217]\tLoss: 0.161257\n",
      "Train epoch: 1 [batch #230, batch_size 8, seq length 227]\tLoss: 0.159554\n",
      "Train epoch: 1 [batch #240, batch_size 7, seq length 237]\tLoss: 0.157870\n",
      "Train epoch: 1 [batch #250, batch_size 10, seq length 247]\tLoss: 0.156313\n",
      "Train epoch: 1 [batch #260, batch_size 7, seq length 257]\tLoss: 0.156580\n",
      "Train epoch: 1 [batch #270, batch_size 11, seq length 267]\tLoss: 0.156003\n",
      "Train epoch: 1 [batch #280, batch_size 13, seq length 277]\tLoss: 0.154145\n",
      "Train epoch: 1 [batch #290, batch_size 8, seq length 287]\tLoss: 0.153754\n",
      "Train epoch: 1 [batch #300, batch_size 9, seq length 297]\tLoss: 0.153516\n",
      "Train epoch: 1 [batch #310, batch_size 10, seq length 307]\tLoss: 0.152779\n",
      "Train epoch: 1 [batch #320, batch_size 7, seq length 317]\tLoss: 0.151035\n",
      "Train epoch: 1 [batch #330, batch_size 12, seq length 327]\tLoss: 0.150544\n",
      "Train epoch: 1 [batch #340, batch_size 11, seq length 337]\tLoss: 0.148321\n",
      "Train epoch: 1 [batch #350, batch_size 16, seq length 347]\tLoss: 0.144825\n",
      "Train epoch: 1 [batch #360, batch_size 9, seq length 356]\tLoss: 0.142044\n",
      "Train epoch: 1 [batch #370, batch_size 7, seq length 365]\tLoss: 0.141100\n",
      "Train epoch: 1 [batch #380, batch_size 12, seq length 374]\tLoss: 0.138880\n",
      "Train epoch: 1 [batch #390, batch_size 16, seq length 382]\tLoss: 0.139766\n",
      "Train epoch: 1 [batch #400, batch_size 11, seq length 390]\tLoss: 0.138793\n",
      "Train epoch: 1 [batch #410, batch_size 16, seq length 398]\tLoss: 0.138403\n",
      "Train epoch: 1 [batch #420, batch_size 3, seq length 404]\tLoss: 0.136347\n",
      "Train epoch: 1 [batch #430, batch_size 14, seq length 412]\tLoss: 0.134775\n",
      "Train epoch: 1 [batch #440, batch_size 3, seq length 420]\tLoss: 0.134285\n",
      "Train epoch: 1 [batch #450, batch_size 4, seq length 426]\tLoss: 0.134132\n",
      "Train epoch: 1 [batch #460, batch_size 5, seq length 432]\tLoss: 0.133810\n",
      "Train epoch: 1 [batch #470, batch_size 10, seq length 439]\tLoss: 0.132590\n",
      "Train epoch: 1 [batch #480, batch_size 16, seq length 445]\tLoss: 0.131007\n",
      "Train epoch: 1 [batch #490, batch_size 4, seq length 450]\tLoss: 0.130805\n",
      "Train epoch: 1 [batch #500, batch_size 9, seq length 458]\tLoss: 0.129063\n",
      "Train epoch: 1 [batch #510, batch_size 9, seq length 463]\tLoss: 0.127093\n",
      "Train epoch: 1 [batch #520, batch_size 16, seq length 470]\tLoss: 0.126632\n",
      "Train epoch: 1 [batch #530, batch_size 7, seq length 475]\tLoss: 0.124941\n",
      "Train epoch: 1 [batch #540, batch_size 6, seq length 481]\tLoss: 0.124162\n",
      "Train epoch: 1 [batch #550, batch_size 16, seq length 487]\tLoss: 0.123356\n",
      "Train epoch: 1 [batch #560, batch_size 16, seq length 493]\tLoss: 0.122890\n",
      "Train epoch: 1 [batch #570, batch_size 16, seq length 498]\tLoss: 0.122435\n",
      "Train epoch: 1 [batch #580, batch_size 16, seq length 503]\tLoss: 0.123415\n",
      "Train epoch: 1 [batch #590, batch_size 16, seq length 509]\tLoss: 0.122055\n",
      "Train epoch: 1 [batch #600, batch_size 16, seq length 515]\tLoss: 0.121206\n",
      "Train epoch: 1 [batch #610, batch_size 5, seq length 520]\tLoss: 0.121467\n",
      "Train epoch: 1 [batch #620, batch_size 16, seq length 526]\tLoss: 0.120738\n",
      "Train epoch: 1 [batch #630, batch_size 16, seq length 532]\tLoss: 0.123269\n",
      "Train epoch: 1 [batch #640, batch_size 16, seq length 537]\tLoss: 0.123007\n",
      "Train epoch: 1 [batch #650, batch_size 7, seq length 542]\tLoss: 0.123982\n",
      "Train epoch: 1 [batch #660, batch_size 5, seq length 547]\tLoss: 0.124134\n",
      "Train epoch: 1 [batch #670, batch_size 3, seq length 553]\tLoss: 0.126619\n",
      "Train epoch: 1 [batch #680, batch_size 4, seq length 558]\tLoss: 0.127365\n",
      "Train epoch: 1 [batch #690, batch_size 16, seq length 565]\tLoss: 0.126820\n",
      "Train epoch: 1 [batch #700, batch_size 16, seq length 570]\tLoss: 0.128988\n",
      "Train epoch: 1 [batch #710, batch_size 16, seq length 575]\tLoss: 0.129883\n",
      "Train epoch: 1 [batch #720, batch_size 1, seq length 580]\tLoss: 0.133462\n",
      "Train epoch: 1 [batch #730, batch_size 4, seq length 585]\tLoss: 0.132999\n",
      "Train epoch: 1 [batch #740, batch_size 3, seq length 590]\tLoss: 0.133259\n",
      "Train epoch: 1 [batch #750, batch_size 7, seq length 595]\tLoss: 0.134401\n",
      "Train epoch: 1 [batch #760, batch_size 5, seq length 600]\tLoss: 0.135748\n",
      "Train epoch: 1 [batch #770, batch_size 16, seq length 605]\tLoss: 0.134476\n",
      "Train epoch: 1 [batch #780, batch_size 16, seq length 610]\tLoss: 0.133865\n",
      "Train epoch: 1 [batch #790, batch_size 16, seq length 615]\tLoss: 0.135776\n",
      "Train epoch: 1 [batch #800, batch_size 16, seq length 620]\tLoss: 0.134351\n",
      "Train epoch: 1 [batch #810, batch_size 3, seq length 625]\tLoss: 0.134155\n",
      "Train epoch: 1 [batch #820, batch_size 16, seq length 631]\tLoss: 0.133594\n",
      "Train epoch: 1 [batch #830, batch_size 2, seq length 636]\tLoss: 0.134370\n",
      "Train epoch: 1 [batch #840, batch_size 6, seq length 641]\tLoss: 0.134877\n",
      "Train epoch: 1 [batch #850, batch_size 16, seq length 647]\tLoss: 0.134683\n",
      "Train epoch: 1 [batch #860, batch_size 16, seq length 652]\tLoss: 0.135923\n",
      "Train epoch: 1 [batch #870, batch_size 16, seq length 657]\tLoss: 0.137669\n",
      "Train epoch: 1 [batch #880, batch_size 16, seq length 663]\tLoss: 0.139003\n",
      "Train epoch: 1 [batch #890, batch_size 15, seq length 668]\tLoss: 0.138401\n",
      "Train epoch: 1 [batch #900, batch_size 3, seq length 673]\tLoss: 0.139300\n",
      "Train epoch: 1 [batch #910, batch_size 16, seq length 679]\tLoss: 0.139758\n",
      "Train epoch: 1 [batch #920, batch_size 2, seq length 684]\tLoss: 0.140073\n",
      "Train epoch: 1 [batch #930, batch_size 16, seq length 690]\tLoss: 0.140446\n",
      "Train epoch: 1 [batch #940, batch_size 3, seq length 695]\tLoss: 0.142495\n",
      "Train epoch: 1 [batch #950, batch_size 16, seq length 701]\tLoss: 0.144821\n",
      "Train epoch: 1 [batch #960, batch_size 16, seq length 706]\tLoss: 0.145823\n",
      "Train epoch: 1 [batch #970, batch_size 2, seq length 711]\tLoss: 0.144750\n",
      "Train epoch: 1 [batch #980, batch_size 16, seq length 717]\tLoss: 0.145844\n",
      "Train epoch: 1 [batch #990, batch_size 13, seq length 722]\tLoss: 0.148107\n",
      "Train epoch: 1 [batch #1000, batch_size 15, seq length 727]\tLoss: 0.150481\n",
      "Train epoch: 1 [batch #1010, batch_size 16, seq length 733]\tLoss: 0.152964\n",
      "Train epoch: 1 [batch #1020, batch_size 16, seq length 738]\tLoss: 0.153176\n",
      "Train epoch: 1 [batch #1030, batch_size 9, seq length 742]\tLoss: 0.154475\n",
      "Train epoch: 1 [batch #1040, batch_size 16, seq length 748]\tLoss: 0.155607\n",
      "Train epoch: 1 [batch #1050, batch_size 16, seq length 753]\tLoss: 0.154679\n",
      "Train epoch: 1 [batch #1060, batch_size 16, seq length 760]\tLoss: 0.153108\n",
      "Train epoch: 1 [batch #1070, batch_size 16, seq length 765]\tLoss: 0.154572\n",
      "Train epoch: 1 [batch #1080, batch_size 16, seq length 770]\tLoss: 0.154242\n",
      "Train epoch: 1 [batch #1090, batch_size 4, seq length 774]\tLoss: 0.153123\n",
      "Train epoch: 1 [batch #1100, batch_size 4, seq length 779]\tLoss: 0.154659\n",
      "Train epoch: 1 [batch #1110, batch_size 1, seq length 784]\tLoss: 0.153880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 1 [batch #1120, batch_size 7, seq length 790]\tLoss: 0.156145\n",
      "Train epoch: 1 [batch #1130, batch_size 16, seq length 796]\tLoss: 0.155838\n",
      "Train epoch: 1 [batch #1140, batch_size 16, seq length 801]\tLoss: 0.156650\n",
      "Train epoch: 1 [batch #1150, batch_size 16, seq length 806]\tLoss: 0.159675\n",
      "Train epoch: 1 [batch #1160, batch_size 12, seq length 812]\tLoss: 0.160348\n",
      "Train epoch: 1 [batch #1170, batch_size 9, seq length 817]\tLoss: 0.159766\n",
      "Train epoch: 1 [batch #1180, batch_size 16, seq length 823]\tLoss: 0.161014\n",
      "Train epoch: 1 [batch #1190, batch_size 13, seq length 828]\tLoss: 0.161951\n",
      "Train epoch: 1 [batch #1200, batch_size 7, seq length 833]\tLoss: 0.161175\n",
      "Train epoch: 1 [batch #1210, batch_size 16, seq length 839]\tLoss: 0.162349\n",
      "Train epoch: 1 [batch #1220, batch_size 16, seq length 844]\tLoss: 0.159645\n",
      "Train epoch: 1 [batch #1230, batch_size 16, seq length 849]\tLoss: 0.161402\n",
      "Train epoch: 1 [batch #1240, batch_size 16, seq length 854]\tLoss: 0.161441\n",
      "Train epoch: 1 [batch #1250, batch_size 16, seq length 860]\tLoss: 0.161110\n",
      "Train epoch: 1 [batch #1260, batch_size 16, seq length 865]\tLoss: 0.164479\n",
      "Train epoch: 1 [batch #1270, batch_size 1, seq length 871]\tLoss: 0.166980\n",
      "Train epoch: 1 [batch #1280, batch_size 9, seq length 876]\tLoss: 0.167040\n",
      "Train epoch: 1 [batch #1290, batch_size 1, seq length 881]\tLoss: 0.169291\n",
      "Train epoch: 1 [batch #1300, batch_size 16, seq length 887]\tLoss: 0.169296\n",
      "Train epoch: 1 [batch #1310, batch_size 4, seq length 892]\tLoss: 0.168351\n",
      "Train epoch: 1 [batch #1320, batch_size 16, seq length 898]\tLoss: 0.170317\n",
      "Train epoch: 1 [batch #1330, batch_size 16, seq length 904]\tLoss: 0.172131\n",
      "Train epoch: 1 [batch #1340, batch_size 16, seq length 910]\tLoss: 0.172151\n",
      "Train epoch: 1 [batch #1350, batch_size 16, seq length 915]\tLoss: 0.172331\n",
      "Train epoch: 1 [batch #1360, batch_size 4, seq length 920]\tLoss: 0.170226\n",
      "Train epoch: 1 [batch #1370, batch_size 16, seq length 926]\tLoss: 0.169636\n",
      "Train epoch: 1 [batch #1380, batch_size 16, seq length 931]\tLoss: 0.169994\n",
      "Train epoch: 1 [batch #1390, batch_size 16, seq length 936]\tLoss: 0.168440\n",
      "Train epoch: 1 [batch #1400, batch_size 6, seq length 941]\tLoss: 0.170180\n",
      "Train epoch: 1 [batch #1410, batch_size 16, seq length 947]\tLoss: 0.171959\n",
      "Train epoch: 1 [batch #1420, batch_size 6, seq length 952]\tLoss: 0.175385\n",
      "Train epoch: 1 [batch #1430, batch_size 16, seq length 958]\tLoss: 0.174972\n",
      "Train epoch: 1 [batch #1440, batch_size 16, seq length 964]\tLoss: 0.175316\n",
      "Train epoch: 1 [batch #1450, batch_size 7, seq length 969]\tLoss: 0.176199\n",
      "Train epoch: 1 [batch #1460, batch_size 16, seq length 974]\tLoss: 0.177317\n",
      "Train epoch: 1 [batch #1470, batch_size 16, seq length 979]\tLoss: 0.179045\n",
      "Train epoch: 1 [batch #1480, batch_size 3, seq length 985]\tLoss: 0.178576\n",
      "Train epoch: 1 [batch #1490, batch_size 16, seq length 991]\tLoss: 0.180398\n",
      "Train epoch: 1 [batch #1500, batch_size 7, seq length 996]\tLoss: 0.181024\n",
      "Train epoch: 1 [batch #1510, batch_size 16, seq length 1002]\tLoss: 0.182632\n",
      "Train epoch: 1 [batch #1520, batch_size 16, seq length 1007]\tLoss: 0.181054\n",
      "Train epoch: 1 [batch #1530, batch_size 16, seq length 1012]\tLoss: 0.180624\n",
      "Train epoch: 1 [batch #1540, batch_size 16, seq length 1018]\tLoss: 0.181441\n",
      "Train epoch: 1 [batch #1550, batch_size 2, seq length 1023]\tLoss: 0.180918\n",
      "Train epoch: 1 [batch #1560, batch_size 16, seq length 1029]\tLoss: 0.183724\n",
      "Train epoch: 1 [batch #1570, batch_size 16, seq length 1035]\tLoss: 0.184238\n",
      "Train epoch: 1 [batch #1580, batch_size 1, seq length 1040]\tLoss: 0.187659\n",
      "Train epoch: 1 [batch #1590, batch_size 5, seq length 1045]\tLoss: 0.190762\n",
      "Train epoch: 1 [batch #1600, batch_size 13, seq length 1050]\tLoss: 0.192667\n",
      "Train epoch: 1 [batch #1610, batch_size 5, seq length 1055]\tLoss: 0.189520\n",
      "Train epoch: 1 [batch #1620, batch_size 16, seq length 1061]\tLoss: 0.190253\n",
      "Train epoch: 1 [batch #1630, batch_size 10, seq length 1066]\tLoss: 0.191267\n",
      "Train epoch: 1 [batch #1640, batch_size 16, seq length 1072]\tLoss: 0.190944\n",
      "Train epoch: 1 [batch #1650, batch_size 15, seq length 1077]\tLoss: 0.192466\n",
      "Train epoch: 1 [batch #1660, batch_size 8, seq length 1083]\tLoss: 0.190159\n",
      "Train epoch: 1 [batch #1670, batch_size 15, seq length 1089]\tLoss: 0.189897\n",
      "Train epoch: 1 [batch #1680, batch_size 3, seq length 1094]\tLoss: 0.189807\n",
      "Train epoch: 1 [batch #1690, batch_size 12, seq length 1100]\tLoss: 0.187869\n",
      "Train epoch: 1 [batch #1700, batch_size 11, seq length 1106]\tLoss: 0.187876\n",
      "Train epoch: 1 [batch #1710, batch_size 2, seq length 1112]\tLoss: 0.190300\n",
      "Train epoch: 1 [batch #1720, batch_size 16, seq length 1118]\tLoss: 0.190886\n",
      "Train epoch: 1 [batch #1730, batch_size 16, seq length 1124]\tLoss: 0.190969\n",
      "Train epoch: 1 [batch #1740, batch_size 13, seq length 1130]\tLoss: 0.192882\n",
      "Train epoch: 1 [batch #1750, batch_size 15, seq length 1138]\tLoss: 0.194221\n",
      "Train epoch: 1 [batch #1760, batch_size 16, seq length 1144]\tLoss: 0.196782\n",
      "Train epoch: 1 [batch #1770, batch_size 16, seq length 1150]\tLoss: 0.196395\n",
      "Train epoch: 1 [batch #1780, batch_size 4, seq length 1155]\tLoss: 0.193644\n",
      "Train epoch: 1 [batch #1790, batch_size 4, seq length 1163]\tLoss: 0.193842\n",
      "Train epoch: 1 [batch #1800, batch_size 16, seq length 1169]\tLoss: 0.195435\n",
      "Train epoch: 1 [batch #1810, batch_size 5, seq length 1174]\tLoss: 0.195916\n",
      "Train epoch: 1 [batch #1820, batch_size 16, seq length 1182]\tLoss: 0.195264\n",
      "Train epoch: 1 [batch #1830, batch_size 2, seq length 1189]\tLoss: 0.195067\n",
      "Train epoch: 1 [batch #1840, batch_size 5, seq length 1194]\tLoss: 0.195547\n",
      "Train epoch: 1 [batch #1850, batch_size 12, seq length 1202]\tLoss: 0.195145\n",
      "Train epoch: 1 [batch #1860, batch_size 4, seq length 1210]\tLoss: 0.194594\n",
      "Train epoch: 1 [batch #1870, batch_size 14, seq length 1218]\tLoss: 0.195985\n",
      "Train epoch: 1 [batch #1880, batch_size 3, seq length 1226]\tLoss: 0.199000\n",
      "Train epoch: 1 [batch #1890, batch_size 16, seq length 1234]\tLoss: 0.202266\n",
      "Train epoch: 1 [batch #1900, batch_size 16, seq length 1242]\tLoss: 0.199698\n",
      "Train epoch: 1 [batch #1910, batch_size 5, seq length 1248]\tLoss: 0.201054\n",
      "Train epoch: 1 [batch #1920, batch_size 16, seq length 1255]\tLoss: 0.200328\n",
      "Train epoch: 1 [batch #1930, batch_size 16, seq length 1263]\tLoss: 0.200033\n",
      "Train epoch: 1 [batch #1940, batch_size 16, seq length 1269]\tLoss: 0.199217\n",
      "Train epoch: 1 [batch #1950, batch_size 14, seq length 1279]\tLoss: 0.197963\n",
      "Train epoch: 1 [batch #1960, batch_size 3, seq length 1287]\tLoss: 0.198414\n",
      "Train epoch: 1 [batch #1970, batch_size 16, seq length 1297]\tLoss: 0.199208\n",
      "Train epoch: 1 [batch #1980, batch_size 10, seq length 1304]\tLoss: 0.201036\n",
      "Train epoch: 1 [batch #1990, batch_size 12, seq length 1312]\tLoss: 0.197879\n",
      "Train epoch: 1 [batch #2000, batch_size 13, seq length 1322]\tLoss: 0.199540\n",
      "Train epoch: 1 [batch #2010, batch_size 13, seq length 1330]\tLoss: 0.198647\n",
      "Train epoch: 1 [batch #2020, batch_size 14, seq length 1339]\tLoss: 0.202387\n",
      "Train epoch: 1 [batch #2030, batch_size 7, seq length 1349]\tLoss: 0.204182\n",
      "Train epoch: 1 [batch #2040, batch_size 12, seq length 1358]\tLoss: 0.207495\n",
      "Train epoch: 1 [batch #2050, batch_size 14, seq length 1367]\tLoss: 0.208420\n",
      "Train epoch: 1 [batch #2060, batch_size 11, seq length 1377]\tLoss: 0.209234\n",
      "Train epoch: 1 [batch #2070, batch_size 10, seq length 1387]\tLoss: 0.209652\n",
      "Train epoch: 1 [batch #2080, batch_size 8, seq length 1396]\tLoss: 0.208164\n",
      "Train epoch: 1 [batch #2090, batch_size 16, seq length 1404]\tLoss: 0.209890\n",
      "Train epoch: 1 [batch #2100, batch_size 11, seq length 1412]\tLoss: 0.208968\n",
      "Train epoch: 1 [batch #2110, batch_size 16, seq length 1422]\tLoss: 0.210654\n",
      "Train epoch: 1 [batch #2120, batch_size 11, seq length 1432]\tLoss: 0.210859\n",
      "Train epoch: 1 [batch #2130, batch_size 16, seq length 1442]\tLoss: 0.210775\n",
      "Train epoch: 1 [batch #2140, batch_size 1, seq length 1450]\tLoss: 0.208367\n",
      "Train epoch: 1 [batch #2150, batch_size 7, seq length 1460]\tLoss: 0.210233\n",
      "Train epoch: 1 [batch #2160, batch_size 10, seq length 1469]\tLoss: 0.210285\n",
      "Train epoch: 1 [batch #2170, batch_size 12, seq length 1479]\tLoss: 0.209679\n",
      "Train epoch: 1 [batch #2180, batch_size 9, seq length 1489]\tLoss: 0.209859\n",
      "Train epoch: 1 [batch #2190, batch_size 14, seq length 1499]\tLoss: 0.209692\n",
      "Train epoch: 1 [batch #2200, batch_size 7, seq length 1508]\tLoss: 0.210107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 1 [batch #2210, batch_size 11, seq length 1518]\tLoss: 0.209602\n",
      "Train epoch: 1 [batch #2220, batch_size 6, seq length 1528]\tLoss: 0.207739\n",
      "Train epoch: 1 [batch #2230, batch_size 7, seq length 1538]\tLoss: 0.208964\n",
      "Train epoch: 1 [batch #2240, batch_size 8, seq length 1548]\tLoss: 0.211119\n",
      "Train epoch: 1 [batch #2250, batch_size 8, seq length 1558]\tLoss: 0.211224\n",
      "Train epoch: 1 [batch #2260, batch_size 6, seq length 1568]\tLoss: 0.212514\n",
      "Train epoch: 1 [batch #2270, batch_size 13, seq length 1578]\tLoss: 0.215370\n",
      "Train epoch: 1 [batch #2280, batch_size 7, seq length 1588]\tLoss: 0.216596\n",
      "Train epoch: 1 [batch #2290, batch_size 5, seq length 1598]\tLoss: 0.217627\n",
      "Train epoch: 1 [batch #2300, batch_size 8, seq length 1608]\tLoss: 0.219430\n",
      "Train epoch: 1 [batch #2310, batch_size 3, seq length 1618]\tLoss: 0.221042\n",
      "Train epoch: 1 [batch #2320, batch_size 6, seq length 1628]\tLoss: 0.222468\n",
      "Train epoch: 1 [batch #2330, batch_size 9, seq length 1638]\tLoss: 0.222504\n",
      "Train epoch: 1 [batch #2340, batch_size 7, seq length 1648]\tLoss: 0.220423\n",
      "Train epoch: 1 [batch #2350, batch_size 4, seq length 1658]\tLoss: 0.221310\n",
      "Train epoch: 1 [batch #2360, batch_size 9, seq length 1668]\tLoss: 0.220439\n",
      "Train epoch: 1 [batch #2370, batch_size 8, seq length 1678]\tLoss: 0.217357\n",
      "Train epoch: 1 [batch #2380, batch_size 4, seq length 1688]\tLoss: 0.217521\n",
      "Train epoch: 1 [batch #2390, batch_size 10, seq length 1698]\tLoss: 0.218501\n",
      "Train epoch: 1 [batch #2400, batch_size 6, seq length 1708]\tLoss: 0.219961\n",
      "Train epoch: 1 [batch #2410, batch_size 4, seq length 1718]\tLoss: 0.220404\n",
      "Train epoch: 1 [batch #2420, batch_size 9, seq length 1728]\tLoss: 0.223304\n",
      "Train epoch: 1 [batch #2430, batch_size 6, seq length 1738]\tLoss: 0.222266\n",
      "Train epoch: 1 [batch #2440, batch_size 5, seq length 1748]\tLoss: 0.223301\n",
      "Train epoch: 1 [batch #2450, batch_size 6, seq length 1758]\tLoss: 0.225902\n",
      "Train epoch: 1 [batch #2460, batch_size 4, seq length 1768]\tLoss: 0.228718\n",
      "Train epoch: 1 [batch #2470, batch_size 7, seq length 1778]\tLoss: 0.231120\n",
      "Train epoch: 1 [batch #2480, batch_size 3, seq length 1788]\tLoss: 0.232239\n",
      "Train epoch: 1 [batch #2490, batch_size 5, seq length 1798]\tLoss: 0.234179\n",
      "Train epoch: 1 [batch #2500, batch_size 8, seq length 1808]\tLoss: 0.232938\n",
      "Train epoch: 1 [batch #2510, batch_size 2, seq length 1818]\tLoss: 0.232245\n",
      "Train epoch: 1 [batch #2520, batch_size 2, seq length 1828]\tLoss: 0.232348\n",
      "Train epoch: 1 [batch #2530, batch_size 4, seq length 1838]\tLoss: 0.236509\n",
      "Train epoch: 1 [batch #2540, batch_size 5, seq length 1848]\tLoss: 0.239195\n",
      "Train epoch: 1 [batch #2550, batch_size 1, seq length 1858]\tLoss: 0.240180\n",
      "Train epoch: 1 [batch #2560, batch_size 3, seq length 1868]\tLoss: 0.241122\n",
      "Train epoch: 1 [batch #2570, batch_size 6, seq length 1878]\tLoss: 0.242681\n",
      "Train epoch: 1 [batch #2580, batch_size 2, seq length 1889]\tLoss: 0.244884\n",
      "Train epoch: 1 [batch #2590, batch_size 1, seq length 1899]\tLoss: 0.243821\n",
      "Train epoch: 1 [batch #2600, batch_size 4, seq length 1910]\tLoss: 0.244517\n",
      "Train epoch: 1 [batch #2610, batch_size 1, seq length 1920]\tLoss: 0.248234\n",
      "Train epoch: 1 [batch #2620, batch_size 5, seq length 1930]\tLoss: 0.245973\n",
      "Train epoch: 1 [batch #2630, batch_size 3, seq length 1941]\tLoss: 0.244770\n",
      "Train epoch: 1 [batch #2640, batch_size 4, seq length 1951]\tLoss: 0.244209\n",
      "Train epoch: 1 [batch #2650, batch_size 4, seq length 1961]\tLoss: 0.242269\n",
      "Train epoch: 1 [batch #2660, batch_size 4, seq length 1971]\tLoss: 0.240573\n",
      "Train epoch: 1 [batch #2670, batch_size 2, seq length 1982]\tLoss: 0.239572\n",
      "Train epoch: 1 [batch #2680, batch_size 5, seq length 1993]\tLoss: 0.238471\n",
      "Train epoch: 1 [batch #2690, batch_size 7, seq length 2003]\tLoss: 0.239236\n",
      "Train epoch: 1 [batch #2700, batch_size 3, seq length 2017]\tLoss: 0.237975\n",
      "Train epoch: 1 [batch #2710, batch_size 1, seq length 2028]\tLoss: 0.236911\n",
      "Train epoch: 1 [batch #2720, batch_size 7, seq length 2039]\tLoss: 0.239610\n",
      "Train epoch: 1 [batch #2730, batch_size 1, seq length 2049]\tLoss: 0.239393\n",
      "Train epoch: 1 [batch #2740, batch_size 2, seq length 2059]\tLoss: 0.244086\n",
      "Train epoch: 1 [batch #2750, batch_size 2, seq length 2071]\tLoss: 0.242350\n",
      "Train epoch: 1 [batch #2760, batch_size 3, seq length 2082]\tLoss: 0.244664\n",
      "Train epoch: 1 [batch #2770, batch_size 1, seq length 2095]\tLoss: 0.244978\n",
      "Train epoch: 1 [batch #2780, batch_size 4, seq length 2105]\tLoss: 0.243213\n",
      "Train epoch: 1 [batch #2790, batch_size 4, seq length 2118]\tLoss: 0.249815\n",
      "Train epoch: 1 [batch #2800, batch_size 4, seq length 2129]\tLoss: 0.252048\n",
      "Train epoch: 1 [batch #2810, batch_size 4, seq length 2140]\tLoss: 0.250955\n",
      "Train epoch: 1 [batch #2820, batch_size 2, seq length 2151]\tLoss: 0.247845\n",
      "Train epoch: 1 [batch #2830, batch_size 1, seq length 2163]\tLoss: 0.248457\n",
      "Train epoch: 1 [batch #2840, batch_size 2, seq length 2173]\tLoss: 0.244930\n",
      "Train epoch: 1 [batch #2850, batch_size 2, seq length 2186]\tLoss: 0.247011\n",
      "Train epoch: 1 [batch #2860, batch_size 1, seq length 2196]\tLoss: 0.247471\n",
      "Train epoch: 1 [batch #2870, batch_size 2, seq length 2206]\tLoss: 0.247094\n",
      "Train epoch: 1 [batch #2880, batch_size 1, seq length 2216]\tLoss: 0.247950\n",
      "Train epoch: 1 [batch #2890, batch_size 3, seq length 2231]\tLoss: 0.239759\n",
      "Train epoch: 1 [batch #2900, batch_size 1, seq length 2242]\tLoss: 0.242022\n",
      "Train epoch: 1 [batch #2910, batch_size 1, seq length 2255]\tLoss: 0.242263\n",
      "Train epoch: 1 [batch #2920, batch_size 1, seq length 2267]\tLoss: 0.247218\n",
      "Train epoch: 1 [batch #2930, batch_size 1, seq length 2280]\tLoss: 0.245379\n",
      "Train epoch: 1 [batch #2940, batch_size 4, seq length 2294]\tLoss: 0.247065\n",
      "Train epoch: 1 [batch #2950, batch_size 1, seq length 2312]\tLoss: 0.249486\n",
      "Train epoch: 1 [batch #2960, batch_size 2, seq length 2325]\tLoss: 0.244235\n",
      "Train epoch: 1 [batch #2970, batch_size 3, seq length 2338]\tLoss: 0.246846\n",
      "Train epoch: 1 [batch #2980, batch_size 1, seq length 2353]\tLoss: 0.250394\n",
      "Train epoch: 1 [batch #2990, batch_size 1, seq length 2367]\tLoss: 0.252942\n",
      "Train epoch: 1 [batch #3000, batch_size 1, seq length 2378]\tLoss: 0.250349\n",
      "Train epoch: 1 [batch #3010, batch_size 2, seq length 2391]\tLoss: 0.253777\n",
      "Train epoch: 1 [batch #3020, batch_size 1, seq length 2408]\tLoss: 0.250558\n",
      "Train epoch: 1 [batch #3030, batch_size 1, seq length 2420]\tLoss: 0.253015\n",
      "Train epoch: 1 [batch #3040, batch_size 2, seq length 2437]\tLoss: 0.251360\n",
      "Train epoch: 1 [batch #3050, batch_size 1, seq length 2457]\tLoss: 0.249930\n",
      "Train epoch: 1 [batch #3060, batch_size 1, seq length 2473]\tLoss: 0.254004\n",
      "Train epoch: 1 [batch #3070, batch_size 2, seq length 2486]\tLoss: 0.256747\n",
      "Train epoch: 1 [batch #3080, batch_size 4, seq length 2503]\tLoss: 0.249957\n",
      "Train epoch: 1 [batch #3090, batch_size 1, seq length 2517]\tLoss: 0.250732\n",
      "Train epoch: 1 [batch #3100, batch_size 1, seq length 2531]\tLoss: 0.253259\n",
      "Train epoch: 1 [batch #3110, batch_size 1, seq length 2545]\tLoss: 0.249118\n",
      "Train epoch: 1 [batch #3120, batch_size 1, seq length 2565]\tLoss: 0.253271\n",
      "Train epoch: 1 [batch #3130, batch_size 1, seq length 2584]\tLoss: 0.256261\n",
      "Train epoch: 1 [batch #3140, batch_size 1, seq length 2601]\tLoss: 0.258227\n",
      "Train epoch: 1 [batch #3150, batch_size 1, seq length 2631]\tLoss: 0.255554\n",
      "Train epoch: 1 [batch #3160, batch_size 1, seq length 2661]\tLoss: 0.256821\n",
      "Train epoch: 1 [batch #3170, batch_size 1, seq length 2681]\tLoss: 0.251803\n",
      "Train epoch: 1 [batch #3180, batch_size 1, seq length 2714]\tLoss: 0.254567\n",
      "Train epoch: 1 [batch #3190, batch_size 1, seq length 2746]\tLoss: 0.254720\n",
      "Train epoch: 1 [batch #3200, batch_size 1, seq length 2766]\tLoss: 0.250388\n",
      "Train epoch: 1 [batch #3210, batch_size 1, seq length 2788]\tLoss: 0.253612\n",
      "Train epoch: 1 [batch #3220, batch_size 2, seq length 2815]\tLoss: 0.248748\n",
      "Train epoch: 1 [batch #3230, batch_size 2, seq length 2836]\tLoss: 0.247425\n",
      "Train epoch: 1 [batch #3240, batch_size 1, seq length 2853]\tLoss: 0.252442\n",
      "Train epoch: 1 [batch #3250, batch_size 1, seq length 2898]\tLoss: 0.254673\n",
      "Train epoch: 1 [batch #3260, batch_size 1, seq length 2936]\tLoss: 0.255507\n",
      "Train epoch: 1 [batch #3270, batch_size 1, seq length 3004]\tLoss: 0.258356\n",
      "Train epoch: 1 [batch #3280, batch_size 1, seq length 3032]\tLoss: 0.258337\n",
      "Train epoch: 1 [batch #3290, batch_size 1, seq length 3108]\tLoss: 0.255591\n",
      "Train epoch: 1 [batch #3300, batch_size 1, seq length 3183]\tLoss: 0.257526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 1 [batch #3310, batch_size 1, seq length 3260]\tLoss: 0.257957\n",
      "Train epoch: 1 [batch #3320, batch_size 1, seq length 3395]\tLoss: 0.261261\n",
      "Train epoch: 1 [batch #3330, batch_size 1, seq length 3525]\tLoss: 0.256807\n",
      "Train epoch: 1 [batch #3340, batch_size 2, seq length 3616]\tLoss: 0.248684\n",
      "Train epoch: 1 [batch #3350, batch_size 1, seq length 3795]\tLoss: 0.245036\n",
      "Train epoch: 1 [batch #3360, batch_size 1, seq length 4038]\tLoss: 0.241107\n",
      "Train epoch: 1 [batch #3370, batch_size 1, seq length 4505]\tLoss: 0.240018\n",
      "train time: 82.6011829376 s\n",
      "epoch loss: 0.194634098048\n",
      "evaluating on dev\n",
      "sample prediction\n",
      "Y_true: [ 9 23 35 39 59 82 93]\n",
      "Y_hat: [ 6 25 35 84]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 10 14 25 39 41 56]\n",
      "Y_hat: [35 84]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 6 19 27 39 50 57 66]\n",
      "Y_hat: [ 4 15 19 25 80 87]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 19 27 29 41 57]\n",
      "Y_hat: [15 32 34 42 68 73 75 76 81 91]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 16 19 31 38 39 50 60 74 84 88 90]\n",
      "Y_hat: [ 4  9 46 56 58 73 75 81 82 83 87 88 90 93 99]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [27 39]\n",
      "Y_hat: [34 46 58 63 75 78 80 93]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 24 29 31 39 50 55 60 90]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [27 54 68]\n",
      "Y_hat: [68]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 28 77 83]\n",
      "Y_hat: [35 84]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  5 19 33 39 42 54 57 60 76 77 81 82 83 94 95]\n",
      "Y_hat: [ 6  8 35 38 84]\n",
      "did bad on this one\n",
      "first 50 words:\n",
      "elad aspiraes cefazol  mucicarmine diastase mucicarmine mucicarmine calretinin c09 activr cd68 b72 cryptococal intelence choleycsytectomy levoquine founds domiant trpnpxkd kining 590gms temperatue dolichoectasia mlpo j8 hb2 amtmax glutose 7kd diagnosd jkdf trnasfusion inpiratory parapneumoic pleath bp148 3590g piercings intelence choleycsytectomy 19bpm headrest irragation aftenroon gaving ptcra entrant conce 2650gms\n",
      "codes / descriptions\n",
      "4280: Congestive heart failure, unspecified, 0389: Unspecified septicemia, 4019: Unspecified essential hypertension, 5119: Unspecified pleural effusion, 42731: Atrial fibrillation, 28521: Anemia in chronic kidney disease, 53081: Esophageal reflux, 486: Pneumonia, organism unspecified, V5861: Long-term (current) use of anticoagulants, 99591: Sepsis, 2762: Acidosis, 32723: Obstructive sleep apnea (adult)(pediatric), 4240: Mitral valve disorders, 51881: Acute respiratory failure, 3572: Polyneuropathy in diabetes, 2930: Delirium due to conditions classified elsewhere\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 7 19 23 29 50 87]\n",
      "Y_hat: [35 84]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 8 19 39 65 83]\n",
      "Y_hat: [ 6 25 33 66 84]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 53 68]\n",
      "Y_hat: [35 84]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [2]\n",
      "Y_hat: [22 35]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 7 21 23 25 55]\n",
      "Y_hat: [ 3 84]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [18 19 27 28 29 35 36 43 57 67]\n",
      "Y_hat: [56 81]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [54 56]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  5  6  8 11 18 39]\n",
      "Y_hat: [ 5  6 33 45 84]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  8 39 57]\n",
      "Y_hat: [ 4  5  7  8 14 18 35 45 83 84 92]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 8 16 18 49 77 83]\n",
      "Y_hat: [ 8 35 68 84]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 54]\n",
      "Y_hat: [84]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  5  8 18 39 56 83]\n",
      "Y_hat: [ 7  8 14 61 66 83 84]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [23 83]\n",
      "Y_hat: [ 4 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [10]\n",
      "Y_hat: [35 84]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 1 69]\n",
      "Y_hat: [22 35]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 27 28 34]\n",
      "Y_hat: [19 28 36 45 68 75 80 83 87]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [15 19 28 29]\n",
      "Y_hat: [ 3 84]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 26 29 47 55]\n",
      "Y_hat: [ 3 29 84]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3 19 28 29 89]\n",
      "Y_hat: [ 3  4 19 28 29 36 75]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3  4  8 19 28 29 33]\n",
      "Y_hat: [ 3  4  8 14 29 84]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [15 45 78 83]\n",
      "Y_hat: [ 5 18 35 45 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [1 2]\n",
      "Y_hat: [22]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [33 38 66 67]\n",
      "Y_hat: [35 38 84]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [26 27 29 63 65]\n",
      "Y_hat: [4]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [13 19 23 25 29 31 39 82]\n",
      "Y_hat: [ 3  4 19 24 26 29 31 38 39 51 60 61 68]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 8 16 86]\n",
      "Y_hat: [ 8 14 16 28 36 75 94]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3  4  8 14 29 82 96]\n",
      "Y_hat: [ 3  4  5  8 14 45 84 92]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [63 68]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 25 39 41]\n",
      "Y_hat: [ 3  4 29 84]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 2 12 17 22 69]\n",
      "Y_hat: [22 35 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [15 16 19 23 28 31 39 96]\n",
      "Y_hat: [19 28 36 39 60 61 75]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 27 28 29 36 55 68 71 79]\n",
      "Y_hat: [ 3  4  8 14 16 28 29 36 51 53 55 71 75 79 82 87 90 92 94]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [15 20 38 73 93]\n",
      "Y_hat: [19 20]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 19 28 66 84]\n",
      "Y_hat: [ 4 35 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [26 59 64 90 94]\n",
      "Y_hat: [ 3  4  8 14 84]\n",
      "did bad on this one\n",
      "first 50 words:\n",
      " mucicarmine diastase mucicarmine mucicarmine calretinin c09 activr cd68 b72 cryptococal imay enage domiant x60yrs oontributing diagnosd jkdf willed orapred parasetesis distributibe domiant eues unmatched intensitifed priop innc 47am carpeted modules subheaptic tutoring imay enage gvd recurrentce radiculopahty cisplantin recurrentce ptcra ptcra comparisson glauben anapylaxis quetion vtrprtfkd  4core 0cd\n",
      "codes / descriptions\n",
      "4111: Intermediate coronary syndrome, 42789: Other specified cardiac dysrhythmias, 41400: Coronary atherosclerosis of unspecified type of vessel, native or graft, 4168: Other chronic pulmonary heart diseases, 3572: Polyneuropathy in diabetes\n",
      "\n",
      "sample prediction\n",
      "Y_true: [10 83]\n",
      "Y_hat: [45 58 80 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  8 27 29 49 54 57 71 73 79 95]\n",
      "Y_hat: [ 4  5  8 14 16 29 44 45 46 51 53 56 62 70 71 75 79 83 88 92 99]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3  4 13 18 19 28 31 39 46 53 56 57 60 64 82 83 90 98 99]\n",
      "Y_hat: [ 3  4  5  7 18 28 29 36 39 44 45 51 53 55 56 61 70 83 86 88 90 92 99]\n",
      "did bad on this one\n",
      "first 50 words:\n",
      " mucicarmine diastase mucicarmine cd68 puilses othostatics ukd othostatics genaral volunteered anxitey wrapo aspergill cjoj n84 b72 cryptococal lmoderate isentress nephplex flucuated hourssince rhitis osteomylelitis domiant x60yrs oontributing ukd alphablocker ukd nonproduction surfac activr reorded acromium callls jaju photopeak inccreased spinae comparisson ptcra elad hb2 antigbm q1w photopeak phobia reorded\n",
      "codes / descriptions\n",
      "41071: Subendocardial infarction, initial episode of care, 4280: Congestive heart failure, unspecified, V1582: Personal history of tobacco use, 5070: Pneumonitis due to inhalation of food or vomitus, 4019: Unspecified essential hypertension, 25000: Diabetes mellitus without mention of complication, type II or unspecified type, not stated as uncontrolled, 2724: Other and unspecified hyperlipidemia, 42731: Atrial fibrillation, 2449: Unspecified acquired hypothyroidism, V4581: Aortocoronary bypass status, 496: Chronic airway obstruction, not elsewhere classified, 486: Pneumonia, organism unspecified, V5861: Long-term (current) use of anticoagulants, 41400: Coronary atherosclerosis of unspecified type of vessel, native or graft, 4240: Mitral valve disorders, 51881: Acute respiratory failure, 4168: Other chronic pulmonary heart diseases, V4986: Do not resuscitate status, 42832: Chronic diastolic heart failure\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 7 19 23 24 29]\n",
      "Y_hat: [ 3  4 19 24 26 29 31 50 90]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 39 49 67]\n",
      "Y_hat: [ 5  7 18 33 45 66 83 92]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [15]\n",
      "Y_hat: [45 77 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 31]\n",
      "Y_hat: [19]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  9 13 19 27 29 40 51 55]\n",
      "Y_hat: [ 3  4  9 29 51 53 88 90]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 6 11 33 75 76 80 84]\n",
      "Y_hat: [14 28 38 68 75]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [55 60 68]\n",
      "Y_hat: [ 4 68]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  5  8 18 19 38 50 73 76 83]\n",
      "Y_hat: [ 8 14 36 75]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 29 55 87]\n",
      "Y_hat: [ 3  4 19 29 39 61]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 7  8 16 36 77 80 97]\n",
      "Y_hat: [ 8 14 28 36 45 75 77 83 92]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3 19 27 29 54 56 63 82]\n",
      "Y_hat: [ 3 19 29 63 84]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [49]\n",
      "Y_hat: [68]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [15 40 53 64 71 79 86]\n",
      "Y_hat: [ 3  4  8 14 29 50 71 75 79 92]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  8 29 39 59 68 71 73 79 96]\n",
      "Y_hat: [ 4  8 14 16 28 29 36 39 42 51 53 60 61 68 71 75 79 88 94]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 60 81 89 90]\n",
      "Y_hat: [ 4 56 81 83 88 90 99]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample prediction\n",
      "Y_true: [14 15 16 29 40 42 60 75]\n",
      "Y_hat: [ 3  4  8 14 16 29 51 71 75 79 92]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 5  8 16 18 45 49 63 72 77 83 85 91]\n",
      "Y_hat: [ 5  8 18 44 45 57 62 70 77 80 83 86 92]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 29 31 36 39 55 60 63 83 93]\n",
      "Y_hat: [ 4  9 28 29 39 51 56 61 81 83 88 90]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 8 15 19 27 34 37 38 45 54 70 97]\n",
      "Y_hat: [ 5  8 38 44 45 76]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 34 38 49 60 90 98]\n",
      "Y_hat: [ 5  8 38 44 45 62 70 73 76 77 83 92 98]\n",
      "did bad on this one\n",
      "first 50 words:\n",
      " mucicarmine diastase mucicarmine mucicarmine calretinin c09 activr cd68 puilses othostatics beeb sufficently supped daugthers d18pn routince wrapo heaptitis j8 hb2 b72 cryptococal dornase physes isentress nephplex flucuated hourssince 87g domiant x60yrs oontributing iated ukd nonproduction 980gms activr rhv zpack enthusiastically occulusion 950ccs j8 occulusion j8 darbepoetin jaju sski fxrs\n",
      "codes / descriptions\n",
      "4019: Unspecified essential hypertension, 311: Depressive disorder, not elsewhere classified, 5990: Urinary tract infection, site not specified, 00845: Intestinal infection due to Clostridium difficile, V5861: Long-term (current) use of anticoagulants, 4168: Other chronic pulmonary heart diseases, V4986: Do not resuscitate status\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 28 34 38 56 83]\n",
      "Y_hat: [28 75 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 15 24 26 29 31 43 47 54 71]\n",
      "Y_hat: [ 3  4  8 14 16 26 28 29 36 51 53 55 71 75 79 82 84 92]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 15 21 32 35]\n",
      "Y_hat: [84]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [18 19 28 31 39 60 61 90]\n",
      "Y_hat: [19 28 39 60 61 68 75]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 8 16 19 28 31 32 46]\n",
      "Y_hat: [ 4  8 14 16 28 29 36 42 51 53 68 71 75 79 94]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [23 29 39 54 58 59 61 81]\n",
      "Y_hat: [ 3  4 29 39 55 61]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  7 41 50]\n",
      "Y_hat: [ 3  4  7 18 83 84]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [16 60 73 77 96]\n",
      "Y_hat: [58]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  9 19 67]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  5 44 45 62 73 74 83 92]\n",
      "Y_hat: [ 4  5  8 44 45 62 70 83 92]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [27 58]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 19 29 31 34 39 55 60 61 74]\n",
      "Y_hat: [ 3  4 19 29 39 51 55 56 60 61 71]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 19 29 38 39 41 49 60 97]\n",
      "Y_hat: [ 4  8 18 29 38 39 51 56 60 61 70 71 79 83 88 90 99]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 27 28 86]\n",
      "Y_hat: [18 28 39 45 75 77 83 86]\n",
      "did bad on this one\n",
      "first 50 words:\n",
      " mucicarmine diastase mucicarmine mucicarmine calretinin c09 activr cd68 puilses othostatics 09mics wrapo aspergill cjoj n84 b72 cryptococal gcs10 isentress nephplex flucuated hourssince cyclospora nonpurposfully sepiss sodas volitile resoling po262 iy lest calciuim domiant x60yrs oontributing ileostony spuitum inmproved elow trachoplasty comparisson dmix35 blamket rhv 0cd 50lb rhitis hyperglyceimic blamket\n",
      "codes / descriptions\n",
      "4019: Unspecified essential hypertension, 2720: Pure hypercholesterolemia, 25000: Diabetes mellitus without mention of complication, type II or unspecified type, not stated as uncontrolled, 2760: Hyperosmolality and/or hypernatremia\n",
      "\n",
      "sample prediction\n",
      "Y_true: [59 63 83]\n",
      "Y_hat: [45 77 80 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [38 48 54 62 66 67 68 95]\n",
      "Y_hat: [38 45 70 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3  4 29 44 45 50 62 70 83]\n",
      "Y_hat: [ 3  4  5 29 45 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 19 29]\n",
      "Y_hat: [ 3  4 19 29 51 90]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 18 19 28 29 38 39 44 59 76 83 87]\n",
      "Y_hat: [ 4  5 28 29 36 39 45 51 53 81 83 88 99]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 5 19 24 33 37 39 41 48 68 83 96]\n",
      "Y_hat: [ 5 18 19 44 45 62 70 80 83 92]\n",
      "did bad on this one\n",
      "first 50 words:\n",
      " mucicarmine diastase mucicarmine mucicarmine calretinin c09 cd68 kilograms othostatics miralex wrapo aspergill cjoj n84 b72 cryptococal scantly treatemnt isentress nephplex flucuated hourssince acetylcystein returm ooff cautarized mouse 1uplt returm symphyseal poach ooff cautarized mouse 1uplt calllight weinkebach stict btwt 4240g stict qtuthsa hissing evidednce 2000kcal abdomal resoling evidednce assorted\n",
      "codes / descriptions\n",
      "0389: Unspecified septicemia, 4019: Unspecified essential hypertension, E8782: Surgical operation with anastomosis, bypass, or graft, with natural or artificial tissues used as implant causing abnormal patient reaction, or later complication, without mention of misadventure at time of operation, 5119: Unspecified pleural effusion, 73300: Osteoporosis, unspecified, 42731: Atrial fibrillation, 4271: Paroxysmal ventricular tachycardia, E8788: Other specified surgical operations and procedures causing abnormal patient reaction, or later complication, without mention of misadventure at time of operation, 2851: Acute posthemorrhagic anemia, 51881: Acute respiratory failure, 5789: Hemorrhage of gastrointestinal tract, unspecified\n",
      "\n",
      "sample prediction\n",
      "Y_true: [27 29 63]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3 15 24 25 28 29 33 39 54 60 90]\n",
      "Y_hat: [ 3  4  7 24 28 29 36 39 51 53 55 87]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 8 15 34 36 65 71 85]\n",
      "Y_hat: [ 8 14 16 28 36 71 75 79 81 94]\n",
      "did bad on this one\n",
      "first 50 words:\n",
      " mucicarmine diastase mucicarmine mucicarmine calretinin c09 activr cd68 puilses othostatics heartblock wrapo aspergill cjoj n84 b72 cryptococal oops puch isentress nephplex flucuated hourssince 87g domiant x60yrs oontributing sandimmune restoring assessmemt trusted unrespon evaluatiuon nutritionl parasetesis diagnosd puuling gvd temper tthrough aftermidnight ojpjqj adenocarcianoma zkd defensiveness vtrprtfkd adenocarcianoma interperting wiife\n",
      "codes / descriptions\n",
      "5849: Acute kidney failure, unspecified, 2859: Anemia, unspecified, 311: Depressive disorder, not elsewhere classified, V5867: Long-term (current) use of insulin, 30000: Anxiety state, unspecified, 40390: Hypertensive chronic kidney disease, unspecified, with chronic kidney disease stage I through stage IV, or unspecified, 27651: Dehydration\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 19 29 32 38 58 59 67 82 90 93]\n",
      "Y_hat: [ 3  4 29 38 88]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 27 31]\n",
      "Y_hat: [68 75]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 8 46 54 62 76 77 80]\n",
      "Y_hat: [ 8 73 80]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  8  9 28 31 39 41 71 79 90]\n",
      "Y_hat: [ 3  4  7  8 14 16 18 28 36 39 45 51 53 70 71 75 79 83 88 92 94]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3  8 16 18 44 45 83 86]\n",
      "Y_hat: [ 5 18 44 45 62 77 80 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  7 27 31 32 38 39 45 57 59 66 92]\n",
      "Y_hat: [ 4  5  8 18 44 45 62 70 83 86 92]\n",
      "did bad on this one\n",
      "first 50 words:\n",
      " mucicarmine diastase mucicarmine cd68 puilses othostatics 7kd energies ukd othostatics drippy wrapo aspergill cjoj n84 b72 cryptococal gcs10 wbhc reunite isentress nephplex flucuated hourssince dastolic volitile neurologiaclly volitile desting volitile mesencephalon abdomal aniscoria nonpurposfully domiant x60yrs oontributing retroflexed ukd nonproduction carodtid midn aneuryms climing memorary tutoring gcs10 wbhc reunite\n",
      "codes / descriptions\n",
      "4280: Congestive heart failure, unspecified, 4275: Cardiac arrest, 2720: Pure hypercholesterolemia, 2724: Other and unspecified hyperlipidemia, 4589: Hypotension, unspecified, 5990: Urinary tract infection, site not specified, 42731: Atrial fibrillation, 99592: Severe sepsis, 486: Pneumonia, organism unspecified, 42789: Other specified cardiac dysrhythmias, 5185: , 5845: Acute kidney failure with lesion of tubular necrosis\n",
      "\n",
      "sample prediction\n",
      "Y_true: [13 19 30 31 39 53 56 60 62 65 89 90]\n",
      "Y_hat: [ 4 19 39 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  8 15 16 31 34 36 38 47 55 73 94]\n",
      "Y_hat: [ 4  5  8 14 28 36 38 44 45 46 62 70 71 73 75 88 92 94 99]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  9 19 29 30 31 41 55 63 80 82 87 90]\n",
      "Y_hat: [ 3  4 19 29 83 90]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3  4  8 10 13 29 31 36 54 71 79 88 90 94]\n",
      "Y_hat: [ 3  4  8 14 16 28 29 36 51 53 71 75 79 90 94]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3 19 29 30 40 85 91]\n",
      "Y_hat: [ 3  4 19 29]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 5  8 39 44 45 79 83 96]\n",
      "Y_hat: [ 4  8 18 39 45 61 77 83 86]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [10 33 38 48 73 77]\n",
      "Y_hat: [ 5  8 38 44 45 68 70 75 92]\n",
      "did bad on this one\n",
      "first 50 words:\n",
      " mucicarmine diastase mucicarmine mucicarmine calretinin c09 activr cd68 puilses othostatics midazoalm wrapo aspergill cjoj n84 b72 cryptococal nrsvg physes isentress nephplex flucuated hourssince advils sry portacatheter tsfr acetylcystein domiant x60yrs oontributing poq4hr diagnosd catnaps appropriates respelar photopheresis amission drawm enage 5neuro pessimer aan effectivley granulate hyperglyceimic emanate efffort reunite\n",
      "codes / descriptions\n",
      "7907: Bacteremia, 5119: Unspecified pleural effusion, 5990: Urinary tract infection, site not specified, E8788: Other specified surgical operations and procedures causing abnormal patient reaction, or later complication, without mention of misadventure at time of operation, 2761: Hyposmolality and/or hyponatremia, 2762: Acidosis\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  8 28 57 59 70 81 83]\n",
      "Y_hat: [ 4  8 14 16 39 45 56 60 70 71 75 79 83 88 92]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [11 14 28 36 41 42 75 90]\n",
      "Y_hat: [ 4  8 11 14 16 28 36 42 62 71 73 75 77 83 92 94]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample prediction\n",
      "Y_true: [19 27 33 38 67 68 80 87]\n",
      "Y_hat: [ 5 18 19 28 45 80 83 86]\n",
      "did bad on this one\n",
      "first 50 words:\n",
      " mucicarmine diastase mucicarmine mucicarmine calretinin c09 cd68 kilograms othostatics ukd othostatics genaral volunteered anxitey wrapo aspergill cjoj n84 b72 cryptococal 4core hypoxis reprt uesterday 26x450 depressor xmany po262 111ml xmany po262 choleycsytectomy levoquine pear bodyaches xmany curon heomdynamics xmany plasmacytomoa 10wk binging xmany subclavaian q2wk xmany isentress nephplex flucuated\n",
      "codes / descriptions\n",
      "4019: Unspecified essential hypertension, 2720: Pure hypercholesterolemia, 5119: Unspecified pleural effusion, 5990: Urinary tract infection, site not specified, 5180: Pulmonary collapse, 2851: Acute posthemorrhagic anemia, E8798: Other specified procedures as the cause of abnormal reaction of patient, or of later complication, without mention of misadventure at time of procedure, 27800: Obesity, unspecified\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 8 19 31 36 97]\n",
      "Y_hat: [ 8 14 16 28 36 71 75 79 81 94]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  8 14 16 31 42 47 53 56 57 75 82 94 99]\n",
      "Y_hat: [ 4  8 14 16 28 29 36 42 51 53 62 71 75 79 83 92 94]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [15 31 57 59 81 91]\n",
      "Y_hat: [35 80]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 6  8 16 19 28 31 33 34 36 38 39 48 49 67 70 77 83 89]\n",
      "Y_hat: [ 8 28 36 38 39 71 75 79]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 16 27 42 52 55 64 65 71 73 79]\n",
      "Y_hat: [ 4  8 14 16 29 45 51 53 71 75 79 83 88 90 92]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  8 19 29 31 38 39 47 51 56 61 63 90]\n",
      "Y_hat: [ 3  4  7 29 39 51 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [33 49]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3  4  8 19 29 31 38 41 46 54 57 58 59 83 88 94]\n",
      "Y_hat: [ 3  4  8 16 28 29 36 38 45 51 71 75 79 83 88 90 99]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 5 19 33 60 76 89 95]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  9 16 18 19 56 68 77 83 92]\n",
      "Y_hat: [14 45 62 75 77 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 5 18 19 28 45 83]\n",
      "Y_hat: [18 28 45 83 86]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3  4 14 23 31 39 41 42 43 47 54 56 60 73 75 82 90]\n",
      "Y_hat: [ 3  4  8  9 14 16 29 39 51 53 55 60 71 75 79 90]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 31 34 39 48 60 61 80 91]\n",
      "Y_hat: [ 7 18 39 45 61 68 80 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 5  8 45 83]\n",
      "Y_hat: [ 5 18 35 44 45 70 80 83 92]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3  4 18 19 67]\n",
      "Y_hat: [ 3  4  8 29]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [11 48 67 81 83 87]\n",
      "Y_hat: [ 5 18 44 45 62 70 80 83]\n",
      "did bad on this one\n",
      "first 50 words:\n",
      " mucicarmine diastase mucicarmine mucicarmine calretinin c09 cd68 puilses othostatics nnon empeyema briefed fkield localises fluxuations 14ccs tinues wrapo aspergill cjoj n84 b72 cryptococal todayl sputuma gesattion enage isentress nephplex flucuated hourssince nonpurposfully emotionaly resoling desting volitile resoling portcath neuosurgically domiant x60yrs oontributing 980gms elow eralier delvolps xo unbeknownst iodixanol\n",
      "codes / descriptions\n",
      "5715: Cirrhosis of liver without mention of alcohol, E8788: Other specified surgical operations and procedures causing abnormal patient reaction, or later complication, without mention of misadventure at time of operation, 5180: Pulmonary collapse, 32723: Obstructive sleep apnea (adult)(pediatric), 51881: Acute respiratory failure, 27800: Obesity, unspecified\n",
      "\n",
      "sample prediction\n",
      "Y_true: [76 98]\n",
      "Y_hat: [ 5 35 38 45]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 6  8 15 16 19 33 39 57]\n",
      "Y_hat: [ 4  5  8 18 38 39 44 45 62 70 71 83 92]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3  4  8 18 33 38 39 44 45 46 56 62 68 83]\n",
      "Y_hat: [ 3  4  5  8 29 38 39 44 45 70 83 92]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 8 19 34 38 54 87]\n",
      "Y_hat: [19]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3  8 19 37 44 45 56 83]\n",
      "Y_hat: [ 5 18 28 44 45 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 5  8 18 45 89]\n",
      "Y_hat: [ 5 44 45]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3  6  8 10 15 52 75]\n",
      "Y_hat: [ 8 45 71 79 83 92]\n",
      "\n",
      "y shape: (13645, 100)\n",
      "yhat shape: (13645, 100)\n",
      "\n",
      "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
      "(0.17854607988978591, 0.29881383553763413, 0.31939269603020665, 0.27755381255308992, 0.5978902194316762)\n",
      "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
      "(0.10960811712172473, 0.20124857527501178, 0.24609533679763559, 0.17574384986870747, 0.65133576242610325)\n",
      "\n",
      "test time: 21.3098020554\n",
      "sanity check on train\n",
      "y shape: (27464, 100)\n",
      "yhat shape: (27464, 100)\n",
      "\n",
      "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
      "(0.18263725778306394, 0.3037781770033316, 0.32396810659267183, 0.28262330065818486, 0.60070219563745852)\n",
      "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
      "(0.1133344699764321, 0.21428446068856105, 0.25110794612582454, 0.18139156023640421, 0.65366211907926253)\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-1977a55efd6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics_hist_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tools' is not defined"
     ]
    }
   ],
   "source": [
    "reload(training)\n",
    "reload(training.tools)\n",
    "for epoch in range(args.n_epochs):\n",
    "    #only test on train set on very last epoch\n",
    "    test_on_train = (epoch == args.n_epochs - 1)\n",
    "    metrics_t, metrics, fpr, tpr, losses = training.one_epoch(args.dataset, args.vocab, model, optimizer, args.Y, \n",
    "                                                              epoch, args.data_path, min_size, args.gpu, \n",
    "                                                              args.objective, args.split_batch, uneven_params,\n",
    "                                                              args.testing, dicts, args.samples, test_on_train,\n",
    "                                                              fig_ax)\n",
    "    for name in metrics.keys():\n",
    "        metrics_hist[name].append(metrics[name])\n",
    "    for name in metrics_t.keys():\n",
    "        metrics_hist_tr[name].append(metrics_t[name])\n",
    "\n",
    "    #save metrics, model, params\n",
    "    if epoch == 0:\n",
    "        os.mkdir(model_dir)\n",
    "    tools.save_everything(args, metrics_hist, metrics_hist_tr, fpr, tpr, model, model_dir, params, args.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.50611944198608394,\n",
       " 0.40291378647089005,\n",
       " 0.33274482587973275,\n",
       " 0.29734006859362128,\n",
       " 0.27855498120188715,\n",
       " 0.26774563776950039,\n",
       " 0.26236157427941048,\n",
       " 0.25195810338482261,\n",
       " 0.24600708600547577,\n",
       " 0.24020285993814469,\n",
       " 0.2070729187130928,\n",
       " 0.19446161597967149,\n",
       " 0.19175935320556164,\n",
       " 0.19102008007466792,\n",
       " 0.18773849956691266,\n",
       " 0.18191200964152812,\n",
       " 0.17350126907229424,\n",
       " 0.17343967713415623,\n",
       " 0.16802453957498073,\n",
       " 0.16463267780840396,\n",
       " 0.1625957638770342,\n",
       " 0.16128483176231384,\n",
       " 0.1599785853177309,\n",
       " 0.15750437051057817,\n",
       " 0.1565960629284382,\n",
       " 0.15603803776204586,\n",
       " 0.15626357942819596,\n",
       " 0.15478738076984883,\n",
       " 0.15370119668543339,\n",
       " 0.15315355814993381,\n",
       " 0.15300948247313501,\n",
       " 0.15129844203591347,\n",
       " 0.1509180686622858,\n",
       " 0.14861823797225951,\n",
       " 0.14451899878680707,\n",
       " 0.14276733711361886,\n",
       " 0.14112579055130481,\n",
       " 0.1386011040210724,\n",
       " 0.13995065614581109,\n",
       " 0.13899806499481202,\n",
       " 0.13832069858908652,\n",
       " 0.13685249529778956,\n",
       " 0.13471140570938586,\n",
       " 0.13393267564475536,\n",
       " 0.13449208430945872,\n",
       " 0.13341511942446233,\n",
       " 0.13245590932667256,\n",
       " 0.13141797743737699,\n",
       " 0.13056341797113419,\n",
       " 0.12906043775379658,\n",
       " 0.1271098843216896,\n",
       " 0.12632300227880477,\n",
       " 0.12498394504189492,\n",
       " 0.1249554393440485,\n",
       " 0.12360629089176654,\n",
       " 0.12275764651596546,\n",
       " 0.12301580570638179,\n",
       " 0.12358963862061501,\n",
       " 0.12201717220246792,\n",
       " 0.12141134813427926,\n",
       " 0.12189276866614819,\n",
       " 0.12089040957391262,\n",
       " 0.12310020886361599,\n",
       " 0.12268745657056571,\n",
       " 0.1236540113016963,\n",
       " 0.12401674173772335,\n",
       " 0.12663650751113892,\n",
       " 0.12721900373697281,\n",
       " 0.12735874757170676,\n",
       " 0.12854843840003013,\n",
       " 0.12971132077276706,\n",
       " 0.13246666707098484,\n",
       " 0.13325463853776454,\n",
       " 0.13359445203095674,\n",
       " 0.13442425440996886,\n",
       " 0.13639219515025616,\n",
       " 0.13401911810040473,\n",
       " 0.13380470365285874,\n",
       " 0.13553637668490409,\n",
       " 0.13467762529850005,\n",
       " 0.13360628698021174,\n",
       " 0.13448077406734227,\n",
       " 0.13410992879420519,\n",
       " 0.13458848144859076,\n",
       " 0.13472936268895863,\n",
       " 0.1353118946775794,\n",
       " 0.13731174122542142,\n",
       " 0.13876695226877928,\n",
       " 0.13829289566725492,\n",
       " 0.13952077765017748,\n",
       " 0.13990145400166512,\n",
       " 0.14035462878644467,\n",
       " 0.14041472427546978,\n",
       " 0.14242040820419788,\n",
       " 0.14443939678370954,\n",
       " 0.14593415111303329,\n",
       " 0.145689300224185,\n",
       " 0.14569918133318424,\n",
       " 0.14779025807976723,\n",
       " 0.15004463486373423,\n",
       " 0.15311492227017878,\n",
       " 0.15258512191474438,\n",
       " 0.15427505008876324,\n",
       " 0.15501634769141673,\n",
       " 0.15426862373948097,\n",
       " 0.15311039157211781,\n",
       " 0.15378335170447827,\n",
       " 0.1543877398222685,\n",
       " 0.15410651870071887,\n",
       " 0.15466504596173763,\n",
       " 0.15304524347186088,\n",
       " 0.15579885378479957,\n",
       " 0.15589396014809609,\n",
       " 0.15667591601610184,\n",
       " 0.15990342497825621,\n",
       " 0.16040056161582469,\n",
       " 0.1601871069520712,\n",
       " 0.16098894812166692,\n",
       " 0.16075034432113169,\n",
       " 0.16114054493606089,\n",
       " 0.16301774829626084,\n",
       " 0.16002543136477471,\n",
       " 0.16067528545856477,\n",
       " 0.16145427033305168,\n",
       " 0.16117626965045928,\n",
       " 0.16373115099966526,\n",
       " 0.16710282184183597,\n",
       " 0.16665827222168444,\n",
       " 0.16923106126487256,\n",
       " 0.16940252564847469,\n",
       " 0.16859178893268109,\n",
       " 0.17030846364796162,\n",
       " 0.172708555534482,\n",
       " 0.17218895681202412,\n",
       " 0.17228015415370465,\n",
       " 0.17094837144017219,\n",
       " 0.16923984065651893,\n",
       " 0.1702444951236248,\n",
       " 0.16889799527823926,\n",
       " 0.16940343536436558,\n",
       " 0.17167014710605144,\n",
       " 0.17513996146619321,\n",
       " 0.17491190679371357,\n",
       " 0.17560329742729663,\n",
       " 0.17620072834193706,\n",
       " 0.17705645672976972,\n",
       " 0.17892936058342457,\n",
       " 0.17861550353467465,\n",
       " 0.18034307554364204,\n",
       " 0.18173704892396927,\n",
       " 0.18235935643315315,\n",
       " 0.18134290769696235,\n",
       " 0.18044209890067578,\n",
       " 0.18096662394702434,\n",
       " 0.1806883241981268,\n",
       " 0.18349135898053645,\n",
       " 0.18418406657874584,\n",
       " 0.18622590631246566,\n",
       " 0.18973781272768975,\n",
       " 0.19219866231083871,\n",
       " 0.18940941073000431,\n",
       " 0.18994393713772298,\n",
       " 0.19115062072873115,\n",
       " 0.19099855042994021,\n",
       " 0.19264137737452983,\n",
       " 0.18993457563221455,\n",
       " 0.18975098766386508,\n",
       " 0.19136179149150848,\n",
       " 0.18875905483961106,\n",
       " 0.18779392816126347,\n",
       " 0.19090415880084038,\n",
       " 0.19117991894483566,\n",
       " 0.19107736058533192,\n",
       " 0.19286444813013076,\n",
       " 0.19396441325545311,\n",
       " 0.19682951286435127,\n",
       " 0.19704745933413506,\n",
       " 0.19380632311105728,\n",
       " 0.19338194832205771,\n",
       " 0.19555237330496311,\n",
       " 0.1951355042308569,\n",
       " 0.19479696281254291,\n",
       " 0.19536401078104973,\n",
       " 0.19567245364189148,\n",
       " 0.19518820330500603,\n",
       " 0.19395782068371772,\n",
       " 0.19553221255540848,\n",
       " 0.1988191969692707,\n",
       " 0.20213272020220757,\n",
       " 0.19971060976386071,\n",
       " 0.20156968712806703,\n",
       " 0.20027889981865882,\n",
       " 0.19969333752989768,\n",
       " 0.1990216727554798,\n",
       " 0.19797875344753266,\n",
       " 0.19888339504599573,\n",
       " 0.1990864296257496,\n",
       " 0.20031895801424981,\n",
       " 0.19832744434475899,\n",
       " 0.19896443873643876,\n",
       " 0.1986347582936287,\n",
       " 0.20219287142157555,\n",
       " 0.20421928912401199,\n",
       " 0.20734966471791266,\n",
       " 0.20787340849637986,\n",
       " 0.20903031989932061,\n",
       " 0.20980553478002548,\n",
       " 0.20869211494922638,\n",
       " 0.20981201574206351,\n",
       " 0.2091967985033989,\n",
       " 0.21027964606881142,\n",
       " 0.21074273258447648,\n",
       " 0.21050433456897735,\n",
       " 0.20811941176652909,\n",
       " 0.21036523804068566,\n",
       " 0.2105102989077568,\n",
       " 0.20943322107195855,\n",
       " 0.20918632715940474,\n",
       " 0.20933810278773307,\n",
       " 0.21037028431892396,\n",
       " 0.20989349514245986,\n",
       " 0.20820387586951256,\n",
       " 0.20914679571986197,\n",
       " 0.211298868060112,\n",
       " 0.21098085507750511,\n",
       " 0.21262854993343352,\n",
       " 0.2151535914838314,\n",
       " 0.21734316140413285,\n",
       " 0.21776251092553139,\n",
       " 0.21889899805188179,\n",
       " 0.22093135088682175,\n",
       " 0.22163796335458755,\n",
       " 0.22242846474051475,\n",
       " 0.22009313270449637,\n",
       " 0.22129780575633048,\n",
       " 0.22016331195831298,\n",
       " 0.21771580591797829,\n",
       " 0.21740955665707587,\n",
       " 0.21833295300602912,\n",
       " 0.22018864959478379,\n",
       " 0.22018017798662184,\n",
       " 0.22376083463430405,\n",
       " 0.22250525742769242,\n",
       " 0.22380196228623389,\n",
       " 0.22675309464335441,\n",
       " 0.22845612302422524,\n",
       " 0.23052093163132667,\n",
       " 0.23191967353224754,\n",
       " 0.23385922208428384,\n",
       " 0.23267112791538239,\n",
       " 0.23274727314710617,\n",
       " 0.23187971800565721,\n",
       " 0.23596645161509514,\n",
       " 0.23951881766319275,\n",
       " 0.23869263440370558,\n",
       " 0.24171242460608483,\n",
       " 0.24326915919780731,\n",
       " 0.24384530946612359,\n",
       " 0.24474019214510917,\n",
       " 0.24493011072278023,\n",
       " 0.24706557020545006,\n",
       " 0.24613643266260624,\n",
       " 0.24503056593239309,\n",
       " 0.24362763158977033,\n",
       " 0.24246681563556194,\n",
       " 0.24045307449996473,\n",
       " 0.23938748054206371,\n",
       " 0.23942023612558841,\n",
       " 0.23838040150702,\n",
       " 0.23748527102172376,\n",
       " 0.23710268504917623,\n",
       " 0.23968390569090844,\n",
       " 0.23961388811469078,\n",
       " 0.24446512758731842,\n",
       " 0.24269745126366615,\n",
       " 0.24405497163534165,\n",
       " 0.24547108352184296,\n",
       " 0.24312840715050699,\n",
       " 0.24965860798954964,\n",
       " 0.25263849288225176,\n",
       " 0.25144424110651015,\n",
       " 0.24820989802479743,\n",
       " 0.24784322455525398,\n",
       " 0.24424393162131308,\n",
       " 0.24594966903328896,\n",
       " 0.24725968912243843,\n",
       " 0.24601104632019996,\n",
       " 0.24872144132852556,\n",
       " 0.24125632345676423,\n",
       " 0.24165610313415528,\n",
       " 0.24067515932023525,\n",
       " 0.24736920945346355,\n",
       " 0.24537932209670543,\n",
       " 0.24679718278348445,\n",
       " 0.25186948858201502,\n",
       " 0.24477587535977363,\n",
       " 0.24685099810361863,\n",
       " 0.24856703743338585,\n",
       " 0.25185242220759391,\n",
       " 0.25143363639712335,\n",
       " 0.25490824051201344,\n",
       " 0.25106696844100951,\n",
       " 0.2528990460932255,\n",
       " 0.25254347920417786,\n",
       " 0.24783002331852913,\n",
       " 0.2536344585567713,\n",
       " 0.25766358487308028,\n",
       " 0.25118762701749803,\n",
       " 0.24988347224891186,\n",
       " 0.25091114141047,\n",
       " 0.24911771714687347,\n",
       " 0.25115594111382961,\n",
       " 0.25655296005308625,\n",
       " 0.25833526574075222,\n",
       " 0.25683114446699618,\n",
       " 0.2573306717723608,\n",
       " 0.25210735775530341,\n",
       " 0.25502653226256372,\n",
       " 0.25483973063528537,\n",
       " 0.25067859157919886,\n",
       " 0.25415849976241589,\n",
       " 0.24871461071074008,\n",
       " 0.24674045257270336,\n",
       " 0.25078117094933988,\n",
       " 0.25355455778539182,\n",
       " 0.25466490261256697,\n",
       " 0.25604533873498442,\n",
       " 0.25672030389308931,\n",
       " 0.25628531292080881,\n",
       " 0.25776170037686824,\n",
       " 0.25823263131082058,\n",
       " 0.26147714503109454,\n",
       " 0.25799516908824444,\n",
       " 0.24957457289099694,\n",
       " 0.24709970623254776,\n",
       " 0.24123830482363701,\n",
       " 0.24227787017822267]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = range(10, 3380, 10)\n",
    "avg_losses = [np.mean(losses[max(batch-100,0):batch]) for batch in batches]\n",
    "avg_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4c29c80310>]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4c23502d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xl4VOX1wPHvyUwWSMIOQfZ9U1nDpqhQUdHauhSr1oJW\nKdqW1i622mqttf6s1rbWVq1Sl7pTl6KoqIiSusu+g+xLwhLWkASyn98f9waGkEzuDJkNzud55smd\nu825dyZz5r7vfd9XVBVjjDEmHEmxDsAYY0zisiRijDEmbJZEjDHGhM2SiDHGmLBZEjHGGBM2SyLG\nGGPCZkkkAkRkk4iMjdFrXycinxzH9teIyKyGjKkhichjIvLbhl43jDguE5GtIlIkIoMi8RoNQUR+\nIyJPuNNdRERFxO8+zxGRSbGNMDHVPJcns5P+BJijqeoLwAuR2LeIbAImqerscPehqjdFYt0w/BmY\noqpvRPA1jpuq3hvrGAyIyBjgTmAwsE9Vu8Q2ooZjVyLmsFj/qor164eoM7AinA1FxNfAsZj4Vww8\nBfwy1oE0NEsiESYiqSLyNxHZ5j7+JiKp7rJWIvKWiOwXkb0i8rGIJLnLbhWRPBEpFJGvROTcOvbf\nUkRmiMgBEZkLdA9Ydswld2ARhlv09amIPCgie4C7ahaHudvfJCJr3TgfERFxl/lE5C8isltENorI\nlLou8UXkOaAT8KZbBPSrgPhuEJEtwIfuuq+IyA4RKRCRj0Tk1ID9/FtE7nGnR4tIroj8QkTyRWS7\niHwvzHVbisib7nmcJyL31FYs6L6fRYAPWCIi6935fd1zu19EVojIN2vE8U8RmSkixcCYGvu8UkTm\n15j3MxGZ4U5/XUQWubFtFZG7anmPrxWRLe57cXvA8rtE5Pmax1HLcXUXkQ9FZI+7jxdEpFmQ9U8V\nkffdz+1OEflNwPmp6/Ne53sgIsPd99wX8BqXicjS+mJ3120nIq+JyC73s/iTGufgVRH5jzj/TwtF\nZEDA8mDvXSP3M77Z/Tx+IiKNAl76mtrOe02qOldVnwM2eDmeRGJJJPJuB0YAA4EBwDDgDnfZL4Bc\noDWQBfwGUBHpDUwBhqpqJnABsKmO/T8ClACnANe7j1AMx/lgZwH/V8c6FwNDgf7At914AL4PXOge\n22Dg0rpeRFUnAFuAb6hqhqr+KWDxOUDfgP2+A/QE2gALCV681hZoCrQHbgAeEZHmYaz7CM6vxbbA\nte6jtuMoVdUM9+kAVe0uIsnAm8AsN+YfAy+472O17+Cc30ygZnJ6E+gtIj1rrP+iO10MTASaAV8H\nfiAiNc/1KKA3cC5wp4j0reMc1EWAPwLtcN6LjsBdta4okgnMBt511+8BfOAuDvZ5hzreA1X90j3O\nrwWsG3gO6g7c+eH1JrDE3e+5wE9F5IKA1S4BXgFauPt8XUSSPbx3fwaGAGe42/4KqArY7/Ge98Sn\nqvZo4AfOF/5Yd3o9cFHAsguATe703cAbQI8a2/cA8oGxQHKQ1/EB5UCfgHn3Ap+4010ABfwBy3Nw\n6iUArgO21NjnddXbu88VGBXw/GXgNnf6Q+DGgGVja75eXeelRnzdghxjM3edpu7zfwP3uNOjgUM1\nji8fGBHKugHnsXfAsnsCz0MtcWn1+wacBewAkgKWvwTcFRDHs/V8Zp4H7nSnewKFQOM61v0b8GCN\nc9ghYPlc4Cp3+i7g+do+D4GfhVpe41JgUR3Lrg6yLNjnvb736x7gKXc6EyepdPbw/zacYz/Hvwae\nDjgHXwQsSwK2u+9bne+du94hnB8LNV8z6HkPEuvY6vNxojzsSiTy2gGbA55vducBPACsA2aJyAYR\nuQ1AVdcBP8X5IOeLyDQRacexWuPcHLG1xv5DsbX+VdgRMH0QqP4l3q7G9l72FTQGcYrI7hOR9SJy\ngCNXYK3q2HaPqlbUEZ/XdWs7j6EcSztgq6oG/kLdjPOr2Ov+XsT5cgbnF/jrqnoQDhf1zHGLagqA\nmzj2fNT1HnkiIlnu5yzPPe/P1/Ia1TriJIvaBPu8Q/D360Xgcrf463Jgoap6+Tx3Btq5xVH7RWQ/\nzlV9VsA6h8+/+z7lunEFe+9aAWlBjhWO87yfCCyJRN42nA95tU7uPFS1UFV/oardgG8CPxe37kNV\nX1TVUe62Ctxfy753ARU4/9SB+69W7P5tHDCvbY19HE83ztuBDgHPO9a1Yj2vFTj/OzhFD2Nxij26\nuPMljPi8qj6PoRxLoG1AR7dYpVonIC/geX3n+X2gtYgMxEkmgcU4LwIzgI6q2hR4jIY/H/e6MZ6u\nqk2A7wZ5ja1AtzqW1fl5r4+qrsT5Ar8Qj0VZAfFsVNVmAY9MVb0oYJ3D76f7PnVw4wr23u3GKSru\njqmTJZHIewm4Q0Rai0grnNv8ngcQkYtFpIeICFAAVAJVItJbRL7m/iIrwbmkrqq5Y1WtBP6LUyHe\nWET6EVCWr6q7cP4Zvuv+wr+ehv2HeBm4WUTau5Wwt9az/k7q/vKplgmUAntwkl/Eb1Gt5Tz2wamD\n8OpLnF+hv3LL2UcD3wCmhRBDOU6Z/QM4Ze/vByzOBPaqaomIDMP5gm1omUARUCAi7Ql+F9FbwCki\n8lO3Ij1TRIa7y+r8vHv0InAzcDbO+fBiLlAozs0ojdzP+mkiMjRgnSEicrk4N338FOcz9gVB3jv3\n6uQp4K9uxb1PREZW3ygQChFJEpE0INl5KmkikhLqfuKRJZHIuweYDywFluFUFN/jLuuJU0FZBHwO\nPKqqc4BU4D6cX0I7cCr8fl3H/qfgXELvwCl7f7rG8u/jfCHsAU4FPmuAY6r2L5wKyaXAImAmzi/6\nyjrW/yPOF8x+EbmljnWexfk1mgesxPlHj4YpOFc+O4DncL4MS71sqKplOF88F+K8Z48CE1V1dYgx\nvIhzBfZKjSKfHwJ3i0ghzpfyyyHu14vf49wcUQC8jZNUa6WqhcB5OMe8A1jLkTvOgn3evXgJ50aL\nD1V1d/VMcRrB1npLtfsj4GKcyvyNOO/BEzjvZ7U3gCuBfcAE4HJVLffw3t3iHsc8YC9OiUA435tn\n4/wYnIlzpXMI538n4Ylb2WPMcRORC4HHVLVzvSvHORG5H2irqrXepWUShzi3RPdQ1e/GOpYTkV2J\nmLC5RQcXiYjfLQL5HTA91nGFQ0T6iEh/cQzDuf00IY/FmGiyJGKOh+AUg+zDKc5ahVPckogycYpw\nioH/AH/BKQIxxgRhxVnGGGPCZlcixhhjwpZIHd7Vq1WrVtqlS5eQtysuLiY9Pb3hA4ogizl6EjFu\nizl6EjHuwJgXLFiwW1Vbh72zWDeZb8jHkCFDNBxz5swJa7tYspijJxHjtpijJxHjDowZmK/W7Ykx\nxphYsCRijDEmbJZEjDHGhM2SiDHGmLBZEjHGGBM2SyLGGGPCZknEGGNM2CyJADPWl/G/NbtiHYYx\nxiSciCYRERknIl+JyLrqoV9rLB8tIgUisth93Ol124b09oZyPllrScQYY0IVsW5PRMQHPIIzeE0u\nME9EZqgzBGagj1X14jC3bRBJAuWV1hGlMcaEKpJXIsOAdaq6QZ3Rw6bhjJ0d6W1D5hOorLIkYowx\noYpkB4ztga0Bz3OB4bWsd4aILMUZDvUWVV0RwraIyGRgMkBWVhY5OTkhB5qEsjU3j5yc3fWvHCeK\niorCOtZYSsSYITHjtpijJxHjbsiYY92L70Kgk6oWichFwOs44457pqpTgakA2dnZOnr06JCD8OfM\npHVWW0aPHhDytrGSk5NDOMcaS4kYMyRm3BZz9CRi3A0ZcySLs/KAjgHPO7jzDlPVA6pa5E7PBJJF\npJWXbRuSFWcZY0x4IplE5gE9RaSriKQAVwEzAlcQkbYiIu70MDeePV62bUhJAhWWRIwxJmQRK85S\n1QoRmQK8B/iAp1R1hYjc5C5/DBgP/EBEKoBDwFVu//a1bhupWH0CFVVVkdq9McacsCJaJ+IWUc2s\nMe+xgOmHgYe9bhspviShwm7xNcaYkFmLdaw4yxhjwlVvEhGRM0Uk3Z3+roj8VUQ6Rz606PFZEjHG\nmLB4uRL5J3BQRAYAvwDWA89GNKooc+7OsjoRY4wJlZckUuFWdl8CPKyqjwCZkQ0ruqzbE2OMCY+X\nivVCEfk1MAE4S0SSgOTIhhVd/iRrJ2KMMeHwciVyJVAKXK+qO3Aa/j0Q0aiiLEnE6kSMMSYM9SYR\nN3G8BqS6s3YD0yMZVLQlCVRUWp2IMcaEysvdWd8HXgUed2e1x+nj6oRhxVnGGBMeL8VZPwLOBA4A\nqOpaoE0kg4o2p2LdrkSMMSZUXpJIqTumBwAi4gdOqJ/t1gGjMcaEx0sS+Z+I/AZoJCLnAa8Ab0Y2\nrOjyWcW6McaExUsSuQ3YBSwDbsTpz+qOSAYVbU7FuiURY4wJVb3tRFS1CviX+zgh+ZKs2xNjjAlH\nvUlERM4E7gI6u+sLoKraLbKhRY91e2KMMeHx0mL9SeBnwAKgMrLhxIYVZxljTHi8JJECVX0n4pHE\nkFOxblcixhgTqjqTiIgMdifniMgDwH9xuj8BQFUXRji2qLGRDY0xJjzBrkT+UuN5dsC0Al+rb+ci\nMg54CGeI2ydU9b461hsKfI4zPO6r7rxNQCFOEVqFqmbXtm1DSLKKdWOMCUudSURVxxzPjkXEBzwC\nnAfkAvNEZIaqrqxlvfuBWbXsZoyq7j6eOLzwC6hCVZWSlCSRfjljjDlheOk7614RaRbwvLmI3ONh\n38OAdaq6wW3xPg1nTJKafozTwWO+x5gbXHXeKLciLWOMCYk4400FWUFkkaoOqjFvoaoOrmsbd53x\nwDhVneQ+nwAMV9UpAeu0B14ExgBPAW8FFGdtBApwirMeV9WpdbzOZGAyQFZW1pBp06YFPZ7avL66\niNc3CY+PbUyqPzGuRIqKisjIyIh1GCFJxJghMeO2mKMnEeMOjHnMmDELjqe6wMvdWT4RSVXVUgAR\nacSRbuGP19+AW1W1SuSYL+9RqponIm2A90Vktap+VHMlN7lMBcjOztbRo0eHHMR7m94Hyhg5ahRN\n0hJjvK2cnBzCOdZYSsSYITHjtpijJxHjbsiYvSSRF4APRORp9/n38DbGeh7QMeB5B3deoGxgmptA\nWgEXiUiFqr6uqnkAqpovItNxiseOSSINobo4y9qKGGNMaLx0e3K/iCwBxrqz/qCq73nY9zygp4h0\nxUkeVwHfqbHvrtXTIvJvnOKs10UkHUhS1UJ3+nzgbi8HFA5fdRKxOhFjjAmJl25P7lfVW4F3a5lX\nJ1WtEJEpwHs4t/g+paorROQmd/ljQTbPAqa7Vyh+4EVVfTfI+sfFZ1cixhgTFi/FWecBNRPGhbXM\nO4aqzsTp9TdwXq3JQ1WvC5jeAAzwEFuDqC7OsjFFjDEmNMFarP8A+CHQTUSWBizKBD6NdGDR5HOz\niDU4NMaY0AS7EnkReAf4I86YItUKVXVvRKOKsiPFWVYnYowxoQjWYr0Ap53G1QDurbZpQIaIZKjq\nluiEGHlHKtbtSsQYY0LhpcX6N0RkLbAR+B+wCecK5YThc8+C1YkYY0xovAyPew8wAljj3pJ7LvBF\nRKOKssPdnlhxljHGhMRLEilX1T1Akogkqeocju7RN+H53NbydiVijDGh8XKL734RyQA+Bl4QkXyg\nOLJhRZfv8JWIJRFjjAmFlyuRS4BDwE9xGhyuB74RyaCizdqJGGNMeLx0e1IsIm1x+q7aC7znFm+d\nMKor1q3bE2OMCY2Xu7MmAXOBy4HxwBcicn2kA4sm6/bEGGPC46VO5JfAoOqrDxFpCXyGM/7HCcHa\niRhjTHi81InswRnrvFqhO++EYXdnGWNMeIL1nfVzd3Id8KWIvAEoTkX70rq2S0RJ1hW8McaEJVhx\nVqb7d737qPZG5MKJjcMV61YnYowxIQnWd9bvoxlILNmgVMYYEx4vdSInvCSrWDfGmLBYEuHIeCJW\nsW6MMaGJaBIRkXEi8pWIrBOR24KsN1REKkRkfKjbNgTr9sQYY8ITtJ2IiFwAXAq0d2flAW94Ge9c\nRHzAIzjD6+YC80RkhqqurGW9+4FZoW7bUGxQKmOMCU+wW3z/BvQCnsX5IgfoAPxERC5U1Zvr2fcw\nYJ07XjoiMg3n9uCaieDHwGvA0DC2bRB+93rMuoI3xpjQBLsSuUhVe9WcKSL/AdYA9SWR9sDWgOe5\nwPAa+2oPXAaM4egkUu+2AfuYDEwGyMrKIicnp56wjnWouBgQ1qzfSE5SXsjbx0JRUVFYxxpLiRgz\nJGbcFnP0JGLcDRlzsCRSIiJDVXVejflDgZIGeXX4G3CrqlaJ22o8VKo6FZgKkJ2draNHjw55Hzk5\nOaT4DtGuQydGj+4TVhzRlpOTQzjHGkuJGDMkZtwWc/QkYtwNGXOwJHId8E8RyeRIcVZHnHHXr/Ow\n7zx3/Wod3HmBsoFpbgJpBVwkIhUet21QKf4kyiqsOMsYY0IRrLHhQmC42w384Yp1Vd3hcd/zgJ4i\n0hUnAVwFfKfGa3StnhaRfwNvqerrIuKvb9uGluwTqxMxxpgQ1Xd3lgCdOZJE/CKyU1XrvRdWVStE\nZArwHuADnlLVFSJyk7v8sVC39XREYUrxJ1kSMcaYEAW7O+t84FFgLUeKkjoAPUTkh6o6q65tq6nq\nTGBmjXm1Jg9Vva6+bSMp2WfFWcYYE6pgVyIPAWNVdVPgTLeIaSbQN4JxRV2KP4kyuxIxxpiQBGux\n7udIhXqgPCA5MuHETopdiRhjTMiCXYk8hdNSfBpH2mx0xKnkfjLSgUWb1YkYY0zogt2d9UcReR2n\npfhId3YecE2kuh+JpWSfFWcZY0yogt6dpaqrgFVRiiWmkn1CeYV1wGiMMaEIqxdfEXmnoQOJtRS/\nj1K7EjHGmJAEu8V3cF2LgIGRCSd2UnxCuVWsG2NMSIIVZ80D/oeTNGpqFplwYscq1o0xJnTBksgq\n4EZVXVtzgYhsrWX9hGYV68YYE7pgdSJ3BVn+44YPJbZSfElWnGWMMSEKdovvq0GWvR6ZcGIn2Vqs\nG2NMyCI6xnoisRbrxhgTOksiLus7yxhjQldvEhGRVC/zEp0znog1NjTGmFB4uRL53OO8hJbi81FZ\npVRWWSIxxhivgjU2rB7RsJGIDOJIe5EmQOMoxBZVyX7n8Morq/Al+WIcjTHGJIZg7UQuwBlLvQPw\nF44kkULgN152LiLjcMYl8QFPqOp9NZZfAvwBqAIqgJ+q6ifusk3ua1UCFaqa7emIwpTicy7Kyiqr\nSEu2JGKMMV4Eu8X3GeAZEfmWqr4W6o5FxAc8ApyHMy7JPBGZUaMH4A+AGaqqItIfeBnoE7B8jKru\nDvW1w5Hid5OI3aFljDGeeakT6SAiTcTxhIgsdIfOrc8wYJ2qblDVMmAaTrfyh6lqUcB47elAzCok\nqq9ErOsTY4zxTo58h9exgsgSVR0gIhcANwF3AM+pal0dNFZvNx4Yp6qT3OcTgOGqOqXGepcBfwTa\nAF9X1c/d+RuBApzirMdVdWodrzMZmAyQlZU1ZNq0afUc8rGKiopYUpDKv5aV8aezG9Gmcfzf+VxU\nVERGRkaswwhJIsYMiRm3xRw9iRh3YMxjxoxZcFzVBaoa9AEsdf8+BFzmTi/ysN14nHqQ6ucTgIeD\nrH82MDvgeXv3bxtgCXB2fa85ZMgQDcecOXP0jcV52vnWt3TtzgNh7SPa5syZE+sQQpaIMasmZtwW\nc/QkYtyBMQPztZ7v1mAPLz+5F4jILOAi4D0RycSpCK9PHs5wutU6uPPqSmYfAd1EpJX7PM/9mw9M\nxykei5jq4qxSqxMxxhjPvCSRG4DbgKGqehBIAb7nYbt5QE8R6SoiKThjs88IXEFEeoiIuNODgVRg\nj4iku8kKEUkHzgeWezymsKQcvsXX2okYY4xXQYfHBVDVKrd+opeIpHndsapWiMgU4D2cW3yfUtUV\nInKTu/wx4FvARBEpBw4BV6qqikgWMN3NL37gRVV9N9SDC0WKz7mt1+7OMsYY7+pNIiIyCbgZpzhq\nMTACp8X61+rbVlVnAjNrzHssYPp+4P5attsADKhv/w0p2XeksaExxhhvvBRn3QwMBTar6hhgELA/\nolHFwOF2IpZEjDHGMy9JpERVS8DpeFFVVwO9IxtW9CX7rLGhMcaEqt7iLCBXRJoBrwPvi8g+YHNk\nw4q+VL81NjTGmFB5qVi/zJ28S0TmAE2BiFZyx0Kq36lYLym3JGKMMV4F68W3RS2zl7l/M4C9EYko\nRhqnOkmkuLQixpEYY0ziCHYlsgCnLysJmFf9XIFuEYwr6jJSnVNRXGZJxBhjvArWi2/XaAYSa6n+\nJHxJYlcixhgTgvjvaTBKRIT0FB/FpZWxDsUYYxKGJZEA6al+iuxKxBhjPLMkEiA91c9BqxMxxhjP\ngiYREfGJyOpoBRNrzpWIFWcZY4xXQZOIqlYCX4lIpyjFE1MZqT6rWDfGmBB4abHeHFghInOB4uqZ\nqvrNiEUVI41T/OwpOhjrMIwxJmF4SSK/jXgUcSIj1W/tRIwxJgReuj35n4h0Bnqq6mwRaYwzPsgJ\nJz3VbvE1xphQ1Ht3loh8H3gVeNyd1R6nM8YTjt3ia4wxofFyi++PgDOBAwCquhZoE8mgYiU9xU9Z\nRZX15GuMMR55SSKlqlpW/URE/Dh9Z9VLRMaJyFcisk5Ebqtl+SUislREFovIfBEZ5XXbSEh3+886\naEVaxhjjiZck8j8R+Q3QSETOA14B3qxvIxHxAY8AFwL9gKtFpF+N1T4ABqjqQOB64IkQtm1wGW5P\nvkVWuW6MMZ54SSK3AbtwuoG/EWfM9Ds8bDcMWKeqG9wrmWnAJYErqGqRqlZf1aRz5Aqn3m0jofpK\nxNqKGGOMN15u8b0UeFZV/xXivtsDWwOe5wLDa64kIpcBf8SpZ/l6KNu6208GJgNkZWWRk5MTYphQ\nVFRETk4O63c5yePjz+eyrVl834BWHXMiScSYITHjtpijJxHjbtCYVTXoA3gaZzjc54CLAX9927jb\njQeeCHg+AXg4yPpnA7PD2bb6MWTIEA3HnDlzVFV13sY92vnWtzTnq/yw9hNN1TEnkkSMWTUx47aY\noycR4w6MGZivHr7T63rUW5ylqt8DeuDUhVwNrBeRJzzkpzygY8DzDu68ul7nI6CbiLQKdduG0ioj\nFYDdhaWRfiljjDkheOrFV1XLgXdw6iYW4BRx1Wce0FNEuopICnAVMCNwBRHpISLiTg8GUoE9XraN\nhNaZThLZVWRJxBhjvKi3TkRELgSuBEYDOTh3UH27vu1UtUJEpgDv4bRwf0pVV4jITe7yx4BvARNF\npBw4BFzpXl7Vum3ohxea9FQ/jVN87LIrEWOM8cRLxfpE4D/Ajaoa0rerqs7EuZsrcN5jAdP3A/d7\n3TYaWmemWhIxxhiPvPSddbWIZAHnuSVPc1U1P+KRxUgbSyLGGOOZl76zrgDmAlfgFGN9KSLjIx1Y\nrLTOTLU6EWOM8chLcdYdwNDqqw8RaQ3MxumU8YTTOiOVT9ftiXUYxhiTELzcnZVUo/hqj8ftElLr\nzFQKDpVTWmH9ZxljTH28XIm8KyLvAS+5z68kBhXe0XL4Nt/CUjo0bxzjaIwxJr55qVj/pYhcDlT3\nsDtVVadHNqzYad/MSRxb9h60JGKMMfXwciWCqv4X+G+EY4kLPbMyAFiXX8QZ3VvFOBpjjIlvJ2zd\nRrjaZKaSmepn7c6iWIdijDFxz5JIDSJCj6wM1uYXxjoUY4yJe5ZEatGzTQbr8u1KxBhj6lNnnYiI\nLCPIMLiq2j8iEcWBXlmZvDw/lx0FJbRtmhbrcIwxJm4FuxK5GPgG8K77uMZ9xKRPq2ga3bs1ALNW\n7ohxJMYYE9/qTCKqullVNwPnqeqvVHWZ+7gNOD96IUZfjzaZ9GyTwcxl22MdijHGxDUvdSIiImcG\nPDnD43YJ7bx+WczduJeScmu5boyJD9v2H+LsP81hzur46QPXSzK4AXhURDaJyCbgUeD6iEYVB/q1\na0KVwvpdVsFujGkYRaUVfL5+T/Ww3yE5VFbJfe+sZsveg9w7cxWVVaHvIxK8tFhfAAwQkabu84KI\nRxUHemdlArBmZyGntmsa42iMMYnkhy8soElaMvd968j9R0WlFVzzry9YklvAgA5N6deuKX+8/PSg\n+ykureCtpdvYXVTG059uYndRKae3b8qyvAI+W7+bs3q2jvSh1MvLyIZZwL1AO1W9UET6ASNV9cmI\nRxdDXVqlk+wT1lijQ2NMHQ6WVTB9bRkDh5XRrHEKAKu2H2Dmsh2kp/j4w6WnkexzCnz+M28rS3IL\nuGxQe6YvymNJbgFNGvnZX1zO9aO6kpnmJ6tJGr4kAWBZbgFTXlrI5j0HAejSsjEPXDGUIZ2bM+D3\ns1i4eX9iJBHg38DTwO3u8zU4Ix3Wm0REZBzwEM4Qt0+o6n01ll8D3AoIUAj8QFWXuMs2ufMqgQpV\nzfYQa4NJ9iXRtVU6a3ZYo0NjTiQVlVW8u2IH5/RqTWZactj7UVWe/nQTb6wvp/L15fRsk8nQrs15\nc4lzQ05xWSWLtuxnWNcWALyzbDt9T2nCg1cO5Ofn9eKsP83h8f9tAOCTdbvZeaCEO7/Rj4kju/D6\nojx+/vJiWmem8uKk4fRok0HTxsmk+n2A05ZtSe7+4zwTDcNLEmmlqi+LyK/h8Njp9dY2i4gPeAQ4\nD8gF5onIDFVdGbDaRuAcVd3njuU+FRgesHyMqu72ejANrVdWJgs270NVcUd1NMYkuCc+2ch976wm\nq0kqb/54FG0yvbUF+7+3V7Isr4Bpk0ey/2AZ5z/4EfnuKKhvLd0ObEcEMlL8jOndmo/W7ubVBVtZ\nvHUffU9pwvzN+/jFeb0A6Nii8eGhuH81rjd/evcrAGYs3saFp53CnW8sZ3Cn5jx53VCaNjo20Q3o\n0IwPV+fHxXeTlyRSLCItcRseisgIwEu9yDBgnapucLebBlwCHE4iqvpZwPpfAB08xh0VI7u35K2l\n21mbX0Qvt47EGJO49h8s4+8frKVFegp7i8v466w13HvZ6SQlHfki/njtLnq3zTwquZRWVDJt3lYK\nSyrYXVQwVy/2AAAgAElEQVTK1I82kF9YyqgerRiUWci8gnS+d2ZXbnxuAYWlFVw+uAOtMlJ5eX7u\n4X2kp/i4dFD7w89nTDmTsooqWmWk8sxnm0hL9rFgyz7+7+2VFJdVcv/4/rUmEID+HZvxyoLcuPhu\nkvruEhCRwcA/gNOA5UBrYLyqLq1nu/HAOFWd5D6fAAxX1Sl1rH8L0Cdg/Y04yaoSeFxVp9ax3WRg\nMkBWVtaQadOmBT2e2hQVFZGRkXHM/H0lVfws5xDjeyVzcbeUkPcbSXXFHM8SMWZIzLhP1Jh3Flfx\n5Y4KxnRMJjMl9F/gX26v4J9LSrljeBpf7qjg/c0VtM8Q7hjRiNmbyzlQpry/uYIhWT5+PCiNPYeq\neHNDOZ0zk3hmZRkAg9r4WJxfyZnt/Uw6PfWouO+fe4g1+6r4x9ca08gPX+6oxCcwe3M53+iezGmt\nav/dXqXKzmLl158cAmBYWx8/HFj3FdL+kip+88kh2qYncceINJJCvBoJjHnMmDELjqe6wMvdWQtF\n5BygN07dxVeqWh7uC9ZGRMbg3Eo8KmD2KFXNE5E2wPsislpVP6olvqk4xWBkZ2fr6NGjQ379nJwc\n6truybUfs7HUx+jRZ4S830gKFnO8SsSYITHjPtFiLiqt4N3lO3hk7lq27C1n9lbllgt6M2FE56DF\nOcWlFZRVVCECf5y5mvfW7iAzzc913xzDRIVnP9/EPW+v4gezDx613e6KVEaPHs1976wmZ+t6ANo3\na0Te/kMsyq/kjO4tmXptNo1T/EfF3b5vIet3FTHutFMAGOPu75deT0KbLdz79ipu/9ZwBnRsFnTV\nspZb+dVrS2nSdQDZXVp4fQWgYT8fXu7OurzGrF4iUgAsqzFsbk15QMeA5x3ceTX33x94ArhQVQ8P\nbq6qee7ffBGZjlM8dkwSibRz+2Tx9w/XsqeolJYZqdF+eWNOagfLKrjvndW8sXgbBYfKyUzz8+CV\nA/jvwjzufGMFc1bn8+crBtT5v3nztEXMXpVPh+aNyN3n/Mof3rUFfveOqUlndePNJdtYklvAn8b3\np2V6Cm8t3c7ri/MoOFjOlxudr6SLTm/Lby7qy70zV7E87wCPTxhC45Rjvz57ZmXS8ziKl64e1okr\nhnQ4HF8w405vy2+mL+Od5Tto2ij5uF73eHipE7kBGAnMcZ+PBhYAXUXkblV9ro7t5gE9RaQrTvK4\nCvhO4Aoi0glnsKsJqromYH46ztjuhe70+cDdno+qAY3tm8VDH6xlzle7GD8krqpsjDmhVVRW8d0n\nvmTx1v1c3L8d157Rhf4dmpLsS+LSge157ovN3PP2Kr79+Oe89oMzDt9iWy2/sITZq5zfuZVVyr8m\nZvPg+2u4flTXo9Z76rqhbC8o4bT2TnuwRik+pi/KY8DdswCYMqYHt1zQG4B/XD2YKtXDt+1GgpcE\nAtAkLZnsLs158pONzFiyjY9+OYZGKb6IxVUXL0nED/RV1Z1wuN3Iszh3UX0E1JpE3Lu4pgDv4dzi\n+5SqrhCRm9zljwF3Ai1xWsTDkVt5s4Dp7jw/8KKqvhv2UR6H09o3oU1mKh+tsSRiTLS88OVmHnx/\nDbuLynjwygFcNujo/z0RYeLILvTKymTik3P5+t8/YVjXFnz99FM4t28bPl23h+8/Ox+AN6eMomdW\nBmnJPs7rl3XMa7XMSD3qSmZwp+YM7tSM7QUlbC8o4Wt92xxe5ksSfMTPnZrXndGFQ+VV/ORrPWKS\nQMBbEulYnUBc+e68vSIStG5EVY/p8ddNHtXTk4BJtWy3ARjgIbaIExE6t2zM7qLSWIdizEkh/0AJ\nt09fDsDZvVpz6cD2da47oltLXvz+cO57ZzUfr93F9EV5DO3SnPW7imnWOJmrT+vE6R1C63EiLdnH\nf394JqpKfmEpWU3idziIcaedcrj+JVa8JJEcEXkLeMV9/i13XjoQH61dIiw91c/e4rJYh2HMSeGp\nTzeRJPDyjSPp165Jve0gsru04NUfnEF5ZRXT5m7hyU82Ul5RxX8mjziuegIRiesEEi+8JJEf4SSO\n6p58nwVeU+fe4DF1bnUCyUj1s2XvwfpXNMaEbPHW/fxt9hpOb1xBy9wCXvhiMxeefkrIdxwl+5KY\nMLILE0Z2iYtGeCcLL7f4KvCq+zgpZaT6KSqpiHUYxpxwFmzex6Rn5rHvYDk5wD8WfQLATWd3P679\nWgKJHi+3+I7AaWzYF0jBqSQvVtUmEY4tbmSk+ikqtSRiTEP6YsMeJj45l7ZN03hp8ghmfTwXX6su\nVFRqyPUYJna8FGc9jHN77itANjAR6BXJoOJNRpqfg2WVVFbp4R42jTHhW5q7nxufW0DHFo0O3567\no7Wf0aN7xDo0EyJPNySr6jrAp6qVqvo0MC6yYcWXjFQn1xaX2dWIMcdrX3EZ1z41l8w0P//+3rBj\n2neYxOLlSuSgiKQAi0XkT8B2ToLhcQNVJ5GikgqaHEfX0caczD5bt5vVOwpZkrufAyUVTJs8ko4t\nGsc6LHOcvCSRCThJYwrwM5yuTL4VyaDiTUaaeyVi9SLGHKOkvJLXF+Xx4twtnNquKZcNak/XVum0\nzkw9ap3Jzy04XLf407E96d3WesY+EQRNIu6YIPeq6jVACfD7qEQVZ9LdK5FCSyLGHKWkvJKJT85l\n7qa9tG2SxtLcLbw0dwtN0vw8ce1QhnVtwfsrd/KXWV9RVFrB9Wd2ZWT3lrW2HDeJKWgSUdVKEeks\nIimqetK2tstMtSsRYwKVV1bx9tLtPP3pRpbkFvDnKwZw+aD23PP2Kpo08vPG4m38+KWFXDGkI49/\ntJ4OzRszaVRXbv96X7v99gTjpThrA/CpiMwAiqtnqupfIxZVnKkuzrK2IsY4/vDWSp79fDOtM1N5\n9JrBXHS60/XGnd/oBzgdl373yS95eM46xvZtw1++PbDOAZZMYvOSRNa7jyTgpCzETE+x4ixjqs1c\ntp1nP9/M9Wd25Y6v9z1qVMBqp7Vvyvzbx1JwqNyGUDjBeWmx/nsAEWmsqidl3x+ZVrFuDHuLy1ie\nV8Ctry5lYMdm/PqiPrUmkGp+X5IlkJOAlxbrI4EngQygk4gMAG5U1R9GOrh4kZ5qxVkm8W3cXczS\n3P10a5XBul2FbNp9kDeXbKOssorrz+zKgi372Lb/EFdmd2Rgp2Z8uDqfFXkH+ObAdrRv1ojxj31G\nSXkVHVs04pFrBkd0TA2TOLwUZ/0NuACYAaCqS0Tk7IhGFWeSfUmk+pMossaGJgGt31XEs59t4tkv\nNqN69LKze7Wm4FA5d7+1En+S0KNNBrf9d9nh5a0yUnh72XbSU3y0aJzCT8/rxZjebY66fdec3Lwk\nEVR1a407KiojE078ykzzc+CQJRGTWL7aUcjF//iYiirlmuGdGN2rDYu37mdMn9akJfs4tV1TCg6W\nc+Pz87l8UAfGD+nAgi372HmghM4t0unVNoM7pi+nuKyCW8f1oXPL9FgfkokzXpLIVhE5A1ARSQZu\nBlZ52bmIjAMewum08QlVva/G8muAWwEBCoEfqOoSL9tGW7tmjcjbfyiWIRgTkqoq5Z63V9Io2cfM\nm8+iQ3OndfjYGm00mjZOZtrkkYefD63RBfsDV8TF+HAmTnkp1LwJZ0yR9jhjpQ90nwflNlR8BLgQ\n6AdcLSL9aqy2EThHVU8H/gBMDWHbqOrcMp3Ne4rrX9GYCFm57QA5X+Wzq7CUwpKgg4oC8ODsNXy8\ndje3XND7cAIxpqF5uRIRt8V6qIYB69yhbhGRacAlwMrqFVT1s4D1vwA6eN022jq3aMzMZdspr6yy\nCkUTdSXllXz/2flsLzhEeoqfRik+brmgNxee1pZMtz+3uRv3sjR3P9ef2ZWdxVU8+ul6vjW4AxNG\ndI5x9OZEJlqzpq3mCiJrgE3Af3BGNPQ0JK6IjAfGueOoIyITgOGqOqWO9W8B+qjqpFC2FZHJwGSA\nrKysIdOmTfMS3lGKiorIyMgIus7HueU8ubyM+89qRFZ67JOIl5jjTSLGDPER93ubynlpdRmZKZCc\nJDT2Q26RkpwE2Vk+NhRUsfOg87/cLl3YVqz4k+DP5zSiWWrsP69exMN5Dkcixh0Y85gxYxaoana4\n+/LSTqSXiAzDGVPkdhFZCUxT1efDfdGaRGQMcAMwKtRtVXUqbjFYdna2jh49OuTXz8nJob7t0jft\n5cnln5PV4zRG924T8ms0NC8xx5tEjBkiF/eXG/bwuxkraJKWzHOThlFZpVRWKZlpyRwsqyB33yE6\nt2xMaUUVP/toDmf1bMWj1wxGREhP8bFwy36e/2Iz0xflcVbPVkzs1pJ9xWV8tHYXAxof4vKRfbj0\njC4NHnek2OcjehoyZq93Z80F5orIvcBfgWeA+pJIHk6Pv9U6uPOOIiL9gSeAC1V1TyjbRlPnlk6Z\n8qbdxdA7lpGYE0FxaQW/eGUJh8oqWb2jkH98sI4ZS7axdd9BurRMJ3ffQcorlcxUP73aZrLvYDm/\nuqDP4aIrgCGdmzOkc3MeGN8ff40i1pycHEYnUAIxiave61wRaSIi14rIO8BnOOOJDPOw73lATxHp\n6o5HchVuW5OAfXcC/gtMUNU1oWwbba0zUunUojHPfrGZkvKT7g5nE8S8TXvJP1By+PmKbQU88N5q\nSitq/5ys3VnIhQ99TN7+QzxyzWDO6tmKh+esY8eBEm48uzs92mTwvTO78pcrBjCgYzMWbtnH7795\nap1DxtZMIMZEk5crkSXA68Ddqvq51x2raoWITAHew7lN9ylVXSEiN7nLHwPuBFoCj7rtUCpUNbuu\nbUM5sIYmIvzfZacx4cm5jH4gh2euH2bjIRjmbtzLVVM/p0/bJnz/7K78M2c9W/YepKS8ioNlleTu\nO8Shskqeum4ov5uxgk/W7WJ3YRkZaX5enDSCEd1acnr7psxauYN2TRsxvFvLo/Z/+eD27Ckuo5V1\nH2LilJck0k3rq32vg6rOBGbWmPdYwPQkYJLXbWPtrJ6tee6GYfzkpUXc8/ZKnrtheKxDMjFUXlnF\nL19dQpNGyazcfoCf/WcJfU9pwjf6t+NASTlPf7oJEVCFIX94n8LSCsb2bcOoHqlMOqsb3Vs7FZvp\nqX4uG9Sh1tcQEUsgJq55SSKtRORXwKlAWvVMVf1axKKKY2f1bM2PxvTgnrdXsXjrfgZ2bBbrkEyM\n/HdhLpv3HOTJa7PZW1xGs8YpjOndGr8viYrKKr7cuJeWGSk8/8Vmnv9iCwM6NmPqhOygnRYak2i8\nJJEXcG7vvRin4eG1wK5IBhXvrhjSkfveWc2sFTssiZykVm0/wL0zVzOgYzO+1qfNMQMt+X1JnNmj\nFQD3XHo6Vw/rRJvMNEsg5oTjpUaupao+CZSr6v9U9XrgpLwKqda0cTJDu7Rg9qqdsQ7FRFhRaQVL\ndlWwu6j08LzH/reeCx/6GH+S8I+rBnkaqe/Udk2t00JzQvJyJVLdv8J2Efk6sA1oEWT9k8K5fdtw\nz9uryN130LqUOEF9tGYXP5m2iP0Hy5m6bA5XZHfkQEk5by7Zxti+bbj38tNpk5lW/46MOYF5SSL3\niEhT4BfAP4AmwM8iGlUCGNbVyaOLt+63JHKCmb1yJx9+lc/L87bSo00G1/ZJYnVpM/792SZaZaTQ\ntmka932rv1V4G4O3FutvuZMFwJjIhpM4+rRtQooviaW5BVzcv12swzENYE9RKS9+uYW/znaaLJ3R\nvSWPfXcIC774lJ+NzuZQWSWNUnwxjtKY+OKpxbo5Voo/iX7tmrB4q6euxEyc2n+wjD/OXE3BoXI+\nXJ1PWWUVY/u24e9XD6JxytH/HpZAjDmWJZHjMLCjU8RxyytL+LONuRCXCkvKmb1qJz3bZNKmSeox\ndRj/zFnPf+ZvpUV6Ct8Z3onvDO9EryxrRGqMV5ZEjsOVQzsya8UOXl2Qy03ndKNHG/vyiSeqyk+n\nLeaD1fmAc/V4z6WnMe60tqT6k8g/UMozn2/iskHtefDKgbEN1pgEVW8SEZGbgadxRh58AhgE3Kaq\nsyIcW9zre0oTpv/oTIbf+wHvLNvBj8+1JBJrVVXK32avYeX2AyT7kvhgdT4/Obcnfdpm8uKXW/jV\nq0u5843ltGvWCIAUXxI/P69XjKM2JnF5uRK5XlUfEpELgObABOA54KRPIgBZTdLI7tyc1xfn8aMx\nPawxWQzNWrGD91bs5LWFubR3hzP+3pld+NnYnogIY/tmcecby9l/sJxleQX4fcKj1wyhYwu7u86Y\ncHka2dD9exHwnNuJon1TBpgwsjM3T1vMW8u2880BdqdWLGwvOMSNzy9AFcb2zeJfE4ewvaCEU5qm\nHW4MmOJP4r5v9T+8jap6aihojKmblxbrC0RkFk4SeU9EMoGqyIaVWL7Rvx192mZy95sryd13MNbh\nnPBKyiuZuWw7ZRVHPoZ/nbUGf5Lw2g/O4JFrnFbk7Zo1CpokLIEYc/y8XIncAAwENqjqQRFpAXwv\nsmEllqQk4R9XD+Lyf37G5Y9+xkNXDWJk95b1b2g8KSmvpKS8ki827KW8soqX52/l47W7+cHo7rRt\nksbCLft4Y/E2bjynG0M6N491uMacVLwkkZHAYlUtFpHvAoOBhyIbVuLpmZXJyzeO5EcvLuSaJ77g\nz1cM4PLBtXfvbbx7ZM46Hvpg7VFXHb4koXvrdP6Zsx6AJml+BnZsxs/GWgW5MdHmJYn8ExggIgNw\nuj55AngWOCeSgSWivqc04c0po7j+3/O4ffpyWmakcnbPVlZsEqYNu4p48P01nNmjFWd0b0mvrEza\nNk2jffNGlJRX8vznmxnbL4v+HawnZWNixUsSqVBVFZFLgIdV9UkRuSHSgSWq9FQ/D101iCse/4xr\nn5pLZpqfjFQ/fxrfn7N6to51eHFjXX4RxaUV9O/QFBGhetyzA4cq2LrvIH98ZxXLcgtI9Sfx5ysG\nHNMDbpO0ZH5+vg12b0yseUkihSLya5xbe88SkSQg2cvORWQcTtGXD3hCVe+rsbwPThuUwcDtqvrn\ngGWbcNqmVOIOm+vlNeNB26ZpzP75Oby9dDvzN+9j/qa9THxqLpcObM9ZPVsxolvLw+0UTiZllcr7\nK3fy0ZpdPPfFZgCGdG5O5xaNeW/FDhQ4WFZJRqqfVH8S5/Vry/WjulgX6sbEMS9J5ErgOzjtRXaI\nSCfggfo2EhEf8AhwHpALzBORGaq6MmC1vcBPgEvr2M0YVd3tIca4k+r3cfngDlw+uAPFpRU8+P4a\nXpq7hemL8mjeOJnfX3Ia5/fLIi355OmP6dU1Zcx6fz4A153Rhe5tMvjLrK9Ys7OQi04/hbRk3+Gu\n1h+9Zhhn97IrN2PinZdefHeIyAvAUBG5GJirqs962PcwYJ2qbgAQkWnAJcDhJKKq+UC+O07JCSs9\n1c8dF/fjl+N6s3p7IVNeWshPXlrEOb1a8/R1Q0+KBoqrth/ggy0VDO7UjFsu6M0Z3Z1R/67M7oii\npPqPJNO7LzmNpo08XewaY2JMqsui61xB5Ns4Vx45OA0PzwJ+qaqv1rPdeGCcqk5yn08AhqvqlFrW\nvQsoqlGctRGn+/lK4HFVnVrH60wGJgNkZWUNmTZtWtDjqU1RUREZGRkhbxeu8irlg80VTPuqjIn9\nUvhap9C/MKMd8/EoKlN+//khSiuruGdUOk1SEitpJtK5rmYxR08ixh0Y85gxYxYcT3WBl+Ks24Gh\n7lUDItIamA0ETSINYJSq5olIG+B9EVmtqh/VXMlNLlMBsrOzdfTo0SG/UE5ODuFsdzzGqrLq0c/4\nJL+cu757TshXI7GIORSlFZU8/OE6dhWWsnL7AQrKSrh1aCO+eX7iDUkT7+e6NhZz9CRi3A0Zs5cW\n60nVCcS1x+N2eUDHgOcd3HmeqGqe+zcfmI5TPHbCEBGuP7MLG3YX88LcLbEOp0GVVVQx6Zn5/OPD\ndcxetZNt+w/xwBX96d7s5Kn/MeZk4eVK5F0ReQ94yX1+JTDTw3bzgJ4i0hUneVyFU0FfLxFJx0le\nhe70+cDdXrZNJBedfgqvLczjt68vR1WZOLJLrEM6bqrKr15dwsdrd/On8f35dvaR3xE5OWtjGJkx\nJhK8VKz/UkS+BZzpzpqqqtM9bFchIlOA93Bu8X3K7bzxJnf5YyLSFpiPM257lYj8FOgHtAKmu430\n/MCLqvpu6IcX35J9Sfxr4hCmvLiIO99Ywe6iMgCuHdmZlgk0fndhSTmvzM9l3qa9rNp+gE17DvLL\nC3oflUCMMScmT4NSqeprwGuh7lxVZ1LjqkVVHwuY3oFTzFXTAeCkGCow1e/j4e8M4tJHPuPvHzi/\n1B/733quHtqR27/ejxS/l5LD2Ji5bDu/m7GCkvJKCksq6NC8Ef1OacI1wzsz6ayusQ7PGBMFdSYR\nESkEart1SwBV1SYRi+okk+r38fh3h/D2su2c1bMVL3y5mWc+38zWfYf418RsfFG8BfhASTm/fX05\nlw1qzxndW/Hlxj0M79ryqGSmqry5dDs3T1vEqe2a0KVlOt87swtDOreIWpzGmPhQZxJRVRumL4o6\ntWzMD0Z3B+CPl/en3ylN+O0bK/jOv77A7xO+1ieLiSM7k+yLzJVJwaFy5m3cy8Nz1rF4634+XJ3P\nae2a8vmGPXRu2Zgzurdk856DVFQq63YVsbe4jCGdm/PCpOEnVYNJY8zRbIz1ODVhZBfKKpWnPtmI\n3yf84a2VvPjlZn57cT/OaeCW3C/N3cLdb67kUHklLdJTuOPrfXnhyy0s3LKPG8/uxv/W7OKV+bn0\nbpuJ35fEBadmcXr7ZlwysJ0lEGNOcpZE4tgNo7pyw6iuqCofrs7nD2+t5Lqn5zGsawuu7tww44Kt\n3VnI795YweDOzbj53F4M6tSMtGQfk87qRlWVkpQk3DquD6UVVTRKsYRhjDmaJZEEICKc2zeLUT1b\n8Z95W7n/ndXcs72KESMPcUrT8Dpy3FdcxqyVO7jnrVU0TvXxj6sHH9PRYXUDyKQksQRijKmVJZEE\nkur3MXFkFwZ1bM4V//yESx7+lJ+c25NrhnfyPGbJrsJSJj83n0Vb9gMwrGsLHhjf33rKNcaExZJI\nAjq9Q1NuHZbGuzsac8fry/nzrK+4dGB7bruwD0ki7CkurfUK5asdhfzfzFWs2HaAX17Qmz5tMxnT\nu81J0QGkMSYyLIkkqK5NfUz75gj+M28rn6zbzb8/28Qbi/Mor1SKSiu48exunH9qW0TgodlrWbuz\nkG0FJQDc9Y1+XHemteMwxhw/SyIJTES4algnrhrWie+O2MO0uVto0iiZwpIKHv9oA49/tOHwumN6\nt+bKoZ24IrvDSTkgljEmMiyJnCBGdGvJiG4tDz+fdFZXNu85SN6+Q6QlJzHhBOiXyxgTfyyJnKBO\nbdeUU9s1jXUYxpgTXPx2zGSMMSbuWRIxxhgTNksixhhjwmZJxBhjTNgsiRhjjAmbJRFjjDFhsyRi\njDEmbJZEjDHGhE1UaxsBNzGJyC5gcxibtgJ2N3A4kWYxR08ixm0xR08ixh0Yc2dVDXukuxMqiYRL\nROaranas4wiFxRw9iRi3xRw9iRh3Q8ZsxVnGGGPCZknEGGNM2CyJOKbGOoAwWMzRk4hxW8zRk4hx\nN1jMVidijDEmbHYlYowxJmyWRIwxxoTtpE4iIjJORL4SkXUiclus4wkkIptEZJmILBaR+e68FiLy\nvoisdf82D1j/1+5xfCUiF0QxzqdEJF9ElgfMCzlOERniHu86Efm7iEiUY75LRPLc871YRC6Ks5g7\nisgcEVkpIitE5GZ3ftye6yAxx/u5ThORuSKyxI379+78eD7XdcUc+XOtqiflA/AB64FuQAqwBOgX\n67gC4tsEtKox70/Abe70bcD97nQ/N/5UoKt7XL4oxXk2MBhYfjxxAnOBEYAA7wAXRjnmu4Bbalk3\nXmI+BRjsTmcCa9zY4vZcB4k53s+1ABnudDLwpfva8Xyu64o54uf6ZL4SGQasU9UNqloGTAMuiXFM\n9bkEeMadfga4NGD+NFUtVdWNwDqc44s4Vf0I2Hs8cYrIKUATVf1CnU/xswHbRCvmusRLzNtVdaE7\nXQisAtoTx+c6SMx1iXnMbqyqqkXu02T3ocT3ua4r5ro0WMwncxJpD2wNeJ5L8A94tCkwW0QWiMhk\nd16Wqm53p3cAWe50vB1LqHG2d6drzo+2H4vIUre4q7qoIu5iFpEuwCCcX5sJca5rxAxxfq5FxCci\ni4F84H1VjftzXUfMEOFzfTInkXg3SlUHAhcCPxKRswMXur8S4v7+7ESJE/gnTtHmQGA78JfYhlM7\nEckAXgN+qqoHApfF67muJea4P9eqWun+/3XA+YV+Wo3lcXeu64g54uf6ZE4ieUDHgOcd3HlxQVXz\n3L/5wHSc4qmd7uUm7t98d/V4O5ZQ48xzp2vOjxpV3en+E1YB/+JIcWDcxCwiyThfxi+o6n/d2XF9\nrmuLORHOdTVV3Q/MAcYR5+e6tpijca5P5iQyD+gpIl1FJAW4CpgR45gAEJF0EcmsngbOB5bjxHet\nu9q1wBvu9AzgKhFJFZGuQE+cyrFYCSlOt4jggIiMcO8EmRiwTVRUfzm4LsM533ETs/saTwKrVPWv\nAYvi9lzXFXMCnOvWItLMnW4EnAesJr7Pda0xR+VcN+QdAon2AC7CuWNkPXB7rOMJiKsbzp0TS4AV\n1bEBLYEPgLXAbKBFwDa3u8fxFRG8c6WWWF/CuUwuxyk/vSGcOIFs9wO+HngYtzeFKMb8HLAMWOr+\ng50SZzGPwik+WQosdh8XxfO5DhJzvJ/r/sAiN77lwJ3u/Hg+13XFHPFzbd2eGGOMCdvJXJxljDHm\nOFkSMcYYEzZLIsYYY8JmScQYY0zYLIkYY4wJmyURY0IkIl0koAdgD+tfJyLtPKzz8PFHZ0x0WRIx\nJvKuA4ImEWMSlSURY8LjF5EXRGSViLwqIo1F5E4RmSciy0VkqjjG4zTeesEdz6GRiAwVkc/EGfth\nboAgp8sAAAFuSURBVHXvBEA7EXlXnPEq/hTDYzPGM0sixoSnN/Coqvb9//buWCWPIArD8HsghRYp\nUlqopYUGU8TC0mBpKQh6B2lyC7mHBGMVLGwCuQU7LSR2gmBtYYo0muJPQPksZgUJaRwJ5of3qQZ2\nd9iFXT52Bs4BroC3wMckS0kWgElgLclX4BjYSiuOdwN8Ad4lWQRWgdEw5ytgA3gJbFTVNNJ/zhCR\n+pwnORzGe7QSHytVdVRVJ8AbYP4v180BF0m+ASS5SnI9HNtPcpnkF3AKzP7bR5Ae79lT34A0pv6s\nFxRgG3id5Lyq3gMTD5zz973xDX6fGgP+iUh9ZqpqeRhvAgfD+MfQP2P93rk/ae1hoRW7m6qqJYCq\nel5VhoXGli+v1OeM1izsM23p6RPwglb99Dut1cCdXWCnqkbAMm3f48NQsntE2xeRxpJVfCVJ3VzO\nkiR1M0QkSd0MEUlSN0NEktTNEJEkdTNEJEndDBFJUrdbD2WxxtT2rHEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4c239fda10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "plt.figure()\n",
    "plt.xlabel('batch')\n",
    "plt.ylabel('loss averaged over last 100 batches')\n",
    "plt.title('loss during training for vanilla conv. epoch 1')\n",
    "plt.grid(True)\n",
    "plt.plot(batches, avg_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attentional Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hacky but w/e\n",
    "ns = argparse.Namespace()\n",
    "ns.model = 'conv_attn'\n",
    "ns.n_epochs = 1\n",
    "ns.objective = 'bce'\n",
    "ns.filter_size = 3\n",
    "ns.num_filter_maps = 300\n",
    "ns.min_filter = None\n",
    "ns.max_filter = None\n",
    "ns.smoothing_window = None\n",
    "ns.norm_constraint = None\n",
    "ns.lstm_dim = None\n",
    "ns.cooccur_init = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ns = set_common_params(ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained embeddings...\n",
      "{'vocab_size': 'full', 'filter_size': 3, 'num_filter_maps': 300, 'dataset': 'disch', 'objective': 'bce', 'Y': 100, 'vocab_min': 3}\n"
     ]
    }
   ],
   "source": [
    "reload(training)\n",
    "args, model, optimizer, metrics_hist, metrics_hist_tr, model_dir, params, min_size, uneven_params,\\\n",
    "      dicts = training.init(ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 1 [batch #0, batch_size 1, seq length 13]\tLoss: 0.656557\n",
      "Train epoch: 1 [batch #10, batch_size 3, seq length 27]\tLoss: 0.442070\n",
      "Train epoch: 1 [batch #20, batch_size 5, seq length 37]\tLoss: 0.335086\n",
      "Train epoch: 1 [batch #30, batch_size 11, seq length 47]\tLoss: 0.288160\n",
      "Train epoch: 1 [batch #40, batch_size 16, seq length 54]\tLoss: 0.267900\n",
      "Train epoch: 1 [batch #50, batch_size 16, seq length 59]\tLoss: 0.256875\n",
      "Train epoch: 1 [batch #60, batch_size 9, seq length 64]\tLoss: 0.251043\n",
      "Train epoch: 1 [batch #70, batch_size 1, seq length 70]\tLoss: 0.246775\n",
      "Train epoch: 1 [batch #80, batch_size 13, seq length 79]\tLoss: 0.239728\n",
      "Train epoch: 1 [batch #90, batch_size 7, seq length 86]\tLoss: 0.235803\n",
      "Train epoch: 1 [batch #100, batch_size 9, seq length 96]\tLoss: 0.226997\n",
      "Train epoch: 1 [batch #110, batch_size 4, seq length 106]\tLoss: 0.202051\n",
      "Train epoch: 1 [batch #120, batch_size 3, seq length 116]\tLoss: 0.198518\n",
      "Train epoch: 1 [batch #130, batch_size 4, seq length 126]\tLoss: 0.196694\n",
      "Train epoch: 1 [batch #140, batch_size 5, seq length 137]\tLoss: 0.193706\n",
      "Train epoch: 1 [batch #150, batch_size 6, seq length 147]\tLoss: 0.190008\n",
      "Train epoch: 1 [batch #160, batch_size 3, seq length 157]\tLoss: 0.183051\n",
      "Train epoch: 1 [batch #170, batch_size 3, seq length 167]\tLoss: 0.176294\n",
      "Train epoch: 1 [batch #180, batch_size 3, seq length 177]\tLoss: 0.175213\n",
      "Train epoch: 1 [batch #190, batch_size 4, seq length 187]\tLoss: 0.168704\n",
      "Train epoch: 1 [batch #200, batch_size 4, seq length 197]\tLoss: 0.164905\n",
      "Train epoch: 1 [batch #210, batch_size 4, seq length 207]\tLoss: 0.163493\n",
      "Train epoch: 1 [batch #220, batch_size 9, seq length 217]\tLoss: 0.160863\n",
      "Train epoch: 1 [batch #230, batch_size 8, seq length 227]\tLoss: 0.158511\n",
      "Train epoch: 1 [batch #240, batch_size 7, seq length 237]\tLoss: 0.156726\n",
      "Train epoch: 1 [batch #250, batch_size 10, seq length 247]\tLoss: 0.154933\n",
      "Train epoch: 1 [batch #260, batch_size 7, seq length 257]\tLoss: 0.155088\n",
      "Train epoch: 1 [batch #270, batch_size 11, seq length 267]\tLoss: 0.154646\n",
      "Train epoch: 1 [batch #280, batch_size 13, seq length 277]\tLoss: 0.152476\n",
      "Train epoch: 1 [batch #290, batch_size 8, seq length 287]\tLoss: 0.151650\n",
      "Train epoch: 1 [batch #300, batch_size 9, seq length 297]\tLoss: 0.151163\n",
      "Train epoch: 1 [batch #310, batch_size 10, seq length 307]\tLoss: 0.150352\n",
      "Train epoch: 1 [batch #320, batch_size 7, seq length 317]\tLoss: 0.148979\n",
      "Train epoch: 1 [batch #330, batch_size 12, seq length 327]\tLoss: 0.149015\n",
      "Train epoch: 1 [batch #340, batch_size 11, seq length 337]\tLoss: 0.146904\n",
      "Train epoch: 1 [batch #350, batch_size 16, seq length 347]\tLoss: 0.143933\n",
      "Train epoch: 1 [batch #360, batch_size 9, seq length 356]\tLoss: 0.142333\n",
      "Train epoch: 1 [batch #370, batch_size 7, seq length 365]\tLoss: 0.141131\n",
      "Train epoch: 1 [batch #380, batch_size 12, seq length 374]\tLoss: 0.140018\n",
      "Train epoch: 1 [batch #390, batch_size 16, seq length 382]\tLoss: 0.141301\n",
      "Train epoch: 1 [batch #400, batch_size 11, seq length 390]\tLoss: 0.140974\n",
      "Train epoch: 1 [batch #410, batch_size 16, seq length 398]\tLoss: 0.141189\n",
      "Train epoch: 1 [batch #420, batch_size 3, seq length 404]\tLoss: 0.140596\n",
      "Train epoch: 1 [batch #430, batch_size 14, seq length 412]\tLoss: 0.139538\n",
      "Train epoch: 1 [batch #440, batch_size 3, seq length 420]\tLoss: 0.140253\n",
      "Train epoch: 1 [batch #450, batch_size 4, seq length 426]\tLoss: 0.141023\n",
      "Train epoch: 1 [batch #460, batch_size 5, seq length 432]\tLoss: 0.141537\n",
      "Train epoch: 1 [batch #470, batch_size 10, seq length 439]\tLoss: 0.142021\n",
      "Train epoch: 1 [batch #480, batch_size 16, seq length 445]\tLoss: 0.141268\n",
      "Train epoch: 1 [batch #490, batch_size 4, seq length 450]\tLoss: 0.141763\n",
      "Train epoch: 1 [batch #500, batch_size 9, seq length 458]\tLoss: 0.140908\n",
      "Train epoch: 1 [batch #510, batch_size 9, seq length 463]\tLoss: 0.140482\n",
      "Train epoch: 1 [batch #520, batch_size 16, seq length 470]\tLoss: 0.140376\n",
      "Train epoch: 1 [batch #530, batch_size 7, seq length 475]\tLoss: 0.139806\n",
      "Train epoch: 1 [batch #540, batch_size 6, seq length 481]\tLoss: 0.139015\n",
      "Train epoch: 1 [batch #550, batch_size 16, seq length 487]\tLoss: 0.138761\n",
      "Train epoch: 1 [batch #560, batch_size 16, seq length 493]\tLoss: 0.138958\n",
      "Train epoch: 1 [batch #570, batch_size 16, seq length 498]\tLoss: 0.137858\n",
      "Train epoch: 1 [batch #580, batch_size 16, seq length 503]\tLoss: 0.138407\n",
      "Train epoch: 1 [batch #590, batch_size 16, seq length 509]\tLoss: 0.138741\n",
      "Train epoch: 1 [batch #600, batch_size 16, seq length 515]\tLoss: 0.138236\n",
      "Train epoch: 1 [batch #610, batch_size 5, seq length 520]\tLoss: 0.137644\n",
      "Train epoch: 1 [batch #620, batch_size 16, seq length 526]\tLoss: 0.137365\n",
      "Train epoch: 1 [batch #630, batch_size 16, seq length 532]\tLoss: 0.139106\n",
      "Train epoch: 1 [batch #640, batch_size 16, seq length 537]\tLoss: 0.139522\n",
      "Train epoch: 1 [batch #650, batch_size 7, seq length 542]\tLoss: 0.139997\n",
      "Train epoch: 1 [batch #660, batch_size 5, seq length 547]\tLoss: 0.139451\n",
      "Train epoch: 1 [batch #670, batch_size 3, seq length 553]\tLoss: 0.142198\n",
      "Train epoch: 1 [batch #680, batch_size 4, seq length 558]\tLoss: 0.142905\n",
      "Train epoch: 1 [batch #690, batch_size 16, seq length 565]\tLoss: 0.141356\n",
      "Train epoch: 1 [batch #700, batch_size 16, seq length 570]\tLoss: 0.143674\n",
      "Train epoch: 1 [batch #710, batch_size 16, seq length 575]\tLoss: 0.144768\n",
      "Train epoch: 1 [batch #720, batch_size 1, seq length 580]\tLoss: 0.146971\n",
      "Train epoch: 1 [batch #730, batch_size 4, seq length 585]\tLoss: 0.146111\n",
      "Train epoch: 1 [batch #740, batch_size 3, seq length 590]\tLoss: 0.145494\n",
      "Train epoch: 1 [batch #750, batch_size 7, seq length 595]\tLoss: 0.146245\n",
      "Train epoch: 1 [batch #760, batch_size 5, seq length 600]\tLoss: 0.146885\n",
      "Train epoch: 1 [batch #770, batch_size 16, seq length 605]\tLoss: 0.145610\n",
      "Train epoch: 1 [batch #780, batch_size 16, seq length 610]\tLoss: 0.145108\n",
      "Train epoch: 1 [batch #790, batch_size 16, seq length 615]\tLoss: 0.146607\n",
      "Train epoch: 1 [batch #800, batch_size 16, seq length 620]\tLoss: 0.143872\n",
      "Train epoch: 1 [batch #810, batch_size 3, seq length 625]\tLoss: 0.143304\n",
      "Train epoch: 1 [batch #820, batch_size 16, seq length 631]\tLoss: 0.142418\n",
      "Train epoch: 1 [batch #830, batch_size 2, seq length 636]\tLoss: 0.142428\n",
      "Train epoch: 1 [batch #840, batch_size 6, seq length 641]\tLoss: 0.142845\n",
      "Train epoch: 1 [batch #850, batch_size 16, seq length 647]\tLoss: 0.141995\n",
      "Train epoch: 1 [batch #860, batch_size 16, seq length 652]\tLoss: 0.142727\n",
      "Train epoch: 1 [batch #870, batch_size 16, seq length 657]\tLoss: 0.143497\n",
      "Train epoch: 1 [batch #880, batch_size 16, seq length 663]\tLoss: 0.143808\n",
      "Train epoch: 1 [batch #890, batch_size 15, seq length 668]\tLoss: 0.142532\n",
      "Train epoch: 1 [batch #900, batch_size 3, seq length 673]\tLoss: 0.143337\n",
      "Train epoch: 1 [batch #910, batch_size 16, seq length 679]\tLoss: 0.143121\n",
      "Train epoch: 1 [batch #920, batch_size 2, seq length 684]\tLoss: 0.143315\n",
      "Train epoch: 1 [batch #930, batch_size 16, seq length 690]\tLoss: 0.144100\n",
      "Train epoch: 1 [batch #940, batch_size 3, seq length 695]\tLoss: 0.145827\n",
      "Train epoch: 1 [batch #950, batch_size 16, seq length 701]\tLoss: 0.148351\n",
      "Train epoch: 1 [batch #960, batch_size 16, seq length 706]\tLoss: 0.149306\n",
      "Train epoch: 1 [batch #970, batch_size 2, seq length 711]\tLoss: 0.148431\n",
      "Train epoch: 1 [batch #980, batch_size 16, seq length 717]\tLoss: 0.149961\n",
      "Train epoch: 1 [batch #990, batch_size 13, seq length 722]\tLoss: 0.152193\n",
      "Train epoch: 1 [batch #1000, batch_size 15, seq length 727]\tLoss: 0.154655\n",
      "Train epoch: 1 [batch #1010, batch_size 16, seq length 733]\tLoss: 0.157404\n",
      "Train epoch: 1 [batch #1020, batch_size 16, seq length 738]\tLoss: 0.157486\n",
      "Train epoch: 1 [batch #1030, batch_size 9, seq length 742]\tLoss: 0.158531\n",
      "Train epoch: 1 [batch #1040, batch_size 16, seq length 748]\tLoss: 0.159520\n",
      "Train epoch: 1 [batch #1050, batch_size 16, seq length 753]\tLoss: 0.158161\n",
      "Train epoch: 1 [batch #1060, batch_size 16, seq length 760]\tLoss: 0.156383\n",
      "Train epoch: 1 [batch #1070, batch_size 16, seq length 765]\tLoss: 0.157665\n",
      "Train epoch: 1 [batch #1080, batch_size 16, seq length 770]\tLoss: 0.156879\n",
      "Train epoch: 1 [batch #1090, batch_size 4, seq length 774]\tLoss: 0.155567\n",
      "Train epoch: 1 [batch #1100, batch_size 4, seq length 779]\tLoss: 0.157059\n",
      "Train epoch: 1 [batch #1110, batch_size 1, seq length 784]\tLoss: 0.156092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 1 [batch #1120, batch_size 7, seq length 790]\tLoss: 0.158532\n",
      "Train epoch: 1 [batch #1130, batch_size 16, seq length 796]\tLoss: 0.158386\n",
      "Train epoch: 1 [batch #1140, batch_size 16, seq length 801]\tLoss: 0.158614\n",
      "Train epoch: 1 [batch #1150, batch_size 16, seq length 806]\tLoss: 0.161573\n",
      "Train epoch: 1 [batch #1160, batch_size 12, seq length 812]\tLoss: 0.162057\n",
      "Train epoch: 1 [batch #1170, batch_size 9, seq length 817]\tLoss: 0.162039\n",
      "Train epoch: 1 [batch #1180, batch_size 16, seq length 823]\tLoss: 0.163559\n",
      "Train epoch: 1 [batch #1190, batch_size 13, seq length 828]\tLoss: 0.164705\n",
      "Train epoch: 1 [batch #1200, batch_size 7, seq length 833]\tLoss: 0.163765\n",
      "Train epoch: 1 [batch #1210, batch_size 16, seq length 839]\tLoss: 0.165059\n",
      "Train epoch: 1 [batch #1220, batch_size 16, seq length 844]\tLoss: 0.162044\n",
      "Train epoch: 1 [batch #1230, batch_size 16, seq length 849]\tLoss: 0.163990\n",
      "Train epoch: 1 [batch #1240, batch_size 16, seq length 854]\tLoss: 0.164445\n",
      "Train epoch: 1 [batch #1250, batch_size 16, seq length 860]\tLoss: 0.164119\n",
      "Train epoch: 1 [batch #1260, batch_size 16, seq length 865]\tLoss: 0.167482\n",
      "Train epoch: 1 [batch #1270, batch_size 1, seq length 871]\tLoss: 0.170020\n",
      "Train epoch: 1 [batch #1280, batch_size 9, seq length 876]\tLoss: 0.170155\n",
      "Train epoch: 1 [batch #1290, batch_size 1, seq length 881]\tLoss: 0.172086\n",
      "Train epoch: 1 [batch #1300, batch_size 16, seq length 887]\tLoss: 0.171998\n",
      "Train epoch: 1 [batch #1310, batch_size 4, seq length 892]\tLoss: 0.170928\n",
      "Train epoch: 1 [batch #1320, batch_size 16, seq length 898]\tLoss: 0.173454\n",
      "Train epoch: 1 [batch #1330, batch_size 16, seq length 904]\tLoss: 0.175080\n",
      "Train epoch: 1 [batch #1340, batch_size 16, seq length 910]\tLoss: 0.175558\n",
      "Train epoch: 1 [batch #1350, batch_size 16, seq length 915]\tLoss: 0.176133\n",
      "Train epoch: 1 [batch #1360, batch_size 4, seq length 920]\tLoss: 0.174857\n",
      "Train epoch: 1 [batch #1370, batch_size 16, seq length 926]\tLoss: 0.173876\n",
      "Train epoch: 1 [batch #1380, batch_size 16, seq length 931]\tLoss: 0.173943\n",
      "Train epoch: 1 [batch #1390, batch_size 16, seq length 936]\tLoss: 0.173064\n",
      "Train epoch: 1 [batch #1400, batch_size 6, seq length 941]\tLoss: 0.174854\n",
      "Train epoch: 1 [batch #1410, batch_size 16, seq length 947]\tLoss: 0.176740\n",
      "Train epoch: 1 [batch #1420, batch_size 6, seq length 952]\tLoss: 0.179842\n",
      "Train epoch: 1 [batch #1430, batch_size 16, seq length 958]\tLoss: 0.179802\n",
      "Train epoch: 1 [batch #1440, batch_size 16, seq length 964]\tLoss: 0.180099\n",
      "Train epoch: 1 [batch #1450, batch_size 7, seq length 969]\tLoss: 0.181471\n",
      "Train epoch: 1 [batch #1460, batch_size 16, seq length 974]\tLoss: 0.182127\n",
      "Train epoch: 1 [batch #1470, batch_size 16, seq length 979]\tLoss: 0.184111\n",
      "Train epoch: 1 [batch #1480, batch_size 3, seq length 985]\tLoss: 0.183604\n",
      "Train epoch: 1 [batch #1490, batch_size 16, seq length 991]\tLoss: 0.185201\n",
      "Train epoch: 1 [batch #1500, batch_size 7, seq length 996]\tLoss: 0.185627\n",
      "Train epoch: 1 [batch #1510, batch_size 16, seq length 1002]\tLoss: 0.187564\n",
      "Train epoch: 1 [batch #1520, batch_size 16, seq length 1007]\tLoss: 0.186347\n",
      "Train epoch: 1 [batch #1530, batch_size 16, seq length 1012]\tLoss: 0.185474\n",
      "Train epoch: 1 [batch #1540, batch_size 16, seq length 1018]\tLoss: 0.186328\n",
      "Train epoch: 1 [batch #1550, batch_size 2, seq length 1023]\tLoss: 0.186056\n",
      "Train epoch: 1 [batch #1560, batch_size 16, seq length 1029]\tLoss: 0.188672\n",
      "Train epoch: 1 [batch #1570, batch_size 16, seq length 1035]\tLoss: 0.189129\n",
      "Train epoch: 1 [batch #1580, batch_size 1, seq length 1040]\tLoss: 0.193311\n",
      "Train epoch: 1 [batch #1590, batch_size 5, seq length 1045]\tLoss: 0.196274\n",
      "Train epoch: 1 [batch #1600, batch_size 13, seq length 1050]\tLoss: 0.198394\n",
      "Train epoch: 1 [batch #1610, batch_size 5, seq length 1055]\tLoss: 0.195060\n",
      "Train epoch: 1 [batch #1620, batch_size 16, seq length 1061]\tLoss: 0.196096\n",
      "Train epoch: 1 [batch #1630, batch_size 10, seq length 1066]\tLoss: 0.197943\n",
      "Train epoch: 1 [batch #1640, batch_size 16, seq length 1072]\tLoss: 0.197561\n",
      "Train epoch: 1 [batch #1650, batch_size 15, seq length 1077]\tLoss: 0.198242\n",
      "Train epoch: 1 [batch #1660, batch_size 8, seq length 1083]\tLoss: 0.196526\n",
      "Train epoch: 1 [batch #1670, batch_size 15, seq length 1089]\tLoss: 0.196626\n",
      "Train epoch: 1 [batch #1680, batch_size 3, seq length 1094]\tLoss: 0.196549\n",
      "Train epoch: 1 [batch #1690, batch_size 12, seq length 1100]\tLoss: 0.194896\n",
      "Train epoch: 1 [batch #1700, batch_size 11, seq length 1106]\tLoss: 0.194668\n",
      "Train epoch: 1 [batch #1710, batch_size 2, seq length 1112]\tLoss: 0.197046\n",
      "Train epoch: 1 [batch #1720, batch_size 16, seq length 1118]\tLoss: 0.197196\n",
      "Train epoch: 1 [batch #1730, batch_size 16, seq length 1124]\tLoss: 0.197280\n",
      "Train epoch: 1 [batch #1740, batch_size 13, seq length 1130]\tLoss: 0.199425\n",
      "Train epoch: 1 [batch #1750, batch_size 15, seq length 1138]\tLoss: 0.201112\n",
      "Train epoch: 1 [batch #1760, batch_size 16, seq length 1144]\tLoss: 0.203481\n",
      "Train epoch: 1 [batch #1770, batch_size 16, seq length 1150]\tLoss: 0.202872\n",
      "Train epoch: 1 [batch #1780, batch_size 4, seq length 1155]\tLoss: 0.200001\n",
      "Train epoch: 1 [batch #1790, batch_size 4, seq length 1163]\tLoss: 0.200444\n",
      "Train epoch: 1 [batch #1800, batch_size 16, seq length 1169]\tLoss: 0.203135\n",
      "Train epoch: 1 [batch #1810, batch_size 5, seq length 1174]\tLoss: 0.203833\n",
      "Train epoch: 1 [batch #1820, batch_size 16, seq length 1182]\tLoss: 0.203314\n",
      "Train epoch: 1 [batch #1830, batch_size 2, seq length 1189]\tLoss: 0.203075\n",
      "Train epoch: 1 [batch #1840, batch_size 5, seq length 1194]\tLoss: 0.203772\n",
      "Train epoch: 1 [batch #1850, batch_size 12, seq length 1202]\tLoss: 0.203049\n",
      "Train epoch: 1 [batch #1860, batch_size 4, seq length 1210]\tLoss: 0.202716\n",
      "Train epoch: 1 [batch #1870, batch_size 14, seq length 1218]\tLoss: 0.203776\n",
      "Train epoch: 1 [batch #1880, batch_size 3, seq length 1226]\tLoss: 0.206706\n",
      "Train epoch: 1 [batch #1890, batch_size 16, seq length 1234]\tLoss: 0.210258\n",
      "Train epoch: 1 [batch #1900, batch_size 16, seq length 1242]\tLoss: 0.207630\n",
      "Train epoch: 1 [batch #1910, batch_size 5, seq length 1248]\tLoss: 0.209502\n",
      "Train epoch: 1 [batch #1920, batch_size 16, seq length 1255]\tLoss: 0.209122\n",
      "Train epoch: 1 [batch #1930, batch_size 16, seq length 1263]\tLoss: 0.208640\n",
      "Train epoch: 1 [batch #1940, batch_size 16, seq length 1269]\tLoss: 0.207617\n",
      "Train epoch: 1 [batch #1950, batch_size 14, seq length 1279]\tLoss: 0.207066\n",
      "Train epoch: 1 [batch #1960, batch_size 3, seq length 1287]\tLoss: 0.208015\n",
      "Train epoch: 1 [batch #1970, batch_size 16, seq length 1297]\tLoss: 0.209246\n",
      "Train epoch: 1 [batch #1980, batch_size 10, seq length 1304]\tLoss: 0.211371\n",
      "Train epoch: 1 [batch #1990, batch_size 12, seq length 1312]\tLoss: 0.208094\n",
      "Train epoch: 1 [batch #2000, batch_size 13, seq length 1322]\tLoss: 0.209260\n",
      "Train epoch: 1 [batch #2010, batch_size 13, seq length 1330]\tLoss: 0.208377\n",
      "Train epoch: 1 [batch #2020, batch_size 14, seq length 1339]\tLoss: 0.212483\n",
      "Train epoch: 1 [batch #2030, batch_size 7, seq length 1349]\tLoss: 0.214145\n",
      "Train epoch: 1 [batch #2040, batch_size 12, seq length 1358]\tLoss: 0.217208\n",
      "Train epoch: 1 [batch #2050, batch_size 14, seq length 1367]\tLoss: 0.218105\n",
      "Train epoch: 1 [batch #2060, batch_size 11, seq length 1377]\tLoss: 0.218106\n",
      "Train epoch: 1 [batch #2070, batch_size 10, seq length 1387]\tLoss: 0.218658\n",
      "Train epoch: 1 [batch #2080, batch_size 8, seq length 1396]\tLoss: 0.217318\n",
      "Train epoch: 1 [batch #2090, batch_size 16, seq length 1404]\tLoss: 0.218859\n",
      "Train epoch: 1 [batch #2100, batch_size 11, seq length 1412]\tLoss: 0.218591\n",
      "Train epoch: 1 [batch #2110, batch_size 16, seq length 1422]\tLoss: 0.219808\n",
      "Train epoch: 1 [batch #2120, batch_size 11, seq length 1432]\tLoss: 0.219547\n",
      "Train epoch: 1 [batch #2130, batch_size 16, seq length 1442]\tLoss: 0.220220\n",
      "Train epoch: 1 [batch #2140, batch_size 1, seq length 1450]\tLoss: 0.218472\n",
      "Train epoch: 1 [batch #2150, batch_size 7, seq length 1460]\tLoss: 0.219922\n",
      "Train epoch: 1 [batch #2160, batch_size 10, seq length 1469]\tLoss: 0.220094\n",
      "Train epoch: 1 [batch #2170, batch_size 12, seq length 1479]\tLoss: 0.219669\n",
      "Train epoch: 1 [batch #2180, batch_size 9, seq length 1489]\tLoss: 0.219596\n",
      "Train epoch: 1 [batch #2190, batch_size 14, seq length 1499]\tLoss: 0.219994\n",
      "Train epoch: 1 [batch #2200, batch_size 7, seq length 1508]\tLoss: 0.220073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 1 [batch #2210, batch_size 11, seq length 1518]\tLoss: 0.220018\n",
      "Train epoch: 1 [batch #2220, batch_size 6, seq length 1528]\tLoss: 0.218650\n",
      "Train epoch: 1 [batch #2230, batch_size 7, seq length 1538]\tLoss: 0.219638\n",
      "Train epoch: 1 [batch #2240, batch_size 8, seq length 1548]\tLoss: 0.221583\n",
      "Train epoch: 1 [batch #2250, batch_size 8, seq length 1558]\tLoss: 0.221936\n",
      "Train epoch: 1 [batch #2260, batch_size 6, seq length 1568]\tLoss: 0.223969\n",
      "Train epoch: 1 [batch #2270, batch_size 13, seq length 1578]\tLoss: 0.226968\n",
      "Train epoch: 1 [batch #2280, batch_size 7, seq length 1588]\tLoss: 0.228304\n",
      "Train epoch: 1 [batch #2290, batch_size 5, seq length 1598]\tLoss: 0.228524\n",
      "Train epoch: 1 [batch #2300, batch_size 8, seq length 1608]\tLoss: 0.230368\n",
      "Train epoch: 1 [batch #2310, batch_size 3, seq length 1618]\tLoss: 0.232275\n",
      "Train epoch: 1 [batch #2320, batch_size 6, seq length 1628]\tLoss: 0.233322\n",
      "Train epoch: 1 [batch #2330, batch_size 9, seq length 1638]\tLoss: 0.233168\n",
      "Train epoch: 1 [batch #2340, batch_size 7, seq length 1648]\tLoss: 0.230997\n",
      "Train epoch: 1 [batch #2350, batch_size 4, seq length 1658]\tLoss: 0.231784\n",
      "Train epoch: 1 [batch #2360, batch_size 9, seq length 1668]\tLoss: 0.230207\n",
      "Train epoch: 1 [batch #2370, batch_size 8, seq length 1678]\tLoss: 0.227263\n",
      "Train epoch: 1 [batch #2380, batch_size 4, seq length 1688]\tLoss: 0.227126\n",
      "Train epoch: 1 [batch #2390, batch_size 10, seq length 1698]\tLoss: 0.228643\n",
      "Train epoch: 1 [batch #2400, batch_size 6, seq length 1708]\tLoss: 0.229929\n",
      "Train epoch: 1 [batch #2410, batch_size 4, seq length 1718]\tLoss: 0.230289\n",
      "Train epoch: 1 [batch #2420, batch_size 9, seq length 1728]\tLoss: 0.233473\n",
      "Train epoch: 1 [batch #2430, batch_size 6, seq length 1738]\tLoss: 0.233329\n",
      "Train epoch: 1 [batch #2440, batch_size 5, seq length 1748]\tLoss: 0.234700\n",
      "Train epoch: 1 [batch #2450, batch_size 6, seq length 1758]\tLoss: 0.237713\n",
      "Train epoch: 1 [batch #2460, batch_size 4, seq length 1768]\tLoss: 0.240918\n",
      "Train epoch: 1 [batch #2470, batch_size 7, seq length 1778]\tLoss: 0.242899\n",
      "Train epoch: 1 [batch #2480, batch_size 3, seq length 1788]\tLoss: 0.244374\n",
      "Train epoch: 1 [batch #2490, batch_size 5, seq length 1798]\tLoss: 0.246405\n",
      "Train epoch: 1 [batch #2500, batch_size 8, seq length 1808]\tLoss: 0.244913\n",
      "Train epoch: 1 [batch #2510, batch_size 2, seq length 1818]\tLoss: 0.243268\n",
      "Train epoch: 1 [batch #2520, batch_size 2, seq length 1828]\tLoss: 0.242890\n",
      "Train epoch: 1 [batch #2530, batch_size 4, seq length 1838]\tLoss: 0.245945\n",
      "Train epoch: 1 [batch #2540, batch_size 5, seq length 1848]\tLoss: 0.248516\n",
      "Train epoch: 1 [batch #2550, batch_size 1, seq length 1858]\tLoss: 0.249402\n",
      "Train epoch: 1 [batch #2560, batch_size 3, seq length 1868]\tLoss: 0.249498\n",
      "Train epoch: 1 [batch #2570, batch_size 6, seq length 1878]\tLoss: 0.251582\n",
      "Train epoch: 1 [batch #2580, batch_size 2, seq length 1889]\tLoss: 0.253979\n",
      "Train epoch: 1 [batch #2590, batch_size 1, seq length 1899]\tLoss: 0.252090\n",
      "Train epoch: 1 [batch #2600, batch_size 4, seq length 1910]\tLoss: 0.253557\n",
      "Train epoch: 1 [batch #2610, batch_size 1, seq length 1920]\tLoss: 0.258227\n",
      "Train epoch: 1 [batch #2620, batch_size 5, seq length 1930]\tLoss: 0.255172\n",
      "Train epoch: 1 [batch #2630, batch_size 3, seq length 1941]\tLoss: 0.253751\n",
      "Train epoch: 1 [batch #2640, batch_size 4, seq length 1951]\tLoss: 0.252716\n",
      "Train epoch: 1 [batch #2650, batch_size 4, seq length 1961]\tLoss: 0.250698\n",
      "Train epoch: 1 [batch #2660, batch_size 4, seq length 1971]\tLoss: 0.250360\n",
      "Train epoch: 1 [batch #2670, batch_size 2, seq length 1982]\tLoss: 0.249236\n",
      "Train epoch: 1 [batch #2680, batch_size 5, seq length 1993]\tLoss: 0.248813\n",
      "Train epoch: 1 [batch #2690, batch_size 7, seq length 2003]\tLoss: 0.250009\n",
      "Train epoch: 1 [batch #2700, batch_size 3, seq length 2017]\tLoss: 0.249088\n",
      "Train epoch: 1 [batch #2710, batch_size 1, seq length 2028]\tLoss: 0.246719\n",
      "Train epoch: 1 [batch #2720, batch_size 7, seq length 2039]\tLoss: 0.249958\n",
      "Train epoch: 1 [batch #2730, batch_size 1, seq length 2049]\tLoss: 0.250510\n",
      "Train epoch: 1 [batch #2740, batch_size 2, seq length 2059]\tLoss: 0.254788\n",
      "Train epoch: 1 [batch #2750, batch_size 2, seq length 2071]\tLoss: 0.254343\n",
      "Train epoch: 1 [batch #2760, batch_size 3, seq length 2082]\tLoss: 0.255306\n",
      "Train epoch: 1 [batch #2770, batch_size 1, seq length 2095]\tLoss: 0.254868\n",
      "Train epoch: 1 [batch #2780, batch_size 4, seq length 2105]\tLoss: 0.251257\n",
      "Train epoch: 1 [batch #2790, batch_size 4, seq length 2118]\tLoss: 0.257009\n",
      "Train epoch: 1 [batch #2800, batch_size 4, seq length 2129]\tLoss: 0.258546\n",
      "Train epoch: 1 [batch #2810, batch_size 4, seq length 2140]\tLoss: 0.258827\n",
      "Train epoch: 1 [batch #2820, batch_size 2, seq length 2151]\tLoss: 0.256001\n",
      "Train epoch: 1 [batch #2830, batch_size 1, seq length 2163]\tLoss: 0.256350\n",
      "Train epoch: 1 [batch #2840, batch_size 2, seq length 2173]\tLoss: 0.253950\n",
      "Train epoch: 1 [batch #2850, batch_size 2, seq length 2186]\tLoss: 0.254303\n",
      "Train epoch: 1 [batch #2860, batch_size 1, seq length 2196]\tLoss: 0.255044\n",
      "Train epoch: 1 [batch #2870, batch_size 2, seq length 2206]\tLoss: 0.255077\n",
      "Train epoch: 1 [batch #2880, batch_size 1, seq length 2216]\tLoss: 0.257025\n",
      "Train epoch: 1 [batch #2890, batch_size 3, seq length 2231]\tLoss: 0.249546\n",
      "Train epoch: 1 [batch #2900, batch_size 1, seq length 2242]\tLoss: 0.251123\n",
      "Train epoch: 1 [batch #2910, batch_size 1, seq length 2255]\tLoss: 0.249954\n",
      "Train epoch: 1 [batch #2920, batch_size 1, seq length 2267]\tLoss: 0.254465\n",
      "Train epoch: 1 [batch #2930, batch_size 1, seq length 2280]\tLoss: 0.252683\n",
      "Train epoch: 1 [batch #2940, batch_size 4, seq length 2294]\tLoss: 0.253601\n",
      "Train epoch: 1 [batch #2950, batch_size 1, seq length 2312]\tLoss: 0.258859\n",
      "Train epoch: 1 [batch #2960, batch_size 2, seq length 2325]\tLoss: 0.255205\n",
      "Train epoch: 1 [batch #2970, batch_size 3, seq length 2338]\tLoss: 0.257343\n",
      "Train epoch: 1 [batch #2980, batch_size 1, seq length 2353]\tLoss: 0.260797\n",
      "Train epoch: 1 [batch #2990, batch_size 1, seq length 2367]\tLoss: 0.262387\n",
      "Train epoch: 1 [batch #3000, batch_size 1, seq length 2378]\tLoss: 0.261351\n",
      "Train epoch: 1 [batch #3010, batch_size 2, seq length 2391]\tLoss: 0.265504\n",
      "Train epoch: 1 [batch #3020, batch_size 1, seq length 2408]\tLoss: 0.262968\n",
      "Train epoch: 1 [batch #3030, batch_size 1, seq length 2420]\tLoss: 0.265842\n",
      "Train epoch: 1 [batch #3040, batch_size 2, seq length 2437]\tLoss: 0.264715\n",
      "Train epoch: 1 [batch #3050, batch_size 1, seq length 2457]\tLoss: 0.260707\n",
      "Train epoch: 1 [batch #3060, batch_size 1, seq length 2473]\tLoss: 0.263366\n",
      "Train epoch: 1 [batch #3070, batch_size 2, seq length 2486]\tLoss: 0.266018\n",
      "Train epoch: 1 [batch #3080, batch_size 4, seq length 2503]\tLoss: 0.259042\n",
      "Train epoch: 1 [batch #3090, batch_size 1, seq length 2517]\tLoss: 0.260903\n",
      "Train epoch: 1 [batch #3100, batch_size 1, seq length 2531]\tLoss: 0.263071\n",
      "Train epoch: 1 [batch #3110, batch_size 1, seq length 2545]\tLoss: 0.259920\n",
      "Train epoch: 1 [batch #3120, batch_size 1, seq length 2565]\tLoss: 0.264316\n",
      "Train epoch: 1 [batch #3130, batch_size 1, seq length 2584]\tLoss: 0.266821\n",
      "Train epoch: 1 [batch #3140, batch_size 1, seq length 2601]\tLoss: 0.268925\n",
      "Train epoch: 1 [batch #3150, batch_size 1, seq length 2631]\tLoss: 0.267197\n",
      "Train epoch: 1 [batch #3160, batch_size 1, seq length 2661]\tLoss: 0.268972\n",
      "Train epoch: 1 [batch #3170, batch_size 1, seq length 2681]\tLoss: 0.263769\n",
      "Train epoch: 1 [batch #3180, batch_size 1, seq length 2714]\tLoss: 0.267200\n",
      "Train epoch: 1 [batch #3190, batch_size 1, seq length 2746]\tLoss: 0.268562\n",
      "Train epoch: 1 [batch #3200, batch_size 1, seq length 2766]\tLoss: 0.263640\n",
      "Train epoch: 1 [batch #3210, batch_size 1, seq length 2788]\tLoss: 0.264783\n",
      "Train epoch: 1 [batch #3220, batch_size 2, seq length 2815]\tLoss: 0.259908\n",
      "Train epoch: 1 [batch #3230, batch_size 2, seq length 2836]\tLoss: 0.257183\n",
      "Train epoch: 1 [batch #3240, batch_size 1, seq length 2853]\tLoss: 0.260099\n",
      "Train epoch: 1 [batch #3250, batch_size 1, seq length 2898]\tLoss: 0.261543\n",
      "Train epoch: 1 [batch #3260, batch_size 1, seq length 2936]\tLoss: 0.262710\n",
      "Train epoch: 1 [batch #3270, batch_size 1, seq length 3004]\tLoss: 0.265504\n",
      "Train epoch: 1 [batch #3280, batch_size 1, seq length 3032]\tLoss: 0.264748\n",
      "Train epoch: 1 [batch #3290, batch_size 1, seq length 3108]\tLoss: 0.260037\n",
      "Train epoch: 1 [batch #3300, batch_size 1, seq length 3183]\tLoss: 0.262538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 1 [batch #3310, batch_size 1, seq length 3260]\tLoss: 0.264855\n",
      "Train epoch: 1 [batch #3320, batch_size 1, seq length 3395]\tLoss: 0.267769\n",
      "Train epoch: 1 [batch #3330, batch_size 1, seq length 3525]\tLoss: 0.264025\n",
      "Train epoch: 1 [batch #3340, batch_size 2, seq length 3616]\tLoss: 0.257802\n",
      "Train epoch: 1 [batch #3350, batch_size 1, seq length 3795]\tLoss: 0.254153\n",
      "Train epoch: 1 [batch #3360, batch_size 1, seq length 4038]\tLoss: 0.249443\n",
      "Train epoch: 1 [batch #3370, batch_size 1, seq length 4505]\tLoss: 0.248409\n",
      "train time: 136.671515942 s\n",
      "epoch loss: 0.201802774403\n",
      "evaluating on dev\n",
      "sample prediction\n",
      "Y_true: [19 28 38 60]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 10 19 38 39 50 56 90]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 28 46 56 92]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 19 54 96]\n",
      "Y_hat: [83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  5 16 19 28 31 38 39 45 49 70 73 92 98]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 26 27 28 29]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 19 20 26 35 96]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 19 28 29 38]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3 19 27 29 39 41]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 83]\n",
      "Y_hat: [45 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  5  6  8 11 18 39]\n",
      "Y_hat: [45 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [20]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 31 51 52 55]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 46]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [48 59 82 93]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 29 31]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 1 12]\n",
      "Y_hat: [45 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 54]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 5  8 16 19 28 40 43 45 56 83 85]\n",
      "Y_hat: [ 5  8 44 45 83 92]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 8 15 19 23 30 31 39 46 48 50]\n",
      "Y_hat: [ 8 39 60]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [46]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [13 19 26 27 29]\n",
      "Y_hat: [4]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  8 19 21 32 47 57 83]\n",
      "Y_hat: [45 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [13 19 23 27 29 39 51]\n",
      "Y_hat: [ 4 29]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [46]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3  4  8 16 19 29 32 38 62 77 83 86]\n",
      "Y_hat: [83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 5  8 15 16 38 39 44 45 60 77 79]\n",
      "Y_hat: [ 4  5  8 14 16 38 39 44 45 57 70 71 83 92 98]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  5 28 29 38 44]\n",
      "Y_hat: [ 4  5  8 14 18 29 38 39 44 45 57 70 83 92 98]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 2 12 22 69]\n",
      "Y_hat: [45 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  5 44 92]\n",
      "Y_hat: [ 4  8 29 45 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 1  2 17 22]\n",
      "Y_hat: [83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 33 38 41 56 57 67 73]\n",
      "Y_hat: [45 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [3]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 21 27 29 51]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 27 39 40 58 59 67]\n",
      "Y_hat: [ 4 39 45]\n",
      "did bad on this one\n",
      "first 50 words:\n",
      " mucicarmine diastase mucicarmine cd68 320k j8 hb2 anapylaxis domiant x60yrs oontributing 7kd diagnosd jkdf orapred comparisson craniotom scopolimine sc30with meroperem po262 ptcra vtrprtfkd 0ab heaviest minimanl 7kd j8 iliotibial babinsky 7kd babinsky 3225gms greeninsh squirts 7kd gaving high70 otheriwse imay enage todayl sputuma emoptional levadopa succynilcholine tthrough 7kd akdla\n",
      "codes / descriptions\n",
      "4019: Unspecified essential hypertension, 2720: Pure hypercholesterolemia, 42731: Atrial fibrillation, 2749: Gout, unspecified, 49390: Asthma, unspecified type, unspecified, 42789: Other specified cardiac dysrhythmias, 5180: Pulmonary collapse\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 38 67]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [34 52 73]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 0  1  2 17]\n",
      "Y_hat: [45 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3 19 29 38 57]\n",
      "Y_hat: [ 4 29 45 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 30 31 39 61]\n",
      "Y_hat: [39]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 27 51 54 60]\n",
      "Y_hat: [4]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3  4 19 31 36 47 94]\n",
      "Y_hat: [4]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 29 39 61 82 90]\n",
      "Y_hat: [ 4  8 39 60 71]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 1  2 12 22 69]\n",
      "Y_hat: [45 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [14 29 31 51 58 75 94]\n",
      "Y_hat: [ 4 29]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 28 31 38 46 59]\n",
      "Y_hat: [ 4 39]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [11 28 30 65]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  8 19 28 31 40 53 64 81 90 99]\n",
      "Y_hat: [ 4 29 39]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 19 28 56 65]\n",
      "Y_hat: [4 8]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  8 21 39 46 47]\n",
      "Y_hat: [ 4  5  8 14 39 45 83 92]\n",
      "did bad on this one\n",
      "first 50 words:\n",
      " mucicarmine diastase mucicarmine cd68 puilses b72 cryptococal b72 cryptococal amtsof moral 400x13 opoids sridha pericardiocensis reass ule domiant x60yrs oontributing 7kd diagnosd jkdf trnasfusion acendind balance24 ule neteral beginnig greeninsh squirts bs80 tutoring pericardiocensis reass ule opoids sridha thimine moral upates 7kd 3225gms ict bs80 1155gms mlng somnalence amtsof\n",
      "codes / descriptions\n",
      "4280: Congestive heart failure, unspecified, 5849: Acute kidney failure, unspecified, 2765: , 42731: Atrial fibrillation, 2449: Unspecified acquired hypothyroidism, 4439: Peripheral vascular disease, unspecified\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  5 15 19 29 38 44 45 83]\n",
      "Y_hat: [ 4  8 29 45 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [34 54 58 81]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [14 28 75 90]\n",
      "Y_hat: [4 8]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [44 45 49 92]\n",
      "Y_hat: [ 5 44 45 70 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  8 10 14 28 34 36 38 39 46 56 57 60 87]\n",
      "Y_hat: [8]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [10 19 37 46 54 58 59 95]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3  4 19 28 29 31 39 55 63]\n",
      "Y_hat: [ 4 29 39]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 8  9 72 82]\n",
      "Y_hat: [ 4 45 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 6 11 33 75 76 80 84]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [34 40 62 65]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 20 28 31]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3  7  8 19 28 29 35 41 77]\n",
      "Y_hat: [ 4  8 29 45 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 39 53 56 74 82 83]\n",
      "Y_hat: [4 8]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [27 34]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [39 71 79]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 5 15 32 45 46 57 83]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 5  6 28 36 38 46 56 62 70 72 76 85]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 5 27 35 38 57 76]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  8 16 27 29 32 39 51 59 71 73 77 79 81 97]\n",
      "Y_hat: [ 4  8 14 19 28 29 36 39 45 71 79]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 30 31]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 5  8 19 31 34 44 45 46 63 77 81 92]\n",
      "Y_hat: [8]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 5  8 19 29 31 37 39 44 45 49 51 86 89 96]\n",
      "Y_hat: [ 4  5  8 14 29 39 44 45 70 71 83]\n",
      "did bad on this one\n",
      "first 50 words:\n",
      " mucicarmine diastase mucicarmine cd68 puilses othostatics sufficently wrapo aspergill cjoj n84 b72 cryptococal jiuce isentress nephplex flucuated hourssince pleaseeval oxycodones chromagranin breatmilk dislysis hemiparesisr mamangment proximated sillihette painfil labetlol suspicions reflectors phillipines phenobarital suspicions phillipines thorocotoomy carpeted mamangment embed verded recurrentventricular 1050uniths 1080kcals apssing interstices po262 g2p resoling natricore\n",
      "codes / descriptions\n",
      "0389: Unspecified septicemia, 5849: Acute kidney failure, unspecified, 4019: Unspecified essential hypertension, 41401: Coronary atherosclerosis of native coronary artery, 2724: Other and unspecified hyperlipidemia, 73300: Osteoporosis, unspecified, 42731: Atrial fibrillation, 78552: Septic shock, 99592: Severe sepsis, 00845: Intestinal infection due to Clostridium difficile, V4582: Percutaneous transluminal coronary angioplasty status, 2760: Hyperosmolality and/or hypernatremia, V1251: Personal history of venous thrombosis and embolism, 5789: Hemorrhage of gastrointestinal tract, unspecified\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 8 19 35 54 63 72]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 8 16 19 28 31 32 46]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 6  8 11 52 70]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 6 19 34 65 68 80 83]\n",
      "Y_hat: [ 8 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3 19 24 29 30 31 48 51 62 92]\n",
      "Y_hat: [4 8]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 5 39 45 56 83]\n",
      "Y_hat: [45 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 7  8 27 29 51]\n",
      "Y_hat: [ 3  4 29]\n",
      "did bad on this one\n",
      "first 50 words:\n",
      " mucicarmine diastase mucicarmine mucicarmine calretinin c09 cd68 puilses othostatics sufficently wrapo aspergill cjoj n84 b72 cryptococal pneumomediastinal recanalize f4ffv suspision isentress nephplex flucuated hourssince unbeknownst balance24 epidsoes volunteered pamridronate esophagojejunal unbeknownst regularized detachable carpeted domiant x60yrs oontributing retroflexed ukd nonproduction diagnosd jkdf boht domiant 47am carpeted prezista mpossible licocaine\n",
      "codes / descriptions\n",
      "4275: Cardiac arrest, 5849: Acute kidney failure, unspecified, 2720: Pure hypercholesterolemia, 41401: Coronary atherosclerosis of native coronary artery, V4582: Percutaneous transluminal coronary angioplasty status\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample prediction\n",
      "Y_true: [ 4 83]\n",
      "Y_hat: [ 4  5  8 39 44 45 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 19 28 29 51 58]\n",
      "Y_hat: [ 4 29]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [10 15 19 28 33 63 83 97]\n",
      "Y_hat: [83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 31 32 39 52 60 61 82]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3 15 19 28 29 31 51 54 59 60]\n",
      "Y_hat: [ 4 29]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 23 24 26 29 30 39 54 55 58 97]\n",
      "Y_hat: [ 3  4 29]\n",
      "did bad on this one\n",
      "first 50 words:\n",
      " mucicarmine diastase mucicarmine mucicarmine calretinin c09 activr cd68 frs othostatics sufficently wrapo aspergill cjoj n84 b72 cryptococal imay enage todayl sputuma isentress nephplex flucuated hourssince f4ffv kgh 47am carpeted modules phenylnephrine integrityassessment uz broncheictasis perplexing kda uz superposition uz ventilatio uz softtly domiant x60yrs oontributing pannel ukd nonproduction diagnosd\n",
      "codes / descriptions\n",
      "4019: Unspecified essential hypertension, 9971: Cardiac complications, not elsewhere classified, E8782: Surgical operation with anastomosis, bypass, or graft, with natural or artificial tissues used as implant causing abnormal patient reaction, or later complication, without mention of misadventure at time of operation, 4111: Intermediate coronary syndrome, 41401: Coronary atherosclerosis of native coronary artery, 45829: Other iatrogenic hypotension, 42731: Atrial fibrillation, 53081: Esophageal reflux, 412: Old myocardial infarction, 49390: Asthma, unspecified type, unspecified, 27652: Hypovolemia\n",
      "\n",
      "sample prediction\n",
      "Y_true: [33 34 39 57 87]\n",
      "Y_hat: [39]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 10 18 21 32 35 56 72 78]\n",
      "Y_hat: [45 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [18 19 20 63 65 78]\n",
      "Y_hat: [83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [15 28 63 83 87]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [18 40 53 54 56 60 63 64 70 71 73 77 79]\n",
      "Y_hat: [4 8]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [13 27 68]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  8 27 28 33 41 53 71 79 90]\n",
      "Y_hat: [4 8]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 8 16 19 28 30 59 70 73 77 81 90 95]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 28 54 62 81 87]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [13 19 28 31 57 81 83 87]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [68 83]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 14 42 53 60 75 76 82]\n",
      "Y_hat: [ 4  8 45]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 10 14 29 38 42 51 60 72 75 83 89]\n",
      "Y_hat: [ 4  8 14 29 45 71 75 79 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 5  8 15 63 70 72 73 76 84]\n",
      "Y_hat: [ 5  8 44 45 70 83 92]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 30 39 60 89]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  8 19 29 31 51 54 56 81 82 90]\n",
      "Y_hat: [4 8]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  8 16 34 38 39 40 43 54 60 73 79 83 88 89 95]\n",
      "Y_hat: [ 4  8 39 45 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [39 42 60 75]\n",
      "Y_hat: [ 4 39]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  5  8 18 27 29 33 34 39 45 46 48 51 55 71 89 93 98]\n",
      "Y_hat: [ 4  5  8 14 29 39 44 45 70 71 83 92]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3  4  8 19 29 31 38 41 46 54 57 58 59 83 88 94]\n",
      "Y_hat: [ 4  8 19 29 83]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 3  7 18 37 42 58 60 62 71 83 92]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 5  8 19 38 45]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 25 26 29 31 47 51 56 68 77 78]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  8 13 28 29 51 55 57 71 73 83 93]\n",
      "Y_hat: [ 4  8 14 29 71]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [19 38]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 14 27 28 29 32 33 39 42 46 47 48 56 75]\n",
      "Y_hat: [ 4  8 29 39 45]\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4 13 15 31 33 54 59 86 92]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [ 4  8 19 37 39 60 82 88]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [73 78]\n",
      "Y_hat: []\n",
      "\n",
      "sample prediction\n",
      "Y_true: [39 44 45 46 54 77 78 80 83 86 98]\n",
      "Y_hat: [ 5  8 44 45 70 83 92 98]\n",
      "\n",
      "y shape: (13645, 100)\n",
      "yhat shape: (13645, 100)\n",
      "\n",
      "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
      "(0.082932104119159689, 0.24361622467734725, 0.10338353745392331, 0.13170848462244369, 0.52351779539364274)\n",
      "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
      "(0.038497327479313233, 0.13565341033810119, 0.057735493962940587, 0.060595684640023607, 0.56196847634451541)\n",
      "\n",
      "test time: 19.8731119633\n",
      "sanity check on train\n",
      "y shape: (27464, 100)\n",
      "yhat shape: (27464, 100)\n",
      "\n",
      "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
      "(0.083808175478519403, 0.24869500773318745, 0.10333754392422573, 0.13321366035799373, 0.52386990319164972)\n",
      "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
      "(0.03898264309581604, 0.13257499503962578, 0.058320971882190077, 0.061037143252990964, 0.56266462657761174)\n",
      "\n",
      "saved metrics, params, model to directory /nethome/jmullenbach3/cnn-medical-text/saved_models/conv_attn_Sep_05_17:01\n"
     ]
    }
   ],
   "source": [
    "reload(training)\n",
    "reload(training.tools)\n",
    "for epoch in range(args.n_epochs):\n",
    "    #only test on train set on very last epoch\n",
    "    test_on_train = (epoch == args.n_epochs - 1)\n",
    "    metrics_t, metrics, fpr, tpr, losses_attn = training.one_epoch(args.dataset, args.vocab, model, optimizer, args.Y, \n",
    "                                                              epoch, args.data_path, min_size, args.gpu, \n",
    "                                                              args.objective, args.split_batch, uneven_params,\n",
    "                                                              args.testing, dicts, args.samples, test_on_train,\n",
    "                                                              fig_ax)\n",
    "    for name in metrics.keys():\n",
    "        metrics_hist[name].append(metrics[name])\n",
    "    for name in metrics_t.keys():\n",
    "        metrics_hist_tr[name].append(metrics_t[name])\n",
    "\n",
    "    #save metrics, model, params\n",
    "    if epoch == 0:\n",
    "        os.mkdir(model_dir)\n",
    "    tools.save_everything(args, metrics_hist, metrics_hist_tr, fpr, tpr, model, model_dir, params, args.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.46020106971263885,\n",
       " 0.34230467081069949,\n",
       " 0.29181802322467171,\n",
       " 0.26979313194751742,\n",
       " 0.25762402445077898,\n",
       " 0.25130136782924334,\n",
       " 0.24805440051215036,\n",
       " 0.24014646653085947,\n",
       " 0.2359659432537026,\n",
       " 0.2315664889663458,\n",
       " 0.2032733253389597,\n",
       " 0.19856074146926403,\n",
       " 0.19649927839636802,\n",
       " 0.19438290625810623,\n",
       " 0.1906559792906046,\n",
       " 0.18422420255839825,\n",
       " 0.17600818701088428,\n",
       " 0.17511099882423878,\n",
       " 0.16947659820318223,\n",
       " 0.16570037879049779,\n",
       " 0.16313012726604939,\n",
       " 0.16098728083074093,\n",
       " 0.15891819477081298,\n",
       " 0.15647753559052943,\n",
       " 0.15530971303582192,\n",
       " 0.15456257559359074,\n",
       " 0.15472859553992749,\n",
       " 0.15325161136686802,\n",
       " 0.15158440254628658,\n",
       " 0.15079417467117309,\n",
       " 0.15059599183499814,\n",
       " 0.1492274995148182,\n",
       " 0.14938122749328614,\n",
       " 0.14716707840561866,\n",
       " 0.14368100315332413,\n",
       " 0.14286545611917972,\n",
       " 0.14128037892282008,\n",
       " 0.13972307287156582,\n",
       " 0.14133251160383226,\n",
       " 0.14114140234887601,\n",
       " 0.14114303328096867,\n",
       " 0.14097670800983905,\n",
       " 0.13921181619167328,\n",
       " 0.13968265667557717,\n",
       " 0.14091065935790539,\n",
       " 0.14117312893271447,\n",
       " 0.14182105362415315,\n",
       " 0.14164803937077522,\n",
       " 0.14162315227091313,\n",
       " 0.14088524959981441,\n",
       " 0.14041516058146952,\n",
       " 0.14007469423115254,\n",
       " 0.13977689638733864,\n",
       " 0.13975987665355205,\n",
       " 0.13910404168069362,\n",
       " 0.13886930853128432,\n",
       " 0.13827861502766609,\n",
       " 0.13846228681504727,\n",
       " 0.13853556789457799,\n",
       " 0.1382919103652239,\n",
       " 0.13808542072772981,\n",
       " 0.13741644918918611,\n",
       " 0.13916591070592405,\n",
       " 0.13932708084583281,\n",
       " 0.13985802538692951,\n",
       " 0.1391558164358139,\n",
       " 0.14215086430311202,\n",
       " 0.14258877322077751,\n",
       " 0.14192892625927925,\n",
       " 0.14331559091806412,\n",
       " 0.14456256903707981,\n",
       " 0.14628734409809113,\n",
       " 0.14647822953760625,\n",
       " 0.14592653460800648,\n",
       " 0.14628132760524751,\n",
       " 0.14776918031275271,\n",
       " 0.14526955887675286,\n",
       " 0.14528534211218358,\n",
       " 0.1464300610125065,\n",
       " 0.1442360494285822,\n",
       " 0.14270619839429854,\n",
       " 0.14308756366372108,\n",
       " 0.14228301100432872,\n",
       " 0.14246290430426597,\n",
       " 0.14207752011716365,\n",
       " 0.14214598186314106,\n",
       " 0.14322852686047555,\n",
       " 0.14362975150346757,\n",
       " 0.14245591528713702,\n",
       " 0.14354472510516644,\n",
       " 0.14338318072259426,\n",
       " 0.14370619721710681,\n",
       " 0.14395624831318854,\n",
       " 0.14580970861017703,\n",
       " 0.14801401771605016,\n",
       " 0.14936174340546132,\n",
       " 0.14940526790916919,\n",
       " 0.14975732274353504,\n",
       " 0.15190003588795661,\n",
       " 0.15411790758371352,\n",
       " 0.15758970662951469,\n",
       " 0.15688577823340893,\n",
       " 0.15836528696119787,\n",
       " 0.15895363718271255,\n",
       " 0.15785010937601329,\n",
       " 0.15640234936028719,\n",
       " 0.15685423683375121,\n",
       " 0.15707826543599368,\n",
       " 0.15661669325083494,\n",
       " 0.15711197543889285,\n",
       " 0.155237284488976,\n",
       " 0.15803982283920048,\n",
       " 0.1584247274324298,\n",
       " 0.15862036991864442,\n",
       " 0.16175910711288452,\n",
       " 0.16213293217122554,\n",
       " 0.16232000470161437,\n",
       " 0.16347648613154889,\n",
       " 0.16352477394044398,\n",
       " 0.16383024826645851,\n",
       " 0.1657055613398552,\n",
       " 0.16253387622535229,\n",
       " 0.16329445891082287,\n",
       " 0.16447179362177849,\n",
       " 0.16422184750437738,\n",
       " 0.16673340566456318,\n",
       " 0.17014144010841847,\n",
       " 0.1698445651680231,\n",
       " 0.17204126290977001,\n",
       " 0.17205599375069142,\n",
       " 0.17111341796815396,\n",
       " 0.1734217596054077,\n",
       " 0.17550375521183015,\n",
       " 0.17559969574213027,\n",
       " 0.17601792067289351,\n",
       " 0.17545478880405427,\n",
       " 0.17357995375990867,\n",
       " 0.1740859082341194,\n",
       " 0.17340935520827772,\n",
       " 0.17429883100092411,\n",
       " 0.17651261679828167,\n",
       " 0.17964546583592891,\n",
       " 0.17982046954333783,\n",
       " 0.1803493670374155,\n",
       " 0.181264144256711,\n",
       " 0.18195576108992101,\n",
       " 0.18390479393303394,\n",
       " 0.18381041347980498,\n",
       " 0.18508051253855229,\n",
       " 0.18609311930835248,\n",
       " 0.18732921294867994,\n",
       " 0.18661138005554676,\n",
       " 0.18530050173401832,\n",
       " 0.18587386205792428,\n",
       " 0.18593387678265572,\n",
       " 0.18841710671782494,\n",
       " 0.18909453272819518,\n",
       " 0.1916659488528967,\n",
       " 0.19531584344804287,\n",
       " 0.19807784207165241,\n",
       " 0.19487212955951691,\n",
       " 0.19573672711849213,\n",
       " 0.19774924002587796,\n",
       " 0.19758714340627193,\n",
       " 0.19850215055048465,\n",
       " 0.19636532925069333,\n",
       " 0.19642439864575864,\n",
       " 0.19816550455987453,\n",
       " 0.19580426581203939,\n",
       " 0.19454330973327161,\n",
       " 0.19782503217458725,\n",
       " 0.19764953598380089,\n",
       " 0.1972889095544815,\n",
       " 0.19939411103725432,\n",
       " 0.2006962190568447,\n",
       " 0.2034531493484974,\n",
       " 0.20358837217092515,\n",
       " 0.20018875390291213,\n",
       " 0.19989843815565109,\n",
       " 0.20309320524334906,\n",
       " 0.20280970953404903,\n",
       " 0.20268223516643047,\n",
       " 0.20350561134517192,\n",
       " 0.20383095450699329,\n",
       " 0.2031961128860712,\n",
       " 0.20232507787644863,\n",
       " 0.20335739843547343,\n",
       " 0.20637741096317769,\n",
       " 0.21019147492945195,\n",
       " 0.20763865761458875,\n",
       " 0.20986575737595559,\n",
       " 0.20913932383060455,\n",
       " 0.20836415961384774,\n",
       " 0.2074848334491253,\n",
       " 0.20707937985658645,\n",
       " 0.20811338409781455,\n",
       " 0.20903880536556244,\n",
       " 0.21081460639834404,\n",
       " 0.20855332747101785,\n",
       " 0.2087508161365986,\n",
       " 0.20856113448739053,\n",
       " 0.21222256168723105,\n",
       " 0.21397777423262596,\n",
       " 0.2171090592443943,\n",
       " 0.21761637523770333,\n",
       " 0.21815906941890717,\n",
       " 0.21881957009434699,\n",
       " 0.21769300997257232,\n",
       " 0.21873737141489982,\n",
       " 0.2188952648639679,\n",
       " 0.21942659661173822,\n",
       " 0.21962822899222373,\n",
       " 0.21996177658438681,\n",
       " 0.2182146941125393,\n",
       " 0.21989340230822563,\n",
       " 0.22022791549563409,\n",
       " 0.21952330812811852,\n",
       " 0.21905210152268409,\n",
       " 0.21973143875598908,\n",
       " 0.22030368000268935,\n",
       " 0.22036501049995422,\n",
       " 0.2189900104701519,\n",
       " 0.21980424791574479,\n",
       " 0.22160387009382249,\n",
       " 0.22170634225010871,\n",
       " 0.22386421576142312,\n",
       " 0.22669302135705949,\n",
       " 0.22900394305586816,\n",
       " 0.22861324295401572,\n",
       " 0.22981593534350395,\n",
       " 0.23211934119462968,\n",
       " 0.23235228717327117,\n",
       " 0.23311382248997689,\n",
       " 0.23086787074804305,\n",
       " 0.23203479379415512,\n",
       " 0.23011805593967438,\n",
       " 0.2276000227034092,\n",
       " 0.22705923616886139,\n",
       " 0.22831225469708444,\n",
       " 0.23018822893500329,\n",
       " 0.23001323387026787,\n",
       " 0.23398400932550431,\n",
       " 0.23380019173026084,\n",
       " 0.23518341183662414,\n",
       " 0.23834776893258094,\n",
       " 0.24071732044219971,\n",
       " 0.24238953858613968,\n",
       " 0.2439793486893177,\n",
       " 0.24602008432149888,\n",
       " 0.24462024241685867,\n",
       " 0.24379536420106887,\n",
       " 0.24250258147716522,\n",
       " 0.24499737888574599,\n",
       " 0.24865730881690978,\n",
       " 0.24732634514570237,\n",
       " 0.24990864664316179,\n",
       " 0.25207205832004548,\n",
       " 0.2531920896470547,\n",
       " 0.25308855563402177,\n",
       " 0.25402516931295394,\n",
       " 0.25721688047051428,\n",
       " 0.25546791896224019,\n",
       " 0.25423315063118934,\n",
       " 0.25203617647290227,\n",
       " 0.25150153279304505,\n",
       " 0.25049321666359903,\n",
       " 0.24906281426548957,\n",
       " 0.24948963135480881,\n",
       " 0.24918834418058394,\n",
       " 0.24832659050822259,\n",
       " 0.24711884647607804,\n",
       " 0.24991065546870231,\n",
       " 0.2507804583013058,\n",
       " 0.25521426081657411,\n",
       " 0.25454908311367036,\n",
       " 0.25462763920426368,\n",
       " 0.25538941651582719,\n",
       " 0.25138560056686399,\n",
       " 0.25675900578498839,\n",
       " 0.25944700226187706,\n",
       " 0.25876185327768325,\n",
       " 0.25650241181254385,\n",
       " 0.25569065600633623,\n",
       " 0.25328458204865456,\n",
       " 0.25334020242094996,\n",
       " 0.25470854833722112,\n",
       " 0.25402088001370432,\n",
       " 0.25766747787594796,\n",
       " 0.25101731352508067,\n",
       " 0.25062112487852573,\n",
       " 0.24878238081932069,\n",
       " 0.25434106126427652,\n",
       " 0.25263388782739638,\n",
       " 0.25365056768059729,\n",
       " 0.26123392418026925,\n",
       " 0.25564676031470301,\n",
       " 0.25732138976454733,\n",
       " 0.25879173412919043,\n",
       " 0.26137465529143811,\n",
       " 0.26215836428105832,\n",
       " 0.2664173121750355,\n",
       " 0.26318251907825468,\n",
       " 0.26560603603720667,\n",
       " 0.26600501999258996,\n",
       " 0.25906262353062631,\n",
       " 0.2631849645078182,\n",
       " 0.2668416479229927,\n",
       " 0.26042086221277716,\n",
       " 0.26022207371890543,\n",
       " 0.26033332221210004,\n",
       " 0.26016264013946055,\n",
       " 0.2621783121675253,\n",
       " 0.26737130917608737,\n",
       " 0.26875200159847734,\n",
       " 0.26790841177105906,\n",
       " 0.26899065136909484,\n",
       " 0.26444597586989405,\n",
       " 0.26784409515559671,\n",
       " 0.26823998920619485,\n",
       " 0.2641485481709242,\n",
       " 0.26526026375591755,\n",
       " 0.26011755786836149,\n",
       " 0.25646221846342088,\n",
       " 0.25854958966374397,\n",
       " 0.26088715635240078,\n",
       " 0.26220961727201941,\n",
       " 0.26302268333733081,\n",
       " 0.26296075507998468,\n",
       " 0.26119393423199655,\n",
       " 0.26339945673942566,\n",
       " 0.26519668430089949,\n",
       " 0.26803685009479522,\n",
       " 0.26498069845139982,\n",
       " 0.25854691445827482,\n",
       " 0.25580110803246497,\n",
       " 0.24971586614847183,\n",
       " 0.2503359879553318]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = range(10, 3380, 10)\n",
    "avg_losses_attn = [np.mean(losses_attn[max(batch-100,0):batch]) for batch in batches]\n",
    "avg_losses_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4c630ff610>]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4c63170810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEWCAYAAACe8xtsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4c63170a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "plt.figure()\n",
    "plt.xlabel('batch')\n",
    "plt.ylabel('loss averaged over last 100 batches')\n",
    "plt.title('running average loss during training for conv_attn')\n",
    "plt.grid(True)\n",
    "plt.plot(batches, avg_losses_attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC comparison at most frequent labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first, get the most frequent 10 labels\n",
    "label_counts = Counter()\n",
    "files = ['../../../mimicdata/disch/%s.csv' % s for s in ['train', 'dev', 'test']]\n",
    "for fl in files:\n",
    "    with open(fl, 'r') as f:\n",
    "        r = csv.reader(f)\n",
    "        #header\n",
    "        next(r)\n",
    "        for row in r:\n",
    "            labels = row[3].split(';')\n",
    "            for label in labels:\n",
    "                label_counts[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_dict, c_dict = datasets.load_lookups(vocab_file='../../../mimicdata/disch/vocab_100_full_3.csv', Y=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "desc_dict = datasets.load_code_descriptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_reverse = {k:v for v, k in c_dict.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vanilla_dir = '/nethome/jmullenbach3/cnn-medical-text/saved_models/cnn_vanilla_Aug_29_01:35'\n",
    "conv_attn_dir = '/nethome/jmullenbach3/cnn-medical-text/saved_models/conv_attn_Aug_29_04:26'\n",
    "smooth_attn_dir = '/nethome/jmullenbach3/cnn-medical-text/saved_models/smooth_attn_Aug_29_05:33'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#least frequent notes (of the top 100)\n",
    "def auc_for_labels(label_counts, c_reverse, top=True):\n",
    "    vanilla_dir = '/nethome/jmullenbach3/cnn-medical-text/saved_models/cnn_vanilla_Aug_29_01:35'\n",
    "    conv_attn_dir = '/nethome/jmullenbach3/cnn-medical-text/saved_models/conv_attn_Aug_29_04:26'\n",
    "    smooth_attn_dir = '/nethome/jmullenbach3/cnn-medical-text/saved_models/smooth_attn_Aug_29_05:33'\n",
    "    freq_labels = sorted(label_counts.items(), key=operator.itemgetter(1), reverse=top)[:10]\n",
    "\n",
    "    freq_code_idxs = [c_reverse[l] for l, cnt in freq_labels]\n",
    "    freq_code_idxs\n",
    "\n",
    "    vanilla_auc = defaultdict(int)\n",
    "    conv_attn_auc = defaultdict(int)\n",
    "    smooth_attn_auc = defaultdict(int)\n",
    "    for dr,auc in zip([vanilla_dir, conv_attn_dir, smooth_attn_dir], [vanilla_auc, conv_attn_auc, smooth_attn_auc]):\n",
    "        with open('%s/metrics.json' % dr, 'r') as mf:\n",
    "            metrics = json.load(mf)\n",
    "            for idx in freq_code_idxs:\n",
    "                auc['auc_%d' % idx] = metrics['auc_%d' % idx][-1]\n",
    "\n",
    "    print(\"CODE, CNN_VANILLA AUC, CONV_ATTN AUC, SMOOTH_ATTN AUC\")\n",
    "    van, attn, smooth = [], [], []\n",
    "    for idx in freq_code_idxs:\n",
    "        van.append(vanilla_auc['auc_%d' % idx])\n",
    "        attn.append(conv_attn_auc['auc_%d' % idx])\n",
    "        smooth.append(smooth_attn_auc['auc_%d' % idx])\n",
    "        print(c_dict[idx], vanilla_auc['auc_%d' % idx], conv_attn_auc['auc_%d' % idx], smooth_attn_auc['auc_%d' % idx])\n",
    "    print(\"mean\", np.mean(van), np.mean(attn), np.mean(smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE, CNN_VANILLA AUC, CONV_ATTN AUC, SMOOTH_ATTN AUC\n",
      "('4019', 0.6891143037011682, 0.7979199456656892, 0.7915347530848379)\n",
      "('4280', 0.8118297486255106, 0.8166476089319229, 0.8147458256532566)\n",
      "('42731', 0.9031083928222556, 0.9044021802531916, 0.9043293739643025)\n",
      "('41401', 0.8543699728433086, 0.8416428509676108, 0.8592848894133888)\n",
      "('5849', 0.780421112481219, 0.782359988503411, 0.7918754980216212)\n",
      "('25000', 0.8106467815063043, 0.8445279406812611, 0.8466894169633351)\n",
      "('2724', 0.84719184369232, 0.805838132456916, 0.8479376043678142)\n",
      "('51881', 0.8119945217255552, 0.7888217143686272, 0.7894389083718798)\n",
      "('5990', 0.818924268481707, 0.8152592181128854, 0.791367198775527)\n",
      "('53081', 0.8834232116031568, 0.8321276685779573, 0.8448425565452893)\n",
      "('mean', 0.82110241574825049, 0.82295472485194732, 0.82820460251612515)\n"
     ]
    }
   ],
   "source": [
    "auc_for_labels(label_counts, c_reverse, top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE, CNN_VANILLA AUC, CONV_ATTN AUC, SMOOTH_ATTN AUC\n",
      "('V502', 0.9663374366561801, 0.5, 0.6559458931361282)\n",
      "('V3000', 0.9784174719038583, 0.5708631665608512, 0.6510365632121901)\n",
      "('07054', 0.8046424987555212, 0.7348429214367816, 0.7306868733160028)\n",
      "('42833', 0.8760530238093939, 0.7230057876913409, 0.832795230080708)\n",
      "('V1046', 0.9036393670846623, 0.7038009452271865, 0.9240623545190675)\n",
      "('42832', 0.8294740452244971, 0.5718427401723155, 0.744944641955774)\n",
      "('42732', 0.8522549116570238, 0.6007449445692925, 0.7961598232224718)\n",
      "('99591', 0.7350045732947643, 0.5045227921227101, 0.5493950905820991)\n",
      "('V103', 0.936430381913394, 0.9290148119087592, 0.9363250908195782)\n",
      "('79902', 0.5891845691845692, 0.5, 0.5268272118272117)\n",
      "('mean', 0.84714382794838627, 0.63386381096892386, 0.73481787726712322)\n"
     ]
    }
   ],
   "source": [
    "auc_for_labels(label_counts, c_reverse, top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do longer notes have more labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    27465.00000\n",
       "mean       963.62596\n",
       "std        545.80944\n",
       "min         13.00000\n",
       "25%        579.00000\n",
       "50%        893.00000\n",
       "75%       1265.00000\n",
       "max       4775.00000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['num_labels'] = df.apply(lambda row: len(row[3].split(';')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = pd.qcut(df['length'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['categories'] = categories\n",
    "cat_stats = df.groupby('categories').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.76228613,  3.90079652,  4.29930479,  4.69727768,  5.23508006,\n",
       "        5.83278569,  6.50818479,  7.03249361,  7.80152672,  8.87490883])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_stats['num_labels'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_labels_vs_length(bins=10):\n",
    "    categories = pd.qcut(df['length'], bins)\n",
    "    df['categories'] = categories\n",
    "    cat_stats = df.groupby('categories').mean()\n",
    "    avg_num_labels = cat_stats['num_labels'].as_matrix()\n",
    "    plt.figure()\n",
    "    plt.xlabel('length percentile')\n",
    "    plt.ylabel('average number of labels')\n",
    "    plt.title('num labels vs. num words')\n",
    "    plt.bar(range(0, 100, 100/bins), avg_num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_labels_vs_length(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #get all the frequency stuff\n",
    "# freqs_idx = [freqs[ind2c[ind]] if ind2c[ind] in freqs.keys() else 0.0 for ind in range(num_labels)]\n",
    "# freqs_idx = np.array(freqs_idx)\n",
    "# log_freqs = np.log(freqs_idx)\n",
    "# #get rid of some 0's\n",
    "# log_freqs[np.where(log_freqs == -np.inf)] = np.log(1e-10)\n",
    "\n",
    "# #move to pandas to get quantiles\n",
    "# q = 10\n",
    "# df = pd.DataFrame(log_freqs, columns=['log_freq'])\n",
    "# df['quantile'] = pd.cut(df['log_freq'], q, labels=False)#, duplicates='drop')\n",
    "# quantile_idxs = [df.index[df['quantile'] == i].tolist() for i in range(q)]\n",
    "\n",
    "# log_freq_quantile_avgs = [np.mean(log_freqs[quantile_idxs[i]]) for i in range(q)]\n",
    "\n",
    "# #How big are the quantiles?\n",
    "# [len(q_i) for q_i in quantile_idxs]\n",
    "\n",
    "# log_freq_quantile_avgs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
